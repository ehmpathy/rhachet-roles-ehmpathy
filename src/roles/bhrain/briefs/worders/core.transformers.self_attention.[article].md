# ğŸ§© .brief.article: `self-attention`

## ğŸ”‘ what is self-attention

self-attention is a mechanism that lets every token in a sequence **dynamically weigh its relationship to every other token** when computing its next representation.

each token generates three vectors:
- **query (q)** â€” what this token is â€œlooking forâ€
- **key (k)** â€” what this token â€œoffersâ€
- **value (v)** â€” the actual information carried

the similarity of query vs key determines how much attention a token pays to another tokenâ€™s value.

mathematically:

\[
\text{Attention}(Q,K,V) = \text{softmax}\!\left(\frac{QK^\top}{\sqrt{d_k}}\right) V
\]

- **qkáµ€** â†’ pairwise similarity scores (**matmul**)
- **softmax** â†’ normalize into attention weights
- **weights Ã— v** â†’ weighted sum of values = new representation

---

## ğŸ¯ purpose
- let tokens reference and integrate information from anywhere in the sequence
- capture long-range dependencies in a single operation
- enable efficient, parallel computation across tokens
- provide multiple relational views through multi-head attention

---

## âš™ï¸ method

1. compute q, k, v for each token via linear projections (**matmuls**)
2. calculate similarity scores qÂ·káµ€ (**matmul**)
3. normalize scores with softmax (**elemuls**)
4. use normalized weights to combine values (**matmul**)
5. update each token representation with the weighted sum

---

## ğŸ”‘ benefits

1. **parallelism** â€” all q, k, v computed at once; no recurrence.
2. **long-range context** â€” any token can directly attend to any other.
3. **scalability** â€” uniform, repeatable structure scales with data/compute.
4. **expressivity** â€” multi-head attention lets the model learn diverse relational patterns.

---

## ğŸ§© intuition example

sentence:
> â€œthe cat sat on the mat because it was tired.â€

letâ€™s track the token **â€œitâ€** and show how its q, k, v vectors interact with **â€œthe cat.â€**
for illustration, assume a toy **5-dimensional hidden space**.

### token: **â€œitâ€**
- query (**q_it**) = `[0.9, 0.1, 0.0, 0.2, 0.3]`
  - â€œlooking for an antecedent noun with certain featuresâ€
- key (**k_it**) = `[0.1, 0.3, 0.2, 0.0, 0.4]`
  - â€œoffersâ€ self as a pronoun needing resolution
- value (**v_it**) = `[0.2, 0.5, 0.1, 0.0, 0.7]`
  - the information carried by â€œitâ€ itself

### token: **â€œcatâ€**
- query (**q_cat**) = `[0.2, 0.4, 0.1, 0.3, 0.0]`
- key (**k_cat**) = `[0.8, 0.2, 0.0, 0.1, 0.3]`
  - describes the features of â€œcatâ€ as a noun subject
- value (**v_cat**) = `[0.7, 0.6, 0.2, 0.4, 0.1]`
  - semantic content of â€œcatâ€

### computing attention
1. similarity score = q_it Â· k_cat =
   `0.9*0.8 + 0.1*0.2 + 0.0*0.0 + 0.2*0.1 + 0.3*0.3 = 0.72 + 0.02 + 0 + 0.02 + 0.09 = 0.85`
2. suppose normalized (softmax) weight for â€œcatâ€ = **0.70**, for others tokens total 0.30.
3. â€œitâ€â€™s updated representation =
   `0.70 * v_cat + 0.30 * (weighted sum of other tokensâ€™ values)`

so â€œitâ€â€™s final vector is now strongly composed of â€œcatâ€â€™s value vector, making the model understand that **â€œitâ€ refers to â€œthe cat.â€**

---

## ğŸ“Š insight
- **queries = what a token seeks**
- **keys = what a token provides**
- **values = the information contributed**
- weighted connections between them create **context-aware representations**.

self-attention is how llms directly model relationships across a sequence â€” allowing â€œitâ€ to learn its referent is â€œthe cat.â€
