# ğŸ§© .brief.demo: `multi-head self-attention`

## .what
this demo illustrates how **multi-head self-attention** allows a transformer to capture **different types of relationships simultaneously** by projecting the same tokens into multiple attention â€œheads.â€ each head learns its own query/key/value space, enabling diverse relational patterns (e.g., pronoun resolution, verb agreement, or topic continuity).

---

## ğŸ¯ purpose
- show how multiple attention heads operate in parallel
- demonstrate how heads specialize in different linguistic/semantic functions
- highlight how combining heads yields richer token representations

---

## âš™ï¸ demo setup

sentence:
> â€œthe cat sat on the mat because it was tired.â€

toy hidden size = **6**, with **2 heads**, each head having its own projection of q, k, v.

---

### ğŸ”¹ head 1 â€” pronoun resolution (antecedent tracking)

focus: linking **â€œitâ€** â†’ **â€œthe cat.â€**

- **q_it (head 1):** `[0.9, 0.1, 0.0]`
- **k_cat (head 1):** `[0.8, 0.2, 0.1]`
- **dot product:** `0.9*0.8 + 0.1*0.2 + 0.0*0.1 = 0.74`
- **attention weight:** 0.74 â†’ dominates over other tokens
- **result:** â€œitâ€ attends strongly to â€œcat,â€ resolving the pronoun.

---

### ğŸ”¹ head 2 â€” verb agreement (syntactic continuity)

focus: linking **subject** (â€œthe catâ€) â†’ **verb** (â€œsatâ€).

- **q_cat (head 2):** `[0.2, 0.7, 0.3]`
- **k_sat (head 2):** `[0.1, 0.9, 0.2]`
- **dot product:** `0.2*0.1 + 0.7*0.9 + 0.3*0.2 = 0.02 + 0.63 + 0.06 = 0.71`
- **attention weight:** strong match
- **result:** â€œcatâ€ attends to â€œsat,â€ enforcing subject-verb connection.

---

## ğŸ§© combined effect

after attention, outputs from all heads are concatenated and linearly transformed:

\[
\text{MultiHead}(Q,K,V) = \text{Concat}(\text{head}_1, \text{head}_2, \dots) W^O
\]

- **head 1 output:** captures **semantic resolution** (it â†” cat).
- **head 2 output:** captures **syntactic relation** (cat â†” sat).
- combined, the model encodes **both** types of context simultaneously.

---

## ğŸ“Š insight
- each head = a separate lens on token relationships.
- heads specialize (some semantic, some syntactic, some positional).
- combining them creates **richer, multifaceted token embeddings.**

in short: **multi-head attention = parallel perspectives on the same sequence.**
