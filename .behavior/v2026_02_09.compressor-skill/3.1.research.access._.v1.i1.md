# research: remote access requirements for brief.compress

**date**: 2026-02-09
**scope**: remote repositories, apis, and interfaces required to fulfill brief.compress skill

---

## executive summary

the brief.compress skill requires access to **LLMLingua-2** compression models. three integration paths are viable:

1. **typescript native** â€” use `@atjsh/llmlingua-2` npm package directly
2. **compiled binary** â€” bun/deno compile typescript to standalone executable
3. **python subprocess** â€” shell to `llmlingua` python package

no hosted api service for llmlingua was discovered. compression must run locally or self-hosted.

---

## remote repository 1: llmlingua python package

### contract

| attribute | value |
|-----------|-------|
| registry | pypi |
| package | `llmlingua` |
| version | 0.2.2 (apr 2024) [1] |
| python | >=3.8.0 [1] |
| install | `pip install llmlingua` [2] |

### interface (python sdk)

```python
from llmlingua import PromptCompressor

llm_lingua = PromptCompressor(
    model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
    use_llmlingua2=True,
)

compressed_prompt = llm_lingua.compress_prompt(
    prompt,
    rate=0.33,  # compression ratio (0-1 scale; 0.33 = 3x compression)
    force_tokens=['\n', '?']  # tokens to preserve
)
```
â€” [2]

### key api parameters

| param | type | description |
|-------|------|-------------|
| `model_name` | string | huggingface model id [2] |
| `use_llmlingua2` | boolean | enable llmlingua-2 mode [2] |
| `rate` | float | compression ratio, 0-1 scale [2] |
| `force_tokens` | list[str] | tokens to never remove [2] |

### dependencies

- transformers (huggingface)
- torch
- optional: `pip install optimum auto-gptq` for quantized models [1]

---

## remote repository 2: huggingface model hub

### models available

| model id | size | params | tradeoff |
|----------|------|--------|----------|
| `microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank` | ~500MB | 0.2B | small, fast, moderate accuracy [3] |
| `microsoft/llmlingua-2-xlm-roberta-large-meetingbank` | ~1.2GB | 0.5B | larger, slower, better accuracy [2] |

### model contract

> "This model was introduced in the paper LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression (Pan et al, 2024). It is a BERT multilingual base model (cased) finetuned to perform token classification for task agnostic prompt compression." â€” [3]

### access pattern

- models auto-download on first use via huggingface transformers
- cached to `~/.cache/huggingface/`
- no api key required for public models

---

## remote repository 3: llmlingua-2-js (javascript/typescript)

### contract

| attribute | value |
|-----------|-------|
| registry | npm |
| package | `@atjsh/llmlingua-2` |
| install | `npm install @atjsh/llmlingua-2` [4] |
| runtime | node.js + browser (webgpu optional) [4] |

### interface (typescript sdk)

```typescript
import LLMLingua2 from '@atjsh/llmlingua-2';

const compressor = await LLMLingua2.createCompressor({
  model: 'TinyBERT'  // or 'MobileBERT', 'BERT', 'XLM-RoBERTa'
});

const results = await compressor.compress(originalPrompt, {
  rate: 0.6,
  force_tokens: ['\n', '.', '!', '?', ','],
  chunk_end_tokens: ['.', '\n'],
  return_word_label: true,
  drop_consecutive: true
});
```
â€” [4]

### models available

| model | size | description |
|-------|------|-------------|
| TinyBERT | 57.1 MB | smallest and fastest, lower accuracy [4] |
| MobileBERT | 99.2 MB | mobile-optimized, moderate accuracy [4] |
| BERT | 710 MB | balance of speed and performance [4] |
| XLM-RoBERTa | 2240 MB | highest accuracy, slower [4] |

### dependencies

- `@huggingface/transformers`
- `@tensorflow/tfjs`
- `js-tiktoken`

---

## api service availability

### finding: no hosted api

no hosted api service for llmlingua was discovered via web research. available options:

| option | status |
|--------|--------|
| microsoft hosted api | not found |
| third-party llmlingua api | not found |
| huggingface inference api | not available for these models |

### implication

the brief.compress skill must either:

1. **run typescript directly** â€” invoke via node/bun/deno with npm package
2. **compile to binary** â€” use bun/deno compile for standalone executable
3. **shell to python** â€” invoke llmlingua python package via subprocess
4. **self-host api** â€” wrap llmlingua in a local http server

---

## portable binary options

### option a: bun compile (typescript â†’ binary)

> "Bun's bundler implements a --compile flag for generating a standalone binary from a TypeScript or JavaScript file. All imported files and packages are bundled into the executable, along with a copy of the Bun runtime." â€” [7]

```bash
# compile typescript to standalone binary
bun build --compile ./brief-compress.ts --outfile brief-compress

# cross-compile for different targets
bun build --compile --target=linux-x64 ./brief-compress.ts --outfile brief-compress-linux
```

| attribute | value |
|-----------|-------|
| binary size | ~96MB (includes bun runtime) [7] |
| cross-compile | yes (linux, macos, windows) [7] |
| dependencies | bundled into executable |

### option b: deno compile (typescript â†’ binary)

> "Since Deno v1.6, the deno compile command has empowered developers to turn JavaScript and TypeScript programs into single, standalone binaries that run on all major platformsâ€”no dependencies, no additional installs." â€” [8]

```bash
# compile to standalone binary
deno compile --output brief-compress ./brief-compress.ts

# with permissions
deno compile --allow-read --allow-write --output brief-compress ./brief-compress.ts
```

| attribute | value |
|-----------|-------|
| binary size | ~105MB (includes deno runtime) [8] |
| cross-compile | yes (linux, macos, windows) [8] |
| code sign | supported [8] |

### option c: python nuitka (python â†’ binary)

> "Nuitka compiles a Python program to a C binaryâ€”not by packaging the CPython runtime with the program bytecode, but by translating Python instructions into C." â€” [9]

```bash
# compile python to standalone binary
python -m nuitka --standalone --onefile compress_brief.py

# output: compress_brief.bin
```

| attribute | value |
|-----------|-------|
| binary size | varies (includes dependencies) |
| performance | 10-50% faster than bytecode [9] |
| build time | slow (full C compilation) [9] |

### option d: pyinstaller (python â†’ frozen bundle)

```bash
# bundle python to single file
pyinstaller --onefile compress_brief.py

# output: dist/compress_brief
```

| attribute | value |
|-----------|-------|
| binary size | ~100-500MB with torch/transformers |
| build time | fast (no compilation) [9] |
| performance | same as python bytecode [9] |

---

## typescript direct execution options

### option 1: node.js with tsx

```bash
# run typescript directly (no compile step)
npx tsx ./brief-compress.ts --input brief.md --output brief.md.min
```

| attribute | value |
|-----------|-------|
| startup time | ~200-500ms |
| dependency | tsx, node.js |

### option 2: bun native

```bash
# bun runs typescript natively
bun ./brief-compress.ts --input brief.md --output brief.md.min
```

| attribute | value |
|-----------|-------|
| startup time | ~50-100ms [10] |
| dependency | bun |

### option 3: deno native

```bash
# deno runs typescript natively
deno run --allow-read --allow-write ./brief-compress.ts
```

| attribute | value |
|-----------|-------|
| startup time | ~100-200ms |
| dependency | deno |

---

## best practices

### industry-wide

1. **use llmlingua-2 over llmlingua-1** â€” 3x-6x faster with better faithfulness [5]

> "LLMLingua-2 is 3x-6x faster than prior prompt compression methods that include original LLMLingua." â€” Pan et al., ACL 2024 [5]

2. **prefer bert-base for speed** â€” 87% memory reduction vs llama-7b alternatives [5]

> "LLMLingua-2 demonstrates 2.1GB peak GPU memory usage on MeetingBank, compared to 16.6GB for LLMLingua and 26.5GB for Selective-Context (87% reduction vs LLMLingua)." â€” Pan et al., ACL 2024 [5]

3. **target 2x-5x compression for safety** â€” 14x validated for reasoning tasks [5]

> "LLMLingua-2 achieves 14x compression on chain-of-thought reasoning (GSM8K) with minimal accuracy loss (78.85% â†’ 77.79%, only 1.06 percentage point degradation)." â€” Pan et al., ACL 2024 [5]

4. **preserve structural tokens** â€” force_tokens for \n, ?, ., etc [2]

### within this repo

1. **prefer typescript path** â€” aligns with repo's node.js ecosystem
2. **use TinyBERT or MobileBERT** â€” smallest models for skill invocation speed
3. **consider bun compile** â€” for portable binary distribution
4. **shell to python only if needed** â€” fallback for features not in js port

---

## integration recommendation

### primary path: typescript via bun

```
brief.compress skill (bash)
  â””â”€â”€ invokes bun ./compress.ts
      â””â”€â”€ uses @atjsh/llmlingua-2
          â””â”€â”€ auto-downloads TinyBERT model (57MB)
```

**pros**:
- native to repo ecosystem
- no python dependency
- fast startup (~50-100ms)
- can compile to binary for distribution

**cons**:
- experimental js port [4]
- smaller model selection than python

### alternative path: compiled binary

```
brief.compress skill (bash)
  â””â”€â”€ invokes ./brief-compress (standalone binary)
      â””â”€â”€ bundled @atjsh/llmlingua-2 + bun runtime
          â””â”€â”€ model cached in ~/.cache/
```

**pros**:
- zero runtime dependencies
- single file distribution
- cross-platform builds

**cons**:
- ~96-105MB binary size
- requires rebuild for updates

### fallback path: python subprocess

```
brief.compress skill (bash)
  â””â”€â”€ shells to python
      â””â”€â”€ uses llmlingua package
          â””â”€â”€ downloads huggingface model (~500MB)
```

**pros**:
- official microsoft implementation
- full model selection
- proven in production [6]

**cons**:
- requires python environment
- larger model download
- additional subprocess overhead

---

## citations

1. [llmlingua Â· PyPI](https://pypi.org/project/llmlingua/) â€” package version, python requirements
2. [GitHub - microsoft/LLMLingua](https://github.com/microsoft/LLMLingua) â€” api usage, model names, force_tokens
3. [microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank Â· Hugging Face](https://huggingface.co/microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank) â€” model size, description, usage
4. [GitHub - atjsh/llmlingua-2-js](https://github.com/atjsh/llmlingua-2-js) â€” npm package, js api, model sizes
5. Pan et al., "LLMLingua-2: Data Distillation for Efficient and Faithful Task-Agnostic Prompt Compression." ACL 2024 Findings. arXiv:2403.12968 â€” performance metrics, architecture
6. [Microsoft Research Blog on LLMLingua](https://www.microsoft.com/en-us/research/project/llmlingua/) â€” production deployment, framework integrations
7. [Bun Single-file Executable](https://bun.com/docs/bundler/executables) â€” bun compile, binary size, cross-compile
8. [Deno Compile](https://deno.com/blog/deno-compile-executable-programs) â€” deno compile, standalone binaries
9. [Nuitka vs PyInstaller](https://sparxeng.com/blog/software/python-standalone-executable-generators-pyinstaller-nuitka-cx-freeze) â€” python compilation options
10. [Bun vs Node.js vs Deno](https://zerotomastery.io/blog/deno-vs-node-vs-bun-comparison-guide/) â€” runtime comparison

---

## quotes from sources

> "LLMLingua-2 formulates prompt compression as a token classification problem using a Transformer encoder (XLM-RoBERTa-large, mBERT) rather than entropy-based token removal." â€” Pan et al., arXiv:2403.12968 [5]

> "Everything can be done in the browser, and if your environment supports WebGPU, you can use it without requiring server-side processing by default." â€” llmlingua-2-js README [4]

> "LLMLingua has been integrated into production frameworks including LangChain, LlamaIndex, Prompt Flow, and Azure enterprise platforms." â€” Microsoft LLMLingua GitHub [6]

> "Bun's bundler implements a --compile flag for generating a standalone binary from a TypeScript or JavaScript file." â€” Bun Docs [7]

> "Since Deno v1.6, the deno compile command has empowered developers to turn JavaScript and TypeScript programs into single, standalone binaries." â€” Deno Blog [8]

---

ğŸ¢ three paths: typescript native, compiled binary, or python subprocess. bun+typescript recommended ğŸŒŠ
