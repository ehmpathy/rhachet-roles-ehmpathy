# vision: brief.compress skill

**wish**: create a skill that compresses mechanic briefs via LLMLingua, emits `.md.min` files

---

## the outcome world

### day-in-the-life

**before**:
```
mechanic session starts
‚îú‚îÄ‚îÄ sessionstart hook loads briefs
‚îú‚îÄ‚îÄ ~61k tokens injected into context
‚îú‚îÄ‚îÄ $0.18 per session just for briefs
‚îî‚îÄ‚îÄ briefs compete with task context for window space
```

**after**:
```
mechanic session starts
‚îú‚îÄ‚îÄ sessionstart hook loads compressed briefs (.md.min)
‚îú‚îÄ‚îÄ ~15k tokens injected into context (75% reduction)
‚îú‚îÄ‚îÄ $0.045 per session for briefs
‚îî‚îÄ‚îÄ more room for task context, code, and conversation
```

### before/after contrast

| metric | before | after | delta |
|--------|--------|-------|-------|
| brief tokens | ~61k | ~15k | -75% |
| brief cost/session | $0.18 | $0.045 | -75% |
| context headroom | constrained | spacious | +46k tokens |
| behavioral equivalence | baseline | validated | same |

### the "aha" moment

> "wait, the mechanic still follows all the same rules and patterns, but my sessions are 4x cheaper and I have way more room for actual work?"

the value clicks when:
1. first session with compressed briefs loads noticeably faster
2. cost dashboard shows 75% reduction in brief overhead
3. complex tasks that previously hit context limits now fit comfortably
4. zero degradation in mechanic behavior ‚Äî same practices, same quality

---

## user experience

### usecases

1. **compress a single brief**
   ```sh
   rhx brief.compress path/to/my-brief.md
   # emits: path/to/my-brief.md.min
   ```

2. **compress all briefs in a role**
   ```sh
   rhx brief.compress --glob "src/domain.roles/mechanic/briefs/**/*.md"
   # emits: .md.min files next to each source
   ```

3. **validate compression quality**
   ```sh
   rhx brief.compress path/to/brief.md --validate
   # compresses, then runs behavioral equivalence check
   ```

4. **preview without emit**
   ```sh
   rhx brief.compress path/to/brief.md --mode plan
   # shows compression ratio, token counts, diff preview
   ```

5. **choose compression model**
   ```sh
   rhx brief.compress path/to/brief.md --mech llmlingua/v2/tinybert
   # fast, small model ‚Äî good for laptops, ~4-10s per brief on CPU

   rhx brief.compress path/to/brief.md --mech llmlingua/v2/xlm-roberta
   # accurate, large model ‚Äî best quality, needs GPU or patience
   ```

### model selection

users choose the compression model based on their hardware and quality needs:

| model | `--mech` value | size | speed | accuracy | use case |
|-------|----------------|------|-------|----------|----------|
| TinyBERT | `llmlingua/v2/tinybert` | 57 MB | fastest | good | laptop CPU, quick iteration |
| MobileBERT | `llmlingua/v2/mobilebert` | 99 MB | fast | better | laptop CPU, balanced |
| BERT | `llmlingua/v2/bert` | 710 MB | moderate | high | GPU or patient CPU |
| XLM-RoBERTa | `llmlingua/v2/xlm-roberta` | 2.2 GB | slow | highest | GPU, production quality |

**default**: `llmlingua/v2/mobilebert` ‚Äî balanced for most developer workflows

### contract inputs & outputs

**inputs**:
- `path` ‚Äî file or glob pattern for briefs to compress
- `--mode` ‚Äî `plan` (preview) or `apply` (emit)
- `--mech` ‚Äî compression model (default: `llmlingua/v2/mobilebert`)
- `--validate` ‚Äî run behavioral equivalence check after compress
- `--ratio` ‚Äî target compression ratio (default: 4x, max safe: 20x)

**outputs**:
- `.md.min` file collocated with source
- compression stats (tokens before/after, ratio achieved)
- validation result (if requested)

### timeline

1. **invoke** ‚Äî `rhx brief.compress path/to/brief.md`
2. **load** ‚Äî read source brief
3. **compress** ‚Äî LLMLingua-2 token classification via selected `--mech`
4. **emit** ‚Äî write `.md.min` file
5. **report** ‚Äî show stats and path

typical duration (per brief):
- GPU: 0.4-0.5s (any model)
- CPU + TinyBERT: ~4-10s
- CPU + MobileBERT: ~8-20s
- CPU + XLM-RoBERTa: ~30-60s

---

## mental model

### how users describe it

> "it's like gzip for briefs, but semantic ‚Äî it removes the words the LLM can figure out from context"

> "minified briefs, like how we minify JS for production"

> "same instructions, fewer tokens, same behavior"

### analogies

| analogy | fit |
|---------|-----|
| **gzip** | compression, but semantic not syntactic |
| **JS minification** | smaller payload, same execution |
| **telegram style** | cut filler words, keep intent |
| **compression codec** | lossy but perceptually equivalent |

### terms

| user term | internal term |
|-----------|---------------|
| "minified brief" | compressed brief |
| "shrink" | compress |
| ".min file" | .md.min |
| "does it still work?" | behavioral equivalence |

---

## evaluation

### how well does it solve the goals?

| goal | solution | confidence |
|------|----------|------------|
| reduce brief token cost | 4-20x compression | high (LLMLingua proven) |
| preserve mechanic behavior | behavioral equivalence validation | high |
| simple invocation | `rhx brief.compress` | high |
| collocated output | `.md.min` next to source | high |
| reversible | source `.md` unchanged | high |

### pros

- **proven method** ‚Äî LLMLingua empirically validated, 20x compression with 1.5% loss
- **zero behavior change** ‚Äî mechanic follows same rules
- **massive cost savings** ‚Äî 75%+ reduction in brief overhead
- **more context headroom** ‚Äî room for larger tasks
- **simple workflow** ‚Äî one command, done

### cons

- **requires local model** ‚Äî LLMLingua-2 needs BERT-level model (57MB-2.2GB based on `--mech`)
- **initial setup** ‚Äî first run downloads selected model
- **validation overhead** ‚Äî behavioral equivalence check adds time
- **edge cases** ‚Äî some briefs may compress poorly (constraint-heavy)
- **CPU speed** ‚Äî laptop CPU inference is 10-100x slower than GPU

### edge cases and pit of success

| edge case | pit of success |
|-----------|----------------|
| brief is mostly constraints | detect low-compression briefs, warn user |
| compression degrades behavior | `--validate` flag catches before deploy |
| user forgets to recompress after edit | git hook or CI check for stale `.md.min` |
| compressed brief unreadable | source `.md` always preserved, `.min` is for machines |
| compression ratio too aggressive | default to safe 4x, require explicit flag for 20x |

### awkward discoveries

1. **LLMLingua requires Python or TypeScript** ‚Äî skill can shell to Python or use `@atjsh/llmlingua-2` npm package
2. **model download** ‚Äî first run downloads selected model (57MB for TinyBERT, up to 2.2GB for XLM-RoBERTa)
3. **local only** ‚Äî no hosted API found; must run inference locally
4. **brief interdependencies** ‚Äî some briefs reference others; compress must preserve references
5. **CPU vs GPU tradeoff** ‚Äî GPU is 10-100x faster; most devs will use CPU and wait

---

## next steps

1. research LLMLingua-2 integration options (Python lib, API, local model)
2. prototype `brief.compress` skill with single-file support
3. add `--validate` with behavioral equivalence check
4. add glob support for batch compression
5. integrate into sessionstart hook to prefer `.md.min` when present

---

üê¢ the vision: same mechanic, 75% fewer tokens, 75% lower cost. shell yeah üåä
