# research: OSS levers for brief.compress

**date**: 2026-02-09
**scope**: open source tools that can be leveraged for the brief.compress skill

---

## executive summary

LLMLingua-2 is the clear winner for prompt compression. two integration paths are viable: the official Python package (`llmlingua`) or the experimental JavaScript port (`@atjsh/llmlingua-2`). the Python path is production-proven with framework integrations; the JavaScript path aligns with the repo's node.js ecosystem. both are actively maintained.

---

## lever.1: llmlingua (Python)

### overview

> "LLMLingua offers an efficient solution to compress prompts by up to 20x, enhancing the utility of LLMs."
‚Äî [microsoft/LLMLingua GitHub](https://github.com/microsoft/LLMLingua) [1]

| attribute | value |
|-----------|-------|
| registry | pypi |
| package | `llmlingua` |
| version | 0.2.2 (apr 2024) [2] |
| license | MIT |
| python | >=3.8.0 [2] |
| status | alpha (active development) [2] |
| github stars | 5.8k [1] |
| maintainer | microsoft research [1] |

### usage

> ```python
> from llmlingua import PromptCompressor
>
> llm_lingua = PromptCompressor(
>     model_name="microsoft/llmlingua-2-bert-base-multilingual-cased-meetingbank",
>     use_llmlingua2=True,
> )
>
> compressed_prompt = llm_lingua.compress_prompt(
>     prompt,
>     rate=0.33,  # compression ratio (0-1 scale; 0.33 = 3x compression)
>     force_tokens=['\\n', '?']  # tokens to preserve
> )
> ```
‚Äî [microsoft/LLMLingua GitHub](https://github.com/microsoft/LLMLingua) [1]

### maintenance status

> "SCBench, a KV cache-centric analysis work, was announced [24/12/13]. RetrievalAttention for accelerated long-context LLM inference was announced [24/09/16]."
‚Äî [microsoft/LLMLingua GitHub](https://github.com/microsoft/LLMLingua) [1]

active development with new papers and tools released through 2024. 85 commits on main branch [1].

### production adoption

> "LLMLingua has been integrated into LangChain and LlamaIndex, two widely-used RAG frameworks."
‚Äî [LLMLingua Series](https://llmlingua.com/) [3]

> "LLMLingua and LongLLMLingua have been incorporated into the LlamaIndex pipeline, which allows for more convenient use of LLMLingua-related technologies in RAG scenarios."
‚Äî [LlamaIndex Blog](https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7) [4]

> "The integration works out of the box with any supported model on LangChain and can be customized to use your preferred compression rate or model variant."
‚Äî [LangChain Docs](https://docs.langchain.com/oss/python/integrations/retrievers/llmlingua) [5]

### pros

- **production-proven** ‚Äî integrated into LangChain, LlamaIndex, Prompt Flow [1]
- **active maintenance** ‚Äî microsoft research team, frequent releases [1]
- **full model selection** ‚Äî supports all LLMLingua-2 model variants [1]
- **comprehensive docs** ‚Äî examples for RAG, meetings, CoT, code [1]
- **high stars** ‚Äî 5.8k GitHub stars indicates community trust [1]

### cons

- **python dependency** ‚Äî requires python runtime in shell skill
- **large models** ‚Äî bert-base is ~500MB, xlm-roberta is ~1.2GB [1]
- **alpha status** ‚Äî api may change [2]
- **subprocess overhead** ‚Äî shell to python adds latency

### recommendation

**[CONSIDER]** ‚Äî use if python subprocess is acceptable; production-proven path

---

## lever.2: @atjsh/llmlingua-2 (JavaScript/TypeScript)

### overview

> "llmlingua-2-js, ported by atjsh, is a pure JavaScript/TypeScript implementation of LLMLingua-2, designed to run in web browsers and Node.js environments."
‚Äî [atjsh/llmlingua-2-js GitHub](https://github.com/atjsh/llmlingua-2-js) [6]

| attribute | value |
|-----------|-------|
| registry | npm |
| package | `@atjsh/llmlingua-2` |
| version | 2.0.3 [7] |
| license | MIT |
| node | compatible with node.js [6] |
| status | experimental [6] |
| github stars | 20 [6] |
| last release | june 8, 2025 (v2.0.0) [6] |

### usage

> ```typescript
> import LLMLingua2 from '@atjsh/llmlingua-2';
>
> const compressor = await LLMLingua2.createCompressor({
>   model: 'TinyBERT'  // or 'MobileBERT', 'BERT', 'XLM-RoBERTa'
> });
>
> const results = await compressor.compress(originalPrompt, {
>   rate: 0.6,
>   force_tokens: ['\\n', '.', '!', '?', ','],
>   chunk_end_tokens: ['.', '\\n'],
>   return_word_label: true,
>   drop_consecutive: true
> });
> ```
‚Äî [@atjsh/llmlingua-2 npm](https://www.npmjs.com/package/@atjsh/llmlingua-2) [7]

### model options

> "Available models include TinyBERT (57.1 MB), MobileBERT (99.2 MB), BERT (710 MB), and XLM-RoBERTa (2240 MB), each with different tradeoffs between speed and accuracy."
‚Äî [atjsh/llmlingua-2-js GitHub](https://github.com/atjsh/llmlingua-2-js) [6]

| model | size | speed | accuracy |
|-------|------|-------|----------|
| TinyBERT | 57 MB | fastest | good |
| MobileBERT | 99 MB | fast | better |
| BERT | 710 MB | moderate | high |
| XLM-RoBERTa | 2.2 GB | slow | highest |

### dependencies

> "The library requires three primary dependencies: @huggingface/transformers, @tensorflow/tfjs, js-tiktoken."
‚Äî [atjsh/llmlingua-2-js GitHub](https://github.com/atjsh/llmlingua-2-js) [6]

### maintenance status

active development with 148 commits on main, 3 releases, latest in june 2025 [6].

### limitations

> "Unit tests are not available at the moment. E2E tests are only partially implemented."
‚Äî [atjsh/llmlingua-2-js GitHub](https://github.com/atjsh/llmlingua-2-js) [6]

> "The project is labeled as experimental, suggesting ongoing development and potential API changes."
‚Äî [atjsh/llmlingua-2-js GitHub](https://github.com/atjsh/llmlingua-2-js) [6]

### pros

- **native to node.js** ‚Äî aligns with repo ecosystem
- **no python** ‚Äî eliminates subprocess overhead
- **smaller models** ‚Äî tinybert at 57MB is viable for cli
- **browser support** ‚Äî bonus for future web use
- **active development** ‚Äî recent release in 2025 [6]

### cons

- **experimental** ‚Äî api may change, limited test coverage [6]
- **lower adoption** ‚Äî 20 stars vs 5.8k for python [6]
- **no framework integrations** ‚Äî no langchain/llamaindex support
- **dependency size** ‚Äî transformers.js + tensorflow.js add weight

### recommendation

**[PRIMARY]** ‚Äî use for brief.compress; aligns with node.js ecosystem, avoids python subprocess

---

## lever.3: @huggingface/transformers (runtime dependency)

### overview

> "Transformers.js is state-of-the-art machine learning for the web that allows running Transformers directly in your browser with no need for a server."
‚Äî [Hugging Face transformers.js](https://huggingface.co/docs/transformers.js/en/index) [8]

| attribute | value |
|-----------|-------|
| registry | npm |
| package | `@huggingface/transformers` |
| version | v4 preview (2025) [8] |
| license | Apache-2.0 |
| maintainer | hugging face (official) [8] |

### why it matters

the `@atjsh/llmlingua-2` package depends on transformers.js for model inference [6]. this is a well-maintained, official hugging face project.

### capabilities

> "By adopting the com.microsoft.MultiHeadAttention operator, a ~4x speedup was achieved for BERT-based embedding models."
‚Äî [Hugging Face Blog](https://huggingface.co/blog/transformersjs-v3) [9]

> "As of the latest release, the community has converted over 1200 models to be compatible with Transformers.js."
‚Äî [Hugging Face Blog](https://huggingface.co/blog/transformersjs-v3) [9]

### node.js support

> "Although Transformers.js was originally designed to be used in the browser, it's also able to run inference on the server, with Node.js support."
‚Äî [Hugging Face Docs](https://huggingface.co/docs/transformers.js/tutorials/node) [10]

### pros

- **official hugging face** ‚Äî well-maintained, trusted
- **optimized** ‚Äî 4x speedup for bert models [9]
- **wide model support** ‚Äî 1200+ compatible models [9]
- **offline support** ‚Äî v4 enables full offline after initial download [8]

### cons

- **transitive dependency** ‚Äî not direct, via @atjsh/llmlingua-2
- **bundle size** ‚Äî adds weight to node_modules

### recommendation

**[REUSE]** ‚Äî comes bundled with @atjsh/llmlingua-2; no direct action needed

---

## lever.4: alternatives considered

### selective-context

> "Selective-Context, a pioneering prompt compression method, enables LLMs to process 2x more content while saving 36% memory and 32% inference time, using self-information to identify and delete less informative parts of prompts."
‚Äî [Prompt Compression Survey](https://github.com/ZongqianLi/Prompt-Compression-Survey) [11]

| attribute | value |
|-----------|-------|
| compression | 2x (vs 20x for LLMLingua) |
| method | self-information based |
| status | academic, no production package |

**verdict**: rejected ‚Äî lower compression ratio, no maintained package

### RECOMP

> "RECOMP, an extractive compression method, selects documents, sentences, or phrases from the original context without altering them."
‚Äî [Prompt Compression Survey](https://github.com/ZongqianLi/Prompt-Compression-Survey) [11]

| attribute | value |
|-----------|-------|
| method | extractive (sentence selection) |
| use case | document retrieval, not token removal |
| status | academic |

**verdict**: rejected ‚Äî wrong granularity (sentences vs tokens), no package

### gist / soft prompt compression

> "Soft prompt compression methods such as GIST and 500-Compressor exist as alternatives."
‚Äî [Prompt Compression Survey](https://github.com/ZongqianLi/Prompt-Compression-Survey) [11]

| attribute | value |
|-----------|-------|
| method | learned soft tokens |
| requirement | LLM fine-tune access |
| status | academic |

**verdict**: rejected ‚Äî requires model fine-tune, not applicable to instruction briefs

---

## comparison matrix

| lever | type | compression | speed | ecosystem fit | production ready | recommendation |
|-------|------|-------------|-------|---------------|------------------|----------------|
| llmlingua (python) | official | 20x | 0.4s (gpu) | subprocess | yes (langchain, llamaindex) | [CONSIDER] |
| @atjsh/llmlingua-2 | port | 20x | 4-20s (cpu) | native node.js | experimental | [PRIMARY] |
| selective-context | academic | 2x | ‚Äî | none | no | [REJECT] |
| RECOMP | academic | varies | ‚Äî | none | no | [REJECT] |
| gist | academic | high | ‚Äî | requires fine-tune | no | [REJECT] |

---

## integration recommendation

### primary path: typescript via @atjsh/llmlingua-2

```
brief.compress skill (bash)
  ‚îî‚îÄ‚îÄ invokes bun ./compress.ts
      ‚îî‚îÄ‚îÄ uses @atjsh/llmlingua-2
          ‚îî‚îÄ‚îÄ auto-downloads MobileBERT model (99MB)
              ‚îî‚îÄ‚îÄ via @huggingface/transformers
```

**rationale**:
- native to repo ecosystem (node.js, typescript)
- no python subprocess overhead
- mobilebert provides good speed/quality balance
- experimental status acceptable for internal tool

### fallback path: python subprocess

```
brief.compress skill (bash)
  ‚îî‚îÄ‚îÄ shells to python3 -c "..."
      ‚îî‚îÄ‚îÄ uses llmlingua package
          ‚îî‚îÄ‚îÄ downloads huggingface model (~500MB)
```

**when to use fallback**:
- if @atjsh/llmlingua-2 lacks critical feature (e.g., force_tokens edge case)
- if production validation requires official implementation
- if larger model (xlm-roberta) needed for quality

---

## citations

1. [microsoft/LLMLingua GitHub](https://github.com/microsoft/LLMLingua) ‚Äî repository, stars, maintenance, examples
2. [llmlingua PyPI](https://pypi.org/project/llmlingua/) ‚Äî version, python requirements, status
3. [LLMLingua Series](https://llmlingua.com/) ‚Äî framework integrations
4. [LlamaIndex Blog](https://www.llamaindex.ai/blog/longllmlingua-bye-bye-to-middle-loss-and-save-on-your-rag-costs-via-prompt-compression-54b559b9ddf7) ‚Äî llamaindex integration
5. [LangChain Docs](https://docs.langchain.com/oss/python/integrations/retrievers/llmlingua) ‚Äî langchain integration
6. [atjsh/llmlingua-2-js GitHub](https://github.com/atjsh/llmlingua-2-js) ‚Äî repository, dependencies, limitations
7. [@atjsh/llmlingua-2 npm](https://www.npmjs.com/package/@atjsh/llmlingua-2) ‚Äî package, version, api
8. [Hugging Face transformers.js](https://huggingface.co/docs/transformers.js/en/index) ‚Äî overview, v4 preview
9. [Hugging Face Blog](https://huggingface.co/blog/transformersjs-v3) ‚Äî performance, model count
10. [Hugging Face Docs](https://huggingface.co/docs/transformers.js/tutorials/node) ‚Äî node.js support
11. [Prompt Compression Survey](https://github.com/ZongqianLi/Prompt-Compression-Survey) ‚Äî alternatives comparison

---

## summary

| decision | lever | rationale |
|----------|-------|-----------|
| [PRIMARY] | @atjsh/llmlingua-2 | native node.js, avoids subprocess, active dev |
| [CONSIDER] | llmlingua (python) | fallback if js port lacks feature |
| [REUSE] | @huggingface/transformers | bundled via primary lever |
| [REJECT] | selective-context, RECOMP, gist | wrong granularity, no packages, or requires fine-tune |

---

üê¢ one primary lever, one fallback, three rejected ‚Äî shell yeah üåä
