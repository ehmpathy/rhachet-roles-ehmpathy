## the outcome world

### before

one compressor mechanism: llmlingua/v2. it does token classification â€” bert models decide which tokens to drop. fast, local, no api calls. but it's blind to semantics. it prunes tokens by statistical salience, not by grasp of what the brief is *for*. a 4x compressed brief keeps ~92% of tokens that "matter" statistically â€” but may lose the one sentence that made the whole brief click.

### after

two compressor families, same interface, same output:

- `--via llmlingua/v2@tinybert` â€” fast, local, statistical token prune (what we have now)
- `--via bhrain/sitrep@xai/grok/code-fast-1` â€” brain-powered semantic distillation via sitrep template

the brain compressor reads the brief, grasps its objective, and distills it the way a field officer writes a sitrep: strip all but what's decision-critical. the output is a sitrep â€” a compressed document that preserves only what a reader *must know* to act on the brief's intent.

### the "aha" moment

you compress a 120-line practice brief about dependency injection. llmlingua drops filler words and gets you from 3000 tokens to 900 â€” but the result reads like a telegram. the sitrep compressor reads the same brief, understands it teaches *why* deps should be injected, and produces a 400-token distillation that a mechanic can act on immediately â€” with the rule, the pattern, the examples, and zero fluff.

the aha: compression isn't just "fewer tokens." it's "fewer tokens that still land."

### the `$press@$brain` convention

the `@` separator cleanly divides press from brain, across both families:

```
$press@$brain
```

| press | brain | full mech |
|--------|-------|-----------|
| `llmlingua/v2` | `tinybert` | `llmlingua/v2@tinybert` |
| `llmlingua/v2` | `xlm-roberta` | `llmlingua/v2@xlm-roberta` |
| `bhrain/sitrep` | `xai/grok/code-fast-1` | `bhrain/sitrep@xai/grok/code-fast-1` |
| `bhrain/sitrep` | `anthropic/claude/sonnet-4` | `bhrain/sitrep@anthropic/claude/sonnet-4` |

- **unambiguous parse** â€” split on `@`, left = press, right = brain
- **familiar convention** â€” `@` as separator is well-established (npm `pkg@version`, docker `image@digest`)
- **unifies both families** â€” llmlingua and bhrain share the same `press@brain` shape
- **extensible** â€” new presses (e.g., `bhrain/debrief`) or new brains slot in without convention changes

---

## user experience

### usecase 1: compress a single brief with brain

```sh
npx rhachet run --skill brief.compress \
  --from path/to/rule.require.dependency-injection.md \
  --via bhrain/sitrep@xai/grok/code-fast-1
```

output:
```
ðŸ¦‰ lets see...

ðŸ“š brief.compress
   â”œâ”€ mode: plan
   â”œâ”€ mech: bhrain/sitrep@xai/grok/code-fast-1
   â”œâ”€ from: rule.require.dependency-injection.md
   â””â”€ result
      â”œâ”€ tokens.before: 3012
      â”œâ”€ tokens.after: 412
      â””â”€ ratio.actual: 7.3x

note: this was a plan. to apply, re-run with --mode apply
```

apply emits `rule.require.dependency-injection.md.min` â€” same as llmlingua, same collocated output, interchangeable.

### usecase 2: batch compress with glob

```sh
npx rhachet run --skill brief.compress \
  --from "src/domain.roles/mechanic/briefs/**/*.md" \
  --via bhrain/sitrep@xai/grok/code-fast-1 \
  --mode apply
```

### usecase 3: explicit output path

```sh
npx rhachet run --skill brief.compress \
  --from path/to/brief.md \
  --via bhrain/sitrep@xai/grok/code-fast-1 \
  --into path/to/output.md
```

### usecase 4: experiment with different brains

```sh
# try grok code-fast-1
npx rhachet run --skill brief.compress \
  --from path/to/brief.md \
  --via bhrain/sitrep@xai/grok/code-fast-1

# try a different brain
npx rhachet run --skill brief.compress \
  --from path/to/brief.md \
  --via bhrain/sitrep@anthropic/claude/sonnet-4
```

the `$press@$brain` pattern means you swap brains freely while the press stays constant â€” or swap presses while the brain stays constant.

### timeline

1. human runs compress with `--via bhrain/sitrep@...`
2. shell command splits on `@` â€” left = press (`bhrain/sitrep`), right = brain (`xai/grok/code-fast-1`)
3. press prefix `bhrain/` routes to `compress.via.bhrain.ts` (vs `llmlingua/` routes to `compress.via.llmlingua.ts`)
4. `compress.via.bhrain.ts` uses `genBrainAtom` with the brain slug
5. brain receives the sitrep prompt template + the source brief content
6. brain returns the distilled output
7. tokens counted, ratio computed, .min emitted

---

## mental model

### how a human would describe this

"we have two ways to compress briefs. the fast one uses a tiny local model that drops tokens mechanically. the smart one sends the brief to an llm with instructions to write a sitrep â€” keep only what's essential to act on."

### analogy

**llmlingua** = a paper shredder with a filter. it keeps fragments that score high on a salience metric. fast, cheap, but the output is shredded prose.

**bhrain/sitrep** = a field officer who reads the full report and writes a sitrep. slower, costs an api call, but the output is coherent and decision-ready.

### terms

| human term | system term | sense |
|------------|-------------|-------|
| compressor | `--via` | which compression engine to use, in `$press@$brain` format |
| press | left of `@` | the compression approach (e.g., `llmlingua/v2`, `bhrain/sitrep`) |
| brain | right of `@` | which model to invoke (e.g., `tinybert`, `xai/grok/code-fast-1`) |
| sitrep | template name | the prompt pattern that guides brain distillation |
| source brief | `BriefOriginal` | the input document before compression |
| compressed brief | `BriefMinified` / `$path.min` | the output artifact (file suffix: `.min`) |

---

## evaluation

### how well does it solve the goals?

| goal | score | notes |
|------|-------|-------|
| experiment with press/brain combos | high | `$press@$brain` is extensible by design |
| preserve brief intent after compression | high | sitrep template instructs brain to preserve decision-critical content |
| same interface as llmlingua | high | same cli flags, same output format, same .min |
| simple implementation | high | single `compress.via.bhrain.ts` file, no domain.operations needed |

### pros

- higher semantic fidelity than token classification
- extensible to new templates (not just sitrep) and new brains
- reuses established `genBrainAtom` pattern from guardBorder â€” proven path
- output is coherent prose, not fragmented tokens
- `$press@$brain` convention unifies both compressor families under one parse rule

### cons

- requires api key (not fully offline like llmlingua)
- costs per invocation (brain api calls)
- slower than local tinybert (~2-5s per brief vs ~4s)
- output quality depends on prompt quality â€” the sitrep template needs to be solid

### edgecases & pit of success

| edgecase | handle |
|----------|--------|
| no api key configured | failfast with clear message, like guardBorder does |
| brain choice not found | `genBrainAtom` throws `BrainChoiceNotFoundError` with suggestions |
| empty brief | return empty .min with 0 tokens |
| brief with code blocks | sitrep template must instruct brain to preserve code examples |
| `--via bhrain/sitrep` (no `@brain`) | default to `bhrain/sitrep@xai/grok/code-fast-1` |
| `--via @xai/grok/code-fast-1` (no press) | failfast: "expected format $press@$brain" |
| `--into` flag with glob `--from` | forbidden: `--into` only valid for single file input |
| backwards compat: `--via tinybert` (no `@`) | treat as shorthand for `llmlingua/v2@tinybert` (current default behavior) |

### what sitrep means here

sitrep = situation report. a military brief format designed to distill complex operational information into decision-critical content. the format prioritizes: what is the situation, what matters, what action is needed. when applied as a compression template, it instructs the brain to read the full brief, identify its objective, and produce only the content a reader needs to act on that objective â€” no filler, no repetition, no nice-to-haves.
