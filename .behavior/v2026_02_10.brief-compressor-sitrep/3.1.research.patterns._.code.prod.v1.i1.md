# research: production codepath patterns for brief compression

## citations

1. `src/domain.roles/librarian/skills/brief.compress/compress.ts:4` â€” `".what = compress markdown via LLMLingua-2 token classification"`
2. `src/domain.roles/librarian/skills/brief.compress/compress.ts:17-26` â€” `parseArgs` with options `input`, `output`, `mech`, `rate`, `check`, `json`
3. `src/domain.roles/librarian/skills/brief.compress/compress.ts:29-55` â€” `MODEL_CONFIG: Record<string, { id: string; factory: 'bert' | 'xlm-roberta'; options?: Record<string, unknown> }>`
4. `src/domain.roles/librarian/skills/brief.compress/compress.ts:61-65` â€” `const countTokens = async (input: { text: string }): Promise<number> => { const { getEncoding } = await import('js-tiktoken'); const encoder = getEncoding('cl100k_base'); return encoder.encode(input.text).length; }`
5. `src/domain.roles/librarian/skills/brief.compress/compress.ts:71-101` â€” `createCompressor` factory with `config.factory === 'xlm-roberta'` branch for `LLMLingua2.WithXLMRoBERTa` vs `LLMLingua2.WithBERTMultilingual`
6. `src/domain.roles/librarian/skills/brief.compress/compress.ts:107-148` â€” `computeCompression` with `compress_prompt`, BERT artifact cleanup, token count, ratio computation
7. `src/domain.roles/librarian/skills/brief.compress/compress.ts:121-126` â€” `compressor.compress_prompt(input.content, { rate: input.rate, force_tokens: ['\n', '.', '!', '?', ',', '\`\`\`', '#', '*', '-', '|', ':'], chunk_end_tokens: ['.', '\n'], drop_consecutive: true })`
8. `src/domain.roles/librarian/skills/brief.compress/compress.ts:129-136` â€” BERT artifact post-process: `.replace(/\bnew\d+\b/g, '')` and punctuation collapse
9. `src/domain.roles/librarian/skills/brief.compress/compress.ts:211-219` â€” JSON output mode: `JSON.stringify(result)` vs human-readable `tokens.before`, `tokens.after`, `ratio`
10. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:34` â€” `MODE="apply"` (default mode)
11. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:35` â€” `MECH="tinybert"` (default mech)
12. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:150-156` â€” mech validation: `case "$MECH" in tinybert|bert|xlm-roberta) ;; *) echo "error: --mech must be tinybert, bert, or xlm-roberta" >&2; exit 1 ;; esac`
13. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:199-200` â€” rate conversion: `RATE=$(echo "scale=4; 1 / $RATIO" | bc)`
14. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:216` â€” output path: `MIN_FILE="${file}.min"`
15. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:228-236` â€” ts engine invocation: `node "$SKILL_DIR/compress.js" --input "$file" --mech "$MECH" --rate "$RATE" --json`
16. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:258-260` â€” file write: `if [[ "$MODE" == "apply" ]]; then echo "$COMPRESSED" > "$MIN_FILE"; fi`
17. `src/domain.roles/librarian/skills/brief.compress/brief.compress.sh:265` â€” mech label: `print_tree_branch "mech: llmlingua/v2/$MECH"`
18. `src/contract/cli/guardBorder.onWebfetch.ts:2` â€” `import { genBrainAtom } from 'rhachet-brains-xai'`
19. `src/contract/cli/guardBorder.onWebfetch.ts:51` â€” `const brain = genBrainAtom({ slug: 'xai/grok/code-fast-1' })`
20. `src/contract/cli/guardBorder.onWebfetch.ts:55-63` â€” domain operation invocation: `const result = await decideIsContentAdmissibleOnWebfetch({ toolName, toolInput, toolResponse, sessionId }, { brain, quarantineDir })`
21. `src/contract/cli/guardBorder.onWebfetch.ts:28-38` â€” api key failfast: `if (!process.env.XAI_API_KEY) { console.error('...border guard not configured...'); process.exit(2); }`
22. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:1` â€” `import type { BrainAtom } from 'rhachet'`
23. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:9-11` â€” `export interface ContextImagineIsContentAdmissible { brain: BrainAtom; }`
24. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:57-60` â€” `export const imagineIsContentAdmissible = async (input: { content: string }, context: ContextImagineIsContentAdmissible)`
25. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:78-112` â€” `const { output } = await withRetry(withTimeout(async () => context.brain.ask({ role: { briefs: [] }, prompt: '...', schema: { output: z.object({ decision: z.enum([...]), reason: z.string().nullable() }) } }), { threshold: { seconds: 30 } }))();`
26. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:62-64` â€” `const maxChars = computeMaxInspectableChars({ contextTokens: context.brain.spec.gain.size.context.tokens })`
27. `src/domain.roles/librarian/skills/brief.compress/output.sh:17-28` â€” output helpers: `print_turtle_header`, `print_tree_start`, `print_tree_branch`, `print_tree_leaf`
28. `package.json:80` â€” `"rhachet-brains-xai": "0.2.1"`
29. `package.json:108` â€” `"rhachet-brains-anthropic": "0.3.3"`
30. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:2` â€” `import { withRetry, withTimeout } from 'wrapper-fns'`
31. `src/domain.operations/guardBorder/imagineIsContentAdmissible.ts:3` â€” `import { z } from 'zod'`

---

## pattern 1: two-tier skill architecture (shell orchestrator + ts engine) [REUSE]

the compress skill uses a two-tier architecture: a bash orchestrator (`brief.compress.sh`) handles argument parse, file enumeration, staleness checks, output format, and mode gate â€” then delegates the actual compression to a compiled ts engine (`compress.js`) [15].

the shell layer:
- parses named args (`--path`, `--glob`, `--mode`, `--mech`, `--ratio`, `--force`) [10-12]
- validates inputs (mode, mech, ratio range, git repo check) [12]
- enumerates files via glob or single path [14]
- checks staleness (skips if `.min` is newer than source) [14]
- invokes the ts engine via `node "$SKILL_DIR/compress.js" --input "$file" --mech "$MECH" --rate "$RATE" --json` [15]
- parses JSON output from the engine via `jq` [15]
- writes `.min` files in apply mode [16]
- formats output via tree helpers (`print_turtle_header`, `print_tree_start`, `print_tree_branch`, `print_tree_leaf`) [17][27]

the ts engine:
- accepts `--input`, `--output`, `--mech`, `--rate`, `--check`, `--json` via `parseArgs` [2]
- returns structured JSON: `{ compressed, tokensBefore, tokensAfter, ratio }` [9]
- is a standalone executable (`#!/usr/bin/env npx tsx`) [1]

**relevance**: the new `compress.via.bhrain.ts` engine will follow this same two-tier pattern. the shell orchestrator routes to the correct ts engine based on the `--via` press family. the ts engine handles brain invocation and returns the same JSON shape.

---

## pattern 2: model config registry + factory selection [EXTEND]

the current compressor uses a `MODEL_CONFIG` registry that maps mech slugs to model IDs and factory types [3]:

```typescript
const MODEL_CONFIG: Record<string, { id: string; factory: 'bert' | 'xlm-roberta'; options?: Record<string, unknown> }> = {
  tinybert: { id: 'atjsh/llmlingua-2-js-tinybert-meetingbank', factory: 'bert' },
  // ...
  'xlm-roberta': { id: 'atjsh/llmlingua-2-js-xlm-roberta-large-meetingbank', factory: 'xlm-roberta' },
};
```

the factory pattern selects the correct constructor based on `config.factory` [5]:

```typescript
if (config.factory === 'xlm-roberta') {
  const { promptCompressor } = await LLMLingua2.WithXLMRoBERTa(config.id, factoryOptions);
  return promptCompressor;
}
const { promptCompressor } = await LLMLingua2.WithBERTMultilingual(config.id, factoryOptions);
return promptCompressor;
```

**relevance**: this pattern stays within `compress.via.llmlingua.ts` (the renamed file). the new `compress.via.bhrain.ts` does not need a model config registry â€” it receives the brain slug via `--via` and delegates to `genBrainAtom`.

---

## pattern 3: token count via tiktoken [REUSE]

token metrics use `js-tiktoken` with `cl100k_base` encoder [4]:

```typescript
const countTokens = async (input: { text: string }): Promise<number> => {
  const { getEncoding } = await import('js-tiktoken');
  const encoder = getEncoding('cl100k_base');
  return encoder.encode(input.text).length;
};
```

**relevance**: both compressor families must report the same token metrics. the bhrain compressor will reuse this exact function to count `tokensBefore` and `tokensAfter`.

---

## pattern 4: compression result shape [REUSE]

the `computeCompression` function returns a standard shape [6]:

```typescript
Promise<{
  compressed: string;
  tokensBefore: number;
  tokensAfter: number;
  ratio: number;
}>
```

the shell layer expects this shape from the ts engine via `--json` output [9][15].

**relevance**: the bhrain compressor must return the exact same shape. this is the contract between the ts engine and the shell orchestrator. both `compress.via.llmlingua.ts` and `compress.via.bhrain.ts` emit `{ compressed, tokensBefore, tokensAfter, ratio }`.

---

## pattern 5: genBrainAtom + brain.ask for LLM invocation [REUSE]

the guardBorder feature demonstrates the established pattern for brain-powered operations:

**step 1 â€” create brain atom** (at CLI entry point) [18][19]:
```typescript
import { genBrainAtom } from 'rhachet-brains-xai';
const brain = genBrainAtom({ slug: 'xai/grok/code-fast-1' });
```

**step 2 â€” define context interface** (at domain operation) [22][23]:
```typescript
import type { BrainAtom } from 'rhachet';
export interface ContextImagineIsContentAdmissible {
  brain: BrainAtom;
}
```

**step 3 â€” call brain.ask with prompt + schema** [25]:
```typescript
const { output } = await withRetry(
  withTimeout(
    async () => context.brain.ask({
      role: { briefs: [] },
      prompt: `you are a security content filter...`,
      schema: {
        output: z.object({
          decision: z.enum(['allow', 'block']),
          reason: z.string().nullable(),
        }),
      },
    }),
    { threshold: { seconds: 30 } },
  ),
)();
```

**step 4 â€” access brain spec for context limits** [26]:
```typescript
const maxChars = computeMaxInspectableChars({
  contextTokens: context.brain.spec.gain.size.context.tokens,
});
```

**relevance**: the bhrain compressor follows this exact pattern. `compress.via.bhrain.ts` creates a brain atom from the brain slug (right of `@`), then calls `brain.ask` with the sitrep template + source brief content. the vision doc says "without the need to create domain.operations due to the simplicity of the task" â€” so the brain invocation lives directly in the ts engine, not in a separate domain operation.

---

## pattern 6: api key failfast [REUSE]

guardBorder failfasts immediately if the required api key is absent [21]:

```typescript
if (!process.env.XAI_API_KEY) {
  console.error(`
ðŸš« webfetch blocked: border guard not configured

the XAI_API_KEY environment variable is required...
`);
  process.exit(2);
}
```

**relevance**: the bhrain compressor needs the same failfast. if `XAI_API_KEY` (or the relevant key for the brain slug) is absent, failfast with a clear message about what to configure. this maps to usecase.5 in the blackbox criteria.

---

## pattern 7: resilience wrappers (retry + timeout) [REUSE]

brain.ask calls are wrapped with `withRetry` + `withTimeout` from `wrapper-fns` [25][30]:

```typescript
const { output } = await withRetry(
  withTimeout(
    async () => context.brain.ask({ ... }),
    { threshold: { seconds: 30 } },
  ),
)();
```

**relevance**: the bhrain compressor will use the same wrappers. LLM api calls can fail or timeout â€” the resilience pattern ensures transient failures are retried.

---

## pattern 8: zod schema for structured LLM output [REUSE]

brain.ask uses zod to define the expected output shape [25][31]:

```typescript
schema: {
  output: z.object({
    decision: z.enum(['allow', 'block']),
    reason: z.string().nullable(),
  }),
}
```

**relevance**: the bhrain compressor will use zod to define the compressed output shape. this ensures the brain returns structured content, not freeform text.

---

## pattern 9: mech label format in output [EXTEND]

the shell orchestrator formats the mech label as `llmlingua/v2/$MECH` [17]:

```bash
print_tree_branch "mech: llmlingua/v2/$MECH"
```

**relevance**: the new `--via` convention uses `$press@$brain` format. the shell will route based on the press family prefix (`bhrain/` vs `llmlingua/`) and format the label as the full `--via` value (e.g., `bhrain/sitrep@xai/grok/code-fast-1`).

---

## pattern 10: .min file write convention [REUSE]

output files use the `$path.min` convention [14][16]:

```bash
MIN_FILE="${file}.min"
# ...
echo "$COMPRESSED" > "$MIN_FILE"
```

**relevance**: both compressor families emit to the same `$path.min` location. this is a core invariant from the vision doc â€” interchangeable output regardless of press.

---

## pattern 11: BERT artifact post-process [REPLACE]

the llmlingua compressor cleans up BERT vocabulary artifacts after compression [8]:

```typescript
const compressed = rawCompressed
  .replace(/\bnew\d+\b/g, '')
  .replace(/[\s]*[.,-]{2,}[\s.,-]*/g, (match) => match.includes('.') ? '.' : ',')
  .replace(/ {2,}/g, ' ')
  .trim();
```

**relevance**: the bhrain compressor does NOT need this. brain-generated output is coherent prose â€” no BERT artifacts to clean. this post-process step is specific to the extractive (llmlingua) press and stays in `compress.via.llmlingua.ts`.

---

## pattern 12: brain slug resolution via packages [REUSE]

two brain packages are already installed [28][29]:

- `rhachet-brains-xai` (v0.2.1) â€” provides `genBrainAtom` for xai/grok models
- `rhachet-brains-anthropic` (v0.3.3) â€” provides `genBrainAtom` for anthropic/claude models

the vision doc specifies both `xai/grok/code-fast-1` and `anthropic/claude/sonnet-4` as supported brains.

**relevance**: the bhrain compressor resolves the brain slug (right of `@`) to the correct package. `xai/*` slugs use `rhachet-brains-xai`, `anthropic/*` slugs use `rhachet-brains-anthropic`. the `genBrainAtom` call is identical across packages.

---

## pattern 13: shell routes to ts engine via node invocation [EXTEND]

the current shell invokes a single ts engine [15]:

```bash
RAW_OUTPUT=$(node "$SKILL_DIR/compress.js" \
  --input "$file" \
  --mech "$MECH" \
  --rate "$RATE" \
  --json 2>&1)
```

**relevance**: with two compressor families, the shell will route to the correct engine based on the press family:
- `llmlingua/*` â†’ `node "$SKILL_DIR/compress.via.llmlingua.js" ...`
- `bhrain/*` â†’ `node "$SKILL_DIR/compress.via.bhrain.js" ...`

the route logic parses the `--via` value, splits on `@`, and inspects the press prefix.

---

## summary: reuse / extend / replace

| # | pattern | verdict | rationale |
|---|---------|---------|-----------|
| 1 | two-tier skill architecture | **[REUSE]** | same shell+ts structure for bhrain compressor |
| 2 | model config registry + factory | **[EXTEND]** | stays in llmlingua; not needed for bhrain (uses genBrainAtom) |
| 3 | token count via tiktoken | **[REUSE]** | identical token metrics across both presses |
| 4 | compression result shape | **[REUSE]** | same `{ compressed, tokensBefore, tokensAfter, ratio }` contract |
| 5 | genBrainAtom + brain.ask | **[REUSE]** | proven pattern from guardBorder; exact same flow for bhrain |
| 6 | api key failfast | **[REUSE]** | same check, different message for compression context |
| 7 | resilience wrappers | **[REUSE]** | withRetry + withTimeout for LLM api calls |
| 8 | zod schema for LLM output | **[REUSE]** | structured output from brain.ask |
| 9 | mech label format | **[EXTEND]** | from `llmlingua/v2/$MECH` to `$press@$brain` format |
| 10 | .min file write | **[REUSE]** | same output convention for both presses |
| 11 | BERT artifact post-process | **[REPLACE]** | not needed for bhrain; stays in llmlingua only |
| 12 | brain slug resolution | **[REUSE]** | rhachet-brains-xai and rhachet-brains-anthropic already installed |
| 13 | shell routes to ts engine | **[EXTEND]** | add route logic to select engine by press family |
