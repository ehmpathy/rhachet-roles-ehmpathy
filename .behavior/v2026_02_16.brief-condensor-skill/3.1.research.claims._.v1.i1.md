# research claims: condense skill

## prompt compression

### [1] [FACT] LLMLingua achieves 20x compression with 1.5% performance loss

> "LLMLingua was able to retain the reasoning capabilities of LLMs at a 20x compression ratio, with only a 1.5% loss in performance."

source: [Microsoft Research - LLMLingua](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)

---

### [2] [FACT] prompt compression uses perplexity to identify removable tokens

> "Using a well-trained small language model, such as GPT2-small or LLaMA-7B, LLMLingua identifies and removes unimportant tokens from prompts."

> "tokens with lower perplexity contribute less information entropy and can be safely removed"

source: [LLMLingua GitHub](https://github.com/microsoft/LLMLingua)

---

### [3] [FACT] extractive compression outperforms other methods for RAG

> "extractive compression using rerankers performs best, often improving accuracy by filtering noise while achieving 2–10x compression, with extractive reranker-based compression achieving +7.89 F1 points at 4.5x compression"

source: [arXiv - Prompt Compression Survey](https://arxiv.org/abs/2410.12388)

---

### [4] [FACT] LLMLingua-2 is 3-6x faster than LLMLingua

> "LLMLingua-2, a small-size yet powerful prompt compression method trained via data distillation from GPT-4 for token classification with a BERT-level encoder, excels in task-agnostic compression. It surpasses LLMLingua in handling out-of-domain data, offering 3x-6x faster performance."

source: [arXiv - LLMLingua-2](https://arxiv.org/abs/2403.12968)

---

### [5] [SUMP] soft prompting with multi-token samples improves information preservation

> "Training with multi-token samples implemented in PISCO significantly improves grounding and information preservation and is crucial for the ability to integrate information across separate units."

source: [arXiv - Information Preservation in Prompt Compression](https://arxiv.org/abs/2503.19114)

---

### [6] [SUMP] 60% token reduction is achievable while preserving output quality

> "CompactPrompt prunes low-information tokens from prompts using self-information scoring and dependency-based phrase grouping... reduces total token usage and inference cost by up to 60% while preserving output quality."

source: [arXiv - CompactPrompt](https://arxiv.org/html/2510.18043v1)

---

## semantic similarity measurement

### [7] [FACT] jaccard similarity measures set overlap

> "Jaccard Similarity is defined as the ratio of the intersection of the documents to the union of the documents. It is measured as the size of the intersection of two sets divided by the size of their union."

source: [IBM - Jaccard Similarity](https://www.ibm.com/think/topics/jaccard-similarity)

---

### [8] [FACT] jaccard is used for cluster stability measurement

> "The R package scclusteval implements all steps of the pipeline: subsampling the cells, repeating the clustering with Seurat and estimation of cluster stability using the Jaccard similarity index"

source: [PubMed - Cluster Stability](https://pubmed.ncbi.nlm.nih.gov/33165513/)

---

### [9] [FACT] semantic similarity measures meaning preservation percentage

> "Semantic similarity measures how well compressed text preserves the original meaning using word embeddings... a score of 0.892 indicates 89.2% of semantic meaning preserved."

source: [PyPI - context-compressor](https://pypi.org/project/context-compressor/)

---

### [10] [OPIN] multiple metrics together evaluate compression quality

> "Beyond semantic similarity and ROUGE scores, compression quality evaluation includes entity preservation rate and readability score metrics."

source: [MDPI - Text Similarity Survey](https://www.mdpi.com/2078-2489/11/9/421)

---

## concept/kernel extraction

### [11] [FACT] keyphrase extraction uses TF-IDF and graph-based methods

> "Term Frequency-Inverse Document Frequency (TF-IDF) assigns importance to words based on their frequency within a document and their rarity across a corpus, with phrases having high TF-IDF scores considered keyphrases."

> "TextRank is a graph-based ranking algorithm for text documents where the text is split into tokens representing nodes, connected with weighted edges based on lexical or semantic relations"

source: [Baeldung - Keyphrase Extraction](https://www.baeldung.com/cs/automatic-keyword-keyphrase-extraction)

---

### [12] [FACT] KeyBERT uses BERT embeddings for keyphrase extraction

> "KeyBERT is a keyword extraction library leveraging embeddings from BERT-like models, using BERT-embeddings and simple cosine similarity to find sub-phrases in a document most similar to the document itself."

source: [Medium - KeyBERT](https://medium.com/nlplanet/two-minutes-nlp-keyword-and-keyphrase-extraction-with-keybert-a9994b06a83)

---

### [13] [FACT] extraction has candidate identification and selection phases

> "The process of automatic keyphrase extraction can be split into candidate identification and keyphrase selection, first identifying a set of words and phrases that can potentially convey the topical content of a text, then ranking them based on a scoring mechanism"

source: [Wikipedia - Keyword Extraction](https://en.wikipedia.org/wiki/Keyword_extraction)

---

## LLM output consistency

### [14] [FACT] LLMs show up to 10% accuracy variation across identical runs

> "A study of 6 deterministically configured LLMs across 8 common tasks with 5 identical runs showed accuracy variations up to 10%."

source: [arXiv - LLM Stability](https://arxiv.org/html/2408.04667v2)

---

### [15] [FACT] models are not deterministic even with temperature=0

> "Interestingly, models are not deterministic even with a temperature of 0."

source: [arXiv - LLM Stability](https://arxiv.org/html/2408.04667v2)

---

### [16] [SUMP] temperature ≤0.3 recommended for consistency

> "LLMs generate different responses based on temperature and sampling strategies, with temperature≤0.3 recommended for consistency."

source: [Keywords AI - LLM Consistency 2025](https://www.keywordsai.co/blog/llm_consistency_2025)

---

### [17] [FACT] self-consistency uses multiple runs to find reliable answers

> "Self-consistency generates multiple reasoning paths and selects the most consistent result, with the model running the prompt several times and identifying the most reliable answer."

source: [CodeSignal - Model Consistency](https://codesignal.com/learn/courses/behavioral-benchmarking-of-llms/lessons/measuring-model-consistency-across-reruns)

---

### [18] [SUMP] cloud APIs show run-to-run drift

> "cloud APIs and self-hosted stacks show run-to-run drift due to sampling, batching/scheduling, and numeric quirks"

source: [Keywords AI - LLM Consistency 2025](https://www.keywordsai.co/blog/llm_consistency_2025)

---

## abstractive summarization

### [19] [FACT] abstractive summarization generates new sentences

> "Unlike extractive summarization which pulls direct sentences from the original text, abstractive summarization generates entirely new sentences that encapsulate the main ideas of the source material"

source: [arXiv - Abstractive Text Summarization](https://arxiv.org/html/2409.02413v1)

---

### [20] [KHUE] how to identify and preserve key information while compressing?

> "Identifying and preserving the key information of a source article while generating a summary in a shorter format is the foremost challenge in this field."

source: [arXiv - Abstractive Text Summarization](https://arxiv.org/html/2409.02413v1)

---

### [21] [OPIN] structure-infused methods improve concept retention

> "The structure-infused copy mechanism incorporates semantic information such as named entities and key phrases along with a graph-based representation of the input text's syntactic structure, and the model can more accurately detect salient information and thus generate summaries that more accurately capture the major concepts"

source: [arXiv - Abstractive Text Summarization](https://arxiv.org/html/2409.02413v1)

---

### [22] [SUMP] key information should guide compression process

> "The abstractive summarization model should contain key information extraction and then use the key information to dynamically guide the process of summary generation."

source: [SpringerOpen - Key Information Guide Network](https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-020-00674-7)

---

### [23] [OPIN] topic-based summarization reduces information loss

> "Summarizing text by topics helps maintain a logical flow and reduce information loss, and it prioritizes the retention of crucial information."

source: [AWS - Document Summarization Techniques](https://aws.amazon.com/blogs/machine-learning/techniques-for-automatic-summarization-of-documents-using-language-models/)

---

## relevance to condense skill

### [24] [KHUE] what stability threshold indicates a brief is too ambiguous?

the condense skill uses meanJaccard ≥0.7 as the threshold. is this empirically validated, or should it be configurable?

---

### [25] [KHUE] should kernel extraction use consensus across runs?

given that LLMs show 10% variance even with temperature=0, consensus-based extraction (multiple runs, vote on kernels) may be necessary for stability. what is the optimal number of runs?

---

### [26] [SUMP] kernel injection guides compression to retain concepts

the condense skill's `req-kernels` brief injects extracted kernels into the compression prompt. this aligns with research that key information should guide the compression process [22].

---

### [27] [OPIN] multiple variance runs trade latency for reliability

the condense skill's `--attempts` parameter (default 3) follows the self-consistency pattern [17] — multiple runs reveal stability and reduce variance. the cost is N× latency.

---

## summary

| type | count |
|------|-------|
| [FACT] | 14 |
| [SUMP] | 6 |
| [KHUE] | 3 |
| [OPIN] | 4 |
