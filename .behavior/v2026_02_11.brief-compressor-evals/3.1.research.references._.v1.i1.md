# research references: brief compressor evals

## 1. core implementation references

### [REF.1] rhachet brain api

**what**: the brain api used for abstractive compression via bhrain/sitrep

**source**: internal — `rhachet` package

**key concepts**:
- `genContextBrain({ choice: { atom: brainSlug } })` — resolve brain by slug
- `brain.choice.ask({ role: { briefs }, prompt, schema })` — structured output with briefs
- briefs passed via `role.briefs` influence brain behavior

**usage in evals**:
```ts
const contextBrain = await genContextBrain({
  choice: { atom: 'xai/grok/code-fast-1' },
});

const { output } = await contextBrain.brain.choice.ask({
  role: { briefs: [sitrepBrief, tscBrief] },
  prompt: compressionPrompt,
  schema: { output: z.object({ sitrep: z.string() }) },
});
```

---

### [REF.2] test-fns bdd framework

**what**: behavior-driven test framework for structured test output

**source**: `test-fns` package — https://www.npmjs.com/package/test-fns

**key concepts**:
- `given()` — describe scenario context
- `when()` — describe action or event
- `then()` — assert expected outcome
- `useBeforeAll()` — shared setup without `let` declarations

**usage in evals**:
```ts
given('[perfeval] methodology brief combination comparison', () => {
  when('[t0] all combinations tested', () => {
    then('stats are computed per combination', () => {
      expect(combinationStats.length).toBe(4);
    });
  });
});
```

---

### [REF.3] sitrep methodology

**what**: situation report compression methodology

**source**: internal — `briefs/sitrep.methodology.md`

**key concepts**:
- sitrep = situation report — distill to decision-critical content
- preserve: rules, constraints, examples that affect decisions
- discard: verbose explanations, redundant phrases, filler

**usage in evals**:
- passed as first methodology brief in `[sitrep]` and `[sitrep, tsc]` combinations
- tests whether sitrep alone achieves target compression

---

### [REF.4] tsc methodology

**what**: telegraphic semantic compression methodology

**source**: internal — `briefs/tsc.methodology.md`

**key concepts**:
- tsc = telegraphic semantic compression
- compress via removal of function words while semantic content is preserved
- applies telegram-style brevity to technical documentation

**usage in evals**:
- passed as methodology brief in `[sitrep, tsc]`, `[tsc, sitrep]`, `[sitrep, tsc, sitrep]`
- tests whether tsc combined with sitrep improves compression ratio

---

## 2. statistical analysis references

### [REF.5] coefficient of variation (CV)

**what**: measure of relative variability for LLM output consistency

**source**: standard statistics — referenced in LLM evaluation literature [10]

**formula**:
```
CV = stddev / mean
```

**usage in evals**:
- target CV < 0.5 for acceptable variance (50% of mean)
- lower CV = more consistent compression across runs

**interpretation**:
| CV range | interpretation |
|----------|----------------|
| < 0.05 | highly stable |
| 0.05-0.15 | stable |
| 0.15-0.30 | moderate variance |
| > 0.30 | high variance |

---

### [REF.6] paired comparison methodology

**what**: technique for fair comparison of methodology combinations

**source**: statistical methodology — referenced in [11]

**key concepts**:
- same input briefs tested across all combinations
- same brain provider for all compressions
- multiple runs per combination for statistical significance
- comparison via mean compression ratio

**usage in evals**:
- each of 4 brief types tested with each of 4 combinations
- 3 runs per combination × brief pair = 48 total compressions
- results grouped by combination for fair comparison

---

## 3. compression metrics references

### [REF.7] compression ratio calculation

**what**: measure of compression effectiveness

**formula**:
```
ratio = tokens_before / tokens_after
```

**interpretation**:
| ratio | result |
|-------|--------|
| 1.0x | no compression |
| 2.0x | 50% token reduction |
| 4.0x | 75% token reduction |
| 10.0x | 90% token reduction |

**usage in evals**:
- primary metric for combination comparison
- target: at least 2.0x (per compression prompt)

---

### [REF.8] token count via tiktoken

**what**: accurate token count for compression measurement

**source**: `tiktoken` or equivalent tokenizer

**key concepts**:
- use same tokenizer for before/after measurement
- cl100k_base encoder for gpt-4 compatibility
- token count = basis for ratio calculation

**usage in evals**:
```ts
const tokensBefore = await countTokens({ text: input.content });
const tokensAfter = await countTokens({ text: compressed });
const ratio = tokensBefore / tokensAfter;
```

---

## 4. parallel execution references

### [REF.9] Promise.all for parallel brain calls

**what**: concurrent execution of independent compression tasks

**source**: javascript standard — `Promise.all()`

**key concepts**:
- all compression tasks are independent (no shared state)
- parallel execution minimizes total wall-clock time
- collect all results for aggregate analysis

**usage in evals**:
```ts
const compressionTasks = combinations.flatMap((combo) =>
  briefs.flatMap((brief) =>
    runs.map((runIndex) =>
      runSingleCompression({ combo, brief, runIndex })
    )
  )
);

const allResults = await Promise.all(compressionTasks);
```

**benefit**:
- 48 compressions in parallel vs sequential
- total time ≈ max(individual times) vs sum(individual times)

---

## 5. brief type references

### [REF.10] brief type taxonomy

**what**: categories of briefs in the mechanic role

**source**: internal — `src/domain.roles/mechanic/briefs/`

**types**:
| type | description | example |
|------|-------------|---------|
| rule | prescriptive constraint | `rule.require.input-context-pattern` |
| concept | explanatory definition | `dependency-injection` |
| lesson | tutorial/howto guide | `howto.write-bdd` |
| tactic | strategic pattern | `idempotency` |

**usage in evals**:
- one brief per type in sample for representative coverage
- results grouped by type to detect type-specific compression behavior

---

## 6. output format references

### [REF.11] comparison table format

**what**: visual format for methodology comparison results

**format**:
```
┌─────────────────────────┬──────────┬──────────┬──────────┬──────────┐
│ combination             │ mean     │ min      │ max      │ stddev   │
├─────────────────────────┼──────────┼──────────┼──────────┼──────────┤
│ [tsc, sitrep]           │ 2.31x    │ 1.89x    │ 2.74x    │ 0.28     │
└─────────────────────────┴──────────┴──────────┴──────────┴──────────┘
```

**columns**:
- combination: methodology brief order
- mean: average compression ratio across runs
- min/max: range of observed ratios
- stddev: standard deviation (variance measure)

**sort order**: by mean ratio, highest first (best compression)

---

## citations index

| ref | name | type | source |
|-----|------|------|--------|
| REF.1 | rhachet brain api | internal | `rhachet` package |
| REF.2 | test-fns bdd | external | npm: test-fns |
| REF.3 | sitrep methodology | internal | briefs/sitrep.methodology.md |
| REF.4 | tsc methodology | internal | briefs/tsc.methodology.md |
| REF.5 | coefficient of variation | standard | statistics |
| REF.6 | paired comparison | standard | statistics |
| REF.7 | compression ratio | standard | information theory |
| REF.8 | token count | external | tiktoken |
| REF.9 | Promise.all | standard | javascript |
| REF.10 | brief type taxonomy | internal | mechanic role |
| REF.11 | comparison table | internal | vision spec |
