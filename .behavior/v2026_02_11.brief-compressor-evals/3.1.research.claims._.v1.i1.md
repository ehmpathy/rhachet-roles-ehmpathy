# research claims: brief compressor evals

## 1. prompt compression performance

**note**: our use case (bhrain/sitrep) uses **abstractive compression** — an LLM semantically distills briefs into shorter form. this differs from extractive methods (like LLMLingua) which select tokens from the original text.

### [FACT] compression ratios of 5-20x are achievable with minimal performance loss

> "Three core techniques — summarization, keyphrase extraction, and semantic chunking — can achieve 5–20x compression while maintaining or improving accuracy, translating to 70–94% cost savings in production AI systems."

— [Prompt Compression Techniques (Medium)](https://medium.com/@kuldeep.paul08/prompt-compression-techniques-reducing-context-window-costs-while-improving-llm-performance-afec1e8f1003) [1]

### [FACT] LLM performance degrades as context length increases

> "Research from Stanford demonstrates that LLM performance drops 15–47% as context length increases, a phenomenon known as 'lost in the middle.'"

— [Prompt Compression Survey (NAACL 2025)](https://arxiv.org/abs/2410.12388) [3]

---

## 2. extractive vs abstractive compression (why we chose abstractive)

### [FACT] extractive compression often outperforms abstractive at moderate ratios

> "Extractive compression often outperforms all other approaches, and enables up to 10× compression with minimal accuracy degradation."

— [Characterizing Prompt Compression Methods](https://arxiv.org/html/2407.08892v1) [4]

### [FACT] extractive compression can improve accuracy by noise reduction

> "Extractive reranker-based compression achieved +7.89 F1 points on 2WikiMultihopQA at 4.5x compression — compression actually improved accuracy by filtering noise. In contrast, abstractive compression at similar ratios decreased performance by 4.69 F1 points."

— [Characterizing Prompt Compression Methods](https://arxiv.org/html/2407.08892v1) [4]

### [FACT] abstractive compression excels at high compression ratios

> "Abstractive LLMs excel at high compression ratios. Unlike extractive methods, they can rephrase and condense information beyond the original tokens, preserving key semantics."

— [Characterizing Prompt Compression Methods](https://arxiv.org/html/2407.08892v1) [4]

### [SUMP] task type determines optimal compression style

> "The retrieval QA task prefers compressed prompts that are extractive and fluent, whereas CoT reasoning favors compressed prompts that are abstractive and maintain a structured format."

— [Style-Compress (ACL)](https://aclanthology.org/2024.findings-emnlp.851/) [5]

### [OPIN] extractive is the safe default for most use cases

> "Start with extractive compression for 80% of use cases — safest, fastest, often accuracy-improving."

— [Prompt Compression Techniques (Medium)](https://medium.com/@kuldeep.paul08/prompt-compression-techniques-reducing-context-window-costs-while-improving-llm-performance-afec1e8f1003) [1]

---

## 3. instruction order in prompts

### [FACT] order of instructions can impact model output

> "The order in which you present information to the model might impact the output. Whether you put instructions before your content ('summarize the following…') or after ('summarize the above…') can make a difference in output."

— [Lakera Prompt Engineering Guide](https://www.lakera.ai/blog/prompt-engineering-guide) [6]

### [FACT] few-shot example order exhibits recency bias

> "The order of few-shot examples can matter, a phenomenon referred to as recency bias."

— [Lakera Prompt Engineering Guide](https://www.lakera.ai/blog/prompt-engineering-guide) [6]

### [FACT] newer models (GPT-4) show reduced order sensitivity

> "In contrast to previous model versions (GPT-3 and prior), testing showed that the model response with ChatGPT and GPT-4 models was the same regardless of whether the technique is utilized."

— [Microsoft Azure Prompt Engineering](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering) [7]

### [SUMP] task-first ordering produces higher quality outputs

> "Telling the model the task you want it to do at the beginning of the prompt, before sharing additional contextual information or examples, can help produce higher-quality outputs."

— [Microsoft Azure Prompt Engineering](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering) [7]

---

## 4. llm output variance and evaluation

### [FACT] LLMs exhibit high intrinsic stochasticity

> "LLMs exhibit high intrinsic stochasticity, especially on challenging math problems, leading to instability and significant output variance. Due to the probabilistic nature of LLMs, setting a fixed seed does not entirely eliminate the variance in responses across multiple runs."

— [Variance-Aware LLM Annotation](https://arxiv.org/html/2601.02370) [8]

### [FACT] multiple runs reduce variance in evaluation

> "Re-sampling answers from the same model multiple times for each question and calculating an average score for each question helps to reduce the random component in individual scores, leading to a smaller standard error and a more accurate representation of the model's true capabilities."

— [Statistical LLM Evaluations (Medium)](https://medium.com/@sulbha.jindal/statistical-llm-evaluations-confidence-scoring-caa6c9d57656) [9]

### [OPIN] target coefficient of variation < 0.05 for stable evaluation

> "Run your complete evaluation pipeline 10 times on identical data with all randomness fixed, calculate the coefficient of variation, and target CV < 0.05 — if your mean score is 80%, variation across runs should not exceed 4 percentage points."

— [The Statistical Reality of LLM Evaluation (Medium)](https://medium.com/@juanc.olamendy/the-statistical-reality-of-llm-evaluation-what-works-what-doesnt-and-when-it-matters-7d9ba6ecdfca) [10]

### [SUMP] paired-difference analysis provides free variance reduction

> "Paired-difference analysis is a powerful technique for comparing the performance of AI models, particularly when accounting for question variance and providing 'free' variance reduction by leveraging the correlation between model performances on shared questions."

— [Data-Driven LLM Evaluation (Towards AI)](https://pub.towardsai.net/data-driven-llm-evaluation-with-statistical-testing-004b1561793f) [11]

---

## 5. open questions for our evals

### [KHUE] does methodology brief order affect compression quality?

research shows instruction order can matter for LLM outputs [6], but newer models show reduced sensitivity [7]. our evals should test whether [sitrep, tsc] vs [tsc, sitrep] produces different compression ratios.

### [KHUE] does brief repetition (reinforcement) improve compression?

no direct research found. our eval of [sitrep, tsc, sitrep] tests whether repetition helps the brain adhere more closely to methodology.

### [KHUE] how many runs are needed for statistical confidence?

research suggests 10 runs for CV < 0.05 [10], but 3 runs may suffice for relative comparison if variance is acceptable.

### [KHUE] is abstractive compression (bhrain/sitrep) appropriate for brief compression?

research suggests abstractive excels at high compression ratios [4] but may lose factual precision [1]. our briefs contain rules and code — factual precision matters.

---

## citations

1. [Prompt Compression Techniques (Medium)](https://medium.com/@kuldeep.paul08/prompt-compression-techniques-reducing-context-window-costs-while-improving-llm-performance-afec1e8f1003)
2. [LLMLingua (Microsoft Research)](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)
3. [Prompt Compression Survey (NAACL 2025)](https://arxiv.org/abs/2410.12388)
4. [Characterizing Prompt Compression Methods](https://arxiv.org/html/2407.08892v1)
5. [Style-Compress (ACL)](https://aclanthology.org/2024.findings-emnlp.851/)
6. [Lakera Prompt Engineering Guide](https://www.lakera.ai/blog/prompt-engineering-guide)
7. [Microsoft Azure Prompt Engineering](https://learn.microsoft.com/en-us/azure/ai-foundry/openai/concepts/prompt-engineering)
8. [Variance-Aware LLM Annotation](https://arxiv.org/html/2601.02370)
9. [Statistical LLM Evaluations (Medium)](https://medium.com/@sulbha.jindal/statistical-llm-evaluations-confidence-scoring-caa6c9d57656)
10. [The Statistical Reality of LLM Evaluation (Medium)](https://medium.com/@juanc.olamendy/the-statistical-reality-of-llm-evaluation-what-works-what-doesnt-and-when-it-matters-7d9ba6ecdfca)
11. [Data-Driven LLM Evaluation (Towards AI)](https://pub.towardsai.net/data-driven-llm-evaluation-with-statistical-testing-004b1561793f)
