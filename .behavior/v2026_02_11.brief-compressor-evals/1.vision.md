# vision: brief compressor evals framework

## the outcome world

### before

- developers tweak methodology briefs blindly â€” no data on which combination compresses best
- compression quality varies but nobody knows why or by how much
- each manual test costs tokens and time; systematic comparison is impractical
- "does sitrep before tsc work better than tsc before sitrep?" â€” no answer, just guesses

### after

- run `npm run test:integration -- compress.via.bhrain.perfeval` and get a clear comparison table
- each methodology brief combination tested against the same representative sample
- parallel brain calls = fast turnaround (minutes, not hours)
- data-driven decisions: "sitrep alone achieves 1.8x; sitrep+tsc achieves 2.1x"

### the "aha" moment

> "we ran the evals overnight and discovered that [tsc, sitrep] produces 15% better compression than [sitrep, tsc] â€” the order matters!"

## user experience

### usecases

1. **experiment with new methodology briefs** â€” add a new compression strategy, run evals, see if it improves the baseline
2. **validate changes don't regress** â€” before merge, confirm compression ratio stays within acceptable range
3. **find optimal configuration** â€” systematically compare all combinations to find the best performer

### contract

**inputs:**
- methodology brief combinations to compare (e.g., `[sitrep]`, `[sitrep, tsc]`, `[tsc, sitrep]`)
- sample briefs from mechanic role (representative of real usage)
- number of runs per combination (default: 3)
- brain provider (e.g., xai/grok)

**outputs:**
- compression ratio per combination (mean, min, max, stddev)
- token counts (before, after)
- duration metrics (time per combination, total time)
- comparison table sorted by compression ratio

### what it looks like

```sh
npm run test:integration -- compress.via.bhrain.perfeval

# output:
ðŸ“Š methodology brief comparison:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ combination             â”‚ mean     â”‚ min      â”‚ max      â”‚ stddev   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ [tsc, sitrep]           â”‚ 2.31x    â”‚ 1.89x    â”‚ 2.74x    â”‚ 0.28     â”‚
â”‚ [sitrep, tsc, sitrep]   â”‚ 2.18x    â”‚ 1.76x    â”‚ 2.61x    â”‚ 0.31     â”‚
â”‚ [sitrep, tsc]           â”‚ 2.05x    â”‚ 1.68x    â”‚ 2.42x    â”‚ 0.24     â”‚
â”‚ [sitrep]                â”‚ 1.72x    â”‚ 1.45x    â”‚ 1.99x    â”‚ 0.18     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â±ï¸  duration: 47s (parallel brain calls)
ðŸ“ sample: 5 briefs Ã— 4 combinations Ã— 3 runs = 60 compressions
```

### timeline

1. **setup** â€” define which combinations to compare
2. **execute** â€” parallel brain calls compress each brief with each combination
3. **aggregate** â€” collect stats across runs and brief types
4. **report** â€” display comparison table with clear winner

## mental model

### the key concept

> **kernels retain, density rises**

good compression keeps the kernels (signal) while it removes tokens (noise). semantic density = kernels / tokens. when compression works well:
- tokens drop (noise removed)
- kernels stay (signal preserved)
- density rises (more signal per token)

this is the core metric that powers compression quality evaluation: max signal to min noise.

### how users describe it

> "it's like a/b test for compression strategies â€” we throw the same briefs at different methodology combinations and see which one wins"

### analogies

- **benchmark suite** â€” like performance benchmarks for code
- **taste test** â€” blind comparison where the best compression wins
- **compiler flags** â€” like `-O2` vs `-O3`, we find which "flags" (briefs) produce best output

### terms

| user term | internal term |
|-----------|---------------|
| "compression strategy" | methodology brief combination |
| "test briefs" | representative sample |
| "runs" | iterations for statistical significance |
| "winner" | highest mean compression ratio |

## evaluation

### how well it solves the goals

| goal | solved? |
|------|---------|
| compare brief combinations | âœ… systematic comparison with stats |
| find best compression rate | âœ… ranked table shows clear winner |
| representative sample | âœ… tests against multiple mechanic brief types |
| parallel execution | âœ… brain calls run concurrently |
| report duration | âœ… total and per-combination time |

### pros

- **data-driven** â€” no more guess work on which combination works best
- **reproducible** â€” same test, same sample, comparable results
- **fast feedback** â€” parallel execution minimizes wait time
- **extensible** â€” easy to add new combinations to compare

### cons

- **token cost** â€” each run costs api tokens (mitigated by limited sample size)
- **variance** â€” llm outputs vary; need multiple runs for statistical confidence
- **sample bias** â€” results depend on which briefs are in the sample

### edge cases and pit of success

| edge case | how we handle it |
|-----------|------------------|
| brain api timeout | withTimeout wrapper, retry logic |
| empty compression output | fail-fast with clear error |
| single brief in sample | warn that sample may not be representative |
| identical results | report tie, suggest more runs |

### success criteria

the framework succeeds when:
1. a developer can run one command and know which combination compresses best
2. results are statistically meaningful (multiple runs, multiple briefs)
3. execution completes in under 5 minutes for typical sample sizes
4. output is clear enough to make a decision without further analysis
