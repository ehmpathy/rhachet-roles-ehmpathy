# research: access requirements for websearch.border.guard

## lesson.1 = Claude Code hooks system

### what it is

Claude Code provides a hooks system that allows interception of tool execution at various lifecycle points. PostToolUse hooks fire after a tool has executed and can inspect or modify the result before it reaches the agent.

### contract

hooks are configured in `.claude/settings.json`:

```json
{
  "hooks": {
    "PostToolUse": [
      {
        "matcher": "WebFetch|WebSearch",
        "hooks": [
          {
            "type": "command",
            "command": "path/to/border-guard.sh"
          }
        ]
      }
    ]
  }
}
```

hook input receives via stdin:
- `tool_name`: the tool that executed
- `tool_input`: the arguments sent to the tool
- `tool_response`: the result returned by the tool

hook output can return:
- `decision: "block"` with `reason` to prevent content from reaching agent
- exit code 2 to block the operation

### citations

[1] Claude Code Hooks Reference - https://docs.claude.com/en/docs/claude-code/hooks

> "PostToolUse hooks fire after a tool has already executed successfully. The input includes both tool_input (the arguments sent to the tool) and tool_response (the result it returned)."

[2] Claude Blog - How to Configure Hooks (December 2025) - https://claude.com/blog/how-to-configure-hooks

> "PostToolUse hooks run after Claude completes an action, making them perfect for cleanup tasks like formatting code, running tests, or logging what happened."

> "PostToolUse hooks can provide feedback to Claude after tool execution. They can return a decision of 'block' along with a reason and additional context for Claude."

[3] Claude Code Hooks Reference - https://docs.claude.com/en/docs/claude-code/hooks

> "In addition to Bash command hooks (type: 'command'), Claude Code supports prompt-based hooks (type: 'prompt') that use an LLM to evaluate whether to allow or block an action."

---

## lesson.2 = xAI Grok API for border guard brain.atom

### what it is

xAI provides fast, low-cost LLM models suitable for content inspection. Grok 3 Mini and Grok 4.1 Fast offer excellent price/performance for simple classification tasks like security content review.

### contract

REST API at `https://api.x.ai/v1/chat/completions` with OpenAI-compatible interface.

authentication: bearer token via `XAI_API_KEY` environment variable.

### price (as of 2025-2026)

| model | input cost | output cost | context | notes |
|-------|------------|-------------|---------|-------|
| grok-3-mini | $0.30/1M tokens | $0.50/1M tokens | 131K | best value for classification |
| grok-4.1-fast | $0.20/1M tokens | $0.50/1M tokens | 2M | larger context window |
| grok-4 | $3.00/1M tokens | $15.00/1M tokens | 256K | full capability |

for border guard use case:
- input: ~500 tokens (truncated content sample)
- output: ~50 tokens (allow/block decision + reason)
- estimated cost per review: ~$0.0002 (grok-3-mini)

### citations

[4] xAI Models and Pricing - https://docs.x.ai/docs/models

> "Grok 3 Mini ($0.30/$0.50 per million tokens) actually outperforms Grok 3 ($3/$15) in benchmarks while costing 90% less."

[5] xAI on X (December 2024) - https://x.com/xai/status/1868045132760842734

> "Pricing is now $2/1M input tokens" (for grok-2-1212)

[6] Grok Review 2026 - https://hackceleration.com/grok-review/

> "Grok 4.1 Fast and Grok 4 Fast models support 2M token context windows - among the largest available."

---

## lesson.3 = rhachet-brains pattern (internal)

### what it is

rhachet-brains-* packages provide a standardized `brain.atom` interface for LLM interactions. `rhachet-brains-anthropic` is already a dev dependency in this repo.

### contract (inferred from package.json)

the pattern appears to be:
- `rhachet-brains-{provider}` packages
- expose a `brain.atom` interface for single LLM calls
- provider-specific: `rhachet-brains-anthropic`, `rhachet-brains-xai` (hypothetical)

for border guard, would need:
- `rhachet-brains-xai` package (to be created or sourced)
- or direct xAI API integration via openai-compatible SDK

### citations

[7] package.json line 99

> `"rhachet-brains-anthropic": "0.3.0"`

---

## lesson.4 = filesystem quarantine access

### what it is

quarantine directory `.quarantine/` stores blocked content for human inspection. requires:
- write access to create quarantine files
- .gitignore to prevent accidental commit
- read denial for claude agent (via permissions denylist)

### contract

filesystem operations via Node.js `fs` module or shell commands.

access control via Claude Code permissions denylist in `.claude/settings.json`:
```json
{
  "permissions": {
    "deny": [
      "Read(.quarantine/*)"
    ]
  }
}
```

### citations

[8] Claude Code Settings Reference

> permissions denylist prevents specified tool patterns from execution without additional hooks or scripts

---

## summary: access requirements

| resource | interface | authentication | cost |
|----------|-----------|----------------|------|
| Claude Code hooks | `.claude/settings.json` | none (local) | free |
| xAI Grok API | REST API (OpenAI-compatible) | `XAI_API_KEY` | ~$0.0002/review |
| filesystem (quarantine) | Node.js fs / shell | none (local) | free |
