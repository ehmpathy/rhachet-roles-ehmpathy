# roadmap: kernelize skill

## phase 0: eject extractKernels to shared location

### prereqs
- [ ] read `.behavior/v2026_02_13.kernelize/3.3.blueprint.v1.i1.md` (blueprint)

### checklist
- [ ] create `src/domain.operations/kernelize/` directory
- [ ] move `extractKernels.ts` from `brief.compress/` to `domain.operations/kernelize/`
- [ ] update imports in `brief.compress/compress.via.bhrain.ts`
- [ ] update imports in `brief.compress/extractKernels.integration.test.ts`
- [ ] update imports in `brief.compress/compress.via.bhrain.perfeval.integration.test.ts`

### acceptance
- [ ] `npm run test:types` passes
- [ ] `npm run test:integration -- extractKernels` passes
- [ ] `npm run test:integration -- compress.via.bhrain` passes

### verification
```sh
npm run test:types && npm run test:integration -- extractKernels && npm run test:integration -- compress.via.bhrain
```

---

## phase 1: create kernelize skill (single mode)

### prereqs
- [ ] phase 0 complete
- [ ] read `.behavior/v2026_02_13.kernelize/1.vision.md` (vision)
- [ ] read `.behavior/v2026_02_13.kernelize/2.1.criteria.blackbox.md` (blackbox criteria)

### checklist
- [ ] create `src/domain.roles/librarian/skills/kernelize/` directory
- [ ] create `kernelize.sh` shell entrypoint
- [ ] create `kernelize.ts` typescript implementation
- [ ] create `output.sh` turtle vibes helpers
- [ ] implement `runKernelize()` for single extraction mode
- [ ] implement `--from` argument
- [ ] implement `--into` argument (optional output path)
- [ ] implement `--mode plan|apply` (plan = preview, apply = emit)

### acceptance
- [ ] `npx rhachet run --skill kernelize --from path/to/brief.md` returns json output
- [ ] output contains `source`, `kernelCount`, `kernels`, `rationale`
- [ ] each kernel has `id`, `concept`, `category`
- [ ] `--into` writes json to specified path
- [ ] empty brief returns `kernelCount: 0`

### verification
```sh
npm run build && npx rhachet roles link --role mechanic
npx rhachet run --skill kernelize --from src/domain.roles/mechanic/briefs/practices/code.prod/evolvable.procedures/rule.require.input-context-pattern.md.pt1.md
```

---

## phase 2: add consensus parameters

### prereqs
- [ ] phase 1 complete
- [ ] read `.behavior/v2026_02_13.kernelize/3.1.research.claims._.v1.i1.md` (research claims)
- [ ] read `.behavior/v2026_02_13.kernelize/3.1.research.references._.v1.i1.md` (research references)

### checklist
- [ ] implement `--consensus N` argument
- [ ] implement `--threshold T` argument (default 0.5)
- [ ] call `extractKernelsWithConsensus()` when `--consensus` is set
- [ ] include `consensus: { runs, threshold }` in output when consensus mode used
- [ ] update `output.sh` to display consensus metadata

### acceptance
- [ ] `--consensus 3` runs 3 parallel extractions
- [ ] `--consensus 3 --threshold 0.7` uses 0.7 threshold
- [ ] output includes `consensus` object with `runs` and `threshold`
- [ ] consensus mode returns fewer or equal kernels vs single mode

### verification
```sh
npx rhachet run --skill kernelize --from src/domain.roles/mechanic/briefs/practices/code.prod/evolvable.procedures/rule.require.input-context-pattern.md.pt1.md --consensus 3
npx rhachet run --skill kernelize --from src/domain.roles/mechanic/briefs/practices/code.prod/evolvable.procedures/rule.require.input-context-pattern.md.pt1.md --consensus 3 --threshold 0.7
```

---

## phase 3: create ground truth fixtures

### prereqs
- [ ] phase 2 complete
- [ ] human labeler available

### checklist
- [ ] create `kernelize/.test/fixtures/ground-truth/` directory
- [ ] human labels kernels for `input-context-pattern` brief
- [ ] human labels kernels for `dependency-injection` brief
- [ ] human labels kernels for `forbid-gerunds` brief
- [ ] human labels kernels for `domain-objects-ref` brief
- [ ] each fixture follows `GroundTruthKernels` schema

### acceptance
- [ ] 4 ground truth files exist
- [ ] each file has `brief`, `source`, `kernels[]`, `labeledBy: "human"`, `labeledAt`
- [ ] kernels are distinct atomic concepts (no duplicates)

### verification
```sh
ls src/domain.roles/librarian/skills/kernelize/.test/fixtures/ground-truth/
cat src/domain.roles/librarian/skills/kernelize/.test/fixtures/ground-truth/input-context-pattern.kernels.json | jq .
```

---

## phase 4: create perfeval infrastructure

### prereqs
- [ ] phase 3 complete
- [ ] read `.behavior/v2026_02_13.kernelize/2.1.criteria.blackbox.md` (usecase.5 eval scenarios)

### checklist
- [ ] create `kernelize.perfeval.ts`
- [ ] implement `EVAL_MATRIX` constant (N × T configurations)
- [ ] implement `runStabilityEval()` — jaccard across N runs
- [ ] implement `runAccuracyEval()` — precision/recall vs ground truth
- [ ] implement `computeJaccard()` — set similarity helper
- [ ] implement `computePrecisionRecall()` — accuracy helper
- [ ] implement `emitPerfevalReport()` — json + markdown output
- [ ] create `kernelize/.perfevals/` directory

### acceptance
- [ ] `runStabilityEval()` returns jaccard score 0-1
- [ ] `runAccuracyEval()` returns precision, recall, f1
- [ ] perfeval report includes all metrics per configuration
- [ ] report emits to `.perfevals/` with timestamp

### verification
```sh
npm run test:integration -- kernelize.perfeval
```

---

## phase 5: run eval matrix

### prereqs
- [ ] phase 4 complete
- [ ] ground truth fixtures verified by human

### checklist
- [ ] run perfevals for N=1 (baseline)
- [ ] run perfevals for N=3, T=0.5
- [ ] run perfevals for N=3, T=0.7
- [ ] run perfevals for N=5, T=0.5
- [ ] run perfevals for N=5, T=0.7
- [ ] run perfevals for N=7, T=0.5
- [ ] collect stability metrics (jaccard)
- [ ] collect accuracy metrics (precision/recall)
- [ ] collect latency metrics (wall clock)
- [ ] collect cost metrics (tokens)

### acceptance
- [ ] all configurations have results
- [ ] stability target: jaccard > 0.8 for consensus modes
- [ ] accuracy target: precision > 0.9, recall > 0.9
- [ ] optimal configuration identified

### verification
```sh
cat src/domain.roles/librarian/skills/kernelize/.perfevals/*.evals.md
```

---

## phase 6: document findings

### prereqs
- [ ] phase 5 complete

### checklist
- [ ] summarize optimal N and T values
- [ ] document stability vs cost tradeoff
- [ ] document precision/recall vs ground truth
- [ ] update vision with empirical findings
- [ ] recommend default configuration for skill

### acceptance
- [ ] perfeval report complete
- [ ] optimal configuration documented
- [ ] skill defaults updated if needed

### verification
- [ ] review `.perfevals/` markdown report
- [ ] verify skill uses optimal defaults

---

## integration tests

### prereqs
- [ ] phase 2 complete

### checklist
- [ ] create `kernelize.integration.test.ts`
- [ ] test: single extraction returns kernels
- [ ] test: consensus extraction returns metadata
- [ ] test: empty brief returns empty kernels
- [ ] test: output matches `KernelizeOutput` schema
- [ ] create `__snapshots__/` for output snapshots

### acceptance
- [ ] `npm run test:integration -- kernelize.integration.test` passes
- [ ] snapshots capture expected output structure

### verification
```sh
npm run test:integration -- kernelize.integration.test
```

---

## summary

| phase | depends on | outcome |
|-------|------------|---------|
| 0 | — | extractKernels ejected |
| 1 | 0 | single extraction works |
| 2 | 1 | consensus mode works |
| 3 | 2 | ground truth created |
| 4 | 3 | perfeval infra ready |
| 5 | 4 | eval matrix complete |
| 6 | 5 | findings documented |
| tests | 2 | integration tests pass |
