# research claims: kernelize skill

## 1. LLM output stability

### [FACT] LLMs exhibit inherent output variance across runs

> "Research found no prior work that ran identical inputs multiple times to assess the stability of LLM output for common reasoning tasks. Most models showed performance spreads of 5-10% across multiple runs."
— [LLM Stability: A detailed analysis with some surprises](https://arxiv.org/html/2408.04667v1) [1]

### [FACT] even temperature=0 does not guarantee determinism

> "Even with temperature set to 0, LLM responses can still be non-deterministic due to technical factors like floating-point math and GPU computation ordering."
— [Why is deterministic output from LLMs nearly impossible?](https://unstract.com/blog/understanding-why-deterministic-output-from-llms-is-nearly-impossible/) [2]

### [FACT] lower temperatures reduce but do not eliminate variance

> "Lower temperatures (0.1–0.3) make output more deterministic by increasing probability mass on likely tokens, while higher temperatures (0.8–1.5) introduce more randomness."
— [Understanding and Mitigating Numerical Sources of Nondeterminism](https://arxiv.org/html/2506.09501v2) [3]

### [SUMP] GPT-3.5 Turbo may have better stability than other models

> "Only GPT-3.5 Turbo maintaining spreads down to 2%."
— [LLM Stability: A detailed analysis](https://arxiv.org/html/2408.04667v1) [1]

assumption: stability characteristics may differ across model families and versions.

---

## 2. ensemble and consensus methods

### [FACT] ensemble methods reduce variance in LLM outputs

> "Ensemble-based averaging techniques aggregate outputs from multiple inference runs to reduce variability and reinforce classification stability."
— [Model uncertainty and variability in LLM-based sentiment analysis](https://pmc.ncbi.nlm.nih.gov/articles/PMC12375657/) [4]

### [FACT] majority voting improves reliability

> "Ensemble aggregation leverages diversity and redundancy to suppress individual errors and reduce variance, mitigating failure modes like hallucinations and inconsistent label usage."
— [Majority Rules: LLM Ensemble is a Winning Approach](https://arxiv.org/html/2511.15714v1) [5]

### [FACT] self-consistency is an established technique

> "Two key reliability techniques include self-consistency (repeating judgments with different seeds and aggregating) and multi-judge ensembles (combining decisions from multiple models or temperatures)."
— [LLM-as-a-Judge: automated evaluation](https://pmc.ncbi.nlm.nih.gov/articles/PMC12319771/) [6]

### [OPIN] token-level consistency filtering may improve quality

> "Token consistency measuring disparities in output probabilities to serve as a filter amplifying reliable tokens and suppressing inconsistent ones."
— [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/html/2510.13855) [7]

---

## 3. evaluation metrics for extraction tasks

### [FACT] precision and recall are standard for extraction evaluation

> "For concept extraction evaluation, four widely adopted metrics are Precision, Recall, F1 Score, and Accuracy, which are commonly used to evaluate the alignment between predicted outputs and ground-truth annotations."
— [LLM Evaluation Metrics: A Complete Guide](https://www.f22labs.com/blogs/llm-evaluation-metrics-a-complete-guide/) [8]

### [FACT] F1 balances precision and recall

> "The F1 score that balances both measures is typically best."
— [How to Evaluate LLM Summarization](https://towardsdatascience.com/how-to-evaluate-llm-summarization-18a040c3905d/) [9]

### [FACT] multiple iterations quantify robustness

> "Running multiple evaluation iterations ensures stability of LLM evaluations and reduces the impact of randomness, with this iterative approach helping quantify robustness across different methodologies."
— [LLM-as-a-Judge explored](https://medium.com/online-inference/llm-as-a-judge-explored-2c6cd0d169fe) [10]

---

## 4. jaccard similarity for set comparison

### [FACT] jaccard similarity measures set overlap

> "Jaccard Similarity is defined as the ratio of the intersection of the documents to the union of the documents—in other words, it's the division of the number of tokens common to all documents by the total number of unique tokens in all documents."
— [Jaccard Similarity – Text Similarity Metric in NLP](https://studymachinelearning.com/jaccard-similarity-text-similarity-metric-in-nlp/) [11]

### [FACT] jaccard ranges from 0 (no overlap) to 1 (identical)

> "The Jaccard Coefficient is expressed as a number between 0 and 1, where a value of 1 means both sets are identical, while 0 means there is no overlap at all."
— [What is Jaccard Similarity? | IBM](https://www.ibm.com/think/topics/jaccard-similarity) [12]

### [FACT] jaccard is appropriate for presence-based comparison

> "The Jaccard similarity measurement works best when the presence of words is much more important than the frequency."
— [Benchmarking of Textual Models - Jaccard Similarity](https://www.traindex.io/blog/benchmarking-of-textual-models-jaccard-similarity-1c3i/) [13]

---

## 5. semantic compression techniques

### [FACT] semantic compression can achieve high ratios

> "Soft Prompt Methods encode prompts into continuous trainable embeddings or key-value pairs, with architectures including CC, GIST, AutoCompressor, and others allowing compression of long contexts with compression ratios up to 480x."
— [LLM Compression Techniques](https://www.projectpro.io/article/llm-compression/1179) [14]

### [FACT] hard prompt methods remove redundant tokens

> "Hard Prompt Methods remove redundant tokens from natural language prompts while retaining semantic meaning, with methods like SelectiveContext, LLMLingua, and Nano-Capsulator achieving up to 20x shorter prompts for black-box LLMs."
— [LLM Compression Techniques](https://www.projectpro.io/article/llm-compression/1179) [14]

### [SUMP] semantic compression can generalize to longer texts

> "A semantic compression method has been proposed that enables generalization to texts 6-8 times longer without significant computational costs or requiring fine-tuning."
— [Extending Context Window via Semantic Compression](https://arxiv.org/html/2312.09571v1) [15]

---

## 6. open questions

### [KHUE] what is the optimal consensus threshold?

our vision proposes testing T=[0.3, 0.5, 0.7, 0.9]. research suggests majority (0.5) is common, but optimal threshold for kernel extraction is unknown.

### [KHUE] how many consensus runs are needed?

our vision proposes testing N=[1, 3, 5, 7]. research shows ensemble benefits, but marginal returns and cost tradeoffs for kernel extraction are unknown.

### [KHUE] does kernel extraction stability vary by brief type?

different brief types (rules, lessons, refs, demos) may have different extraction characteristics. no research found on this specific question.

### [KHUE] what grain of semantic unit is optimal?

research covers document-level and sentence-level compression, but atomic "kernel" extraction is less studied. what constitutes the right granularity?

---

## citations

1. [LLM Stability: A detailed analysis with some surprises](https://arxiv.org/html/2408.04667v1)
2. [Why is deterministic output from LLMs nearly impossible?](https://unstract.com/blog/understanding-why-deterministic-output-from-llms-is-nearly-impossible/)
3. [Understanding and Mitigating Numerical Sources of Nondeterminism](https://arxiv.org/html/2506.09501v2)
4. [Model uncertainty and variability in LLM-based sentiment analysis](https://pmc.ncbi.nlm.nih.gov/articles/PMC12375657/)
5. [Majority Rules: LLM Ensemble is a Winning Approach](https://arxiv.org/html/2511.15714v1)
6. [LLM-as-a-Judge: automated evaluation](https://pmc.ncbi.nlm.nih.gov/articles/PMC12319771/)
7. [Harnessing Consistency for Robust Test-Time LLM Ensemble](https://arxiv.org/html/2510.13855)
8. [LLM Evaluation Metrics: A Complete Guide](https://www.f22labs.com/blogs/llm-evaluation-metrics-a-complete-guide/)
9. [How to Evaluate LLM Summarization](https://towardsdatascience.com/how-to-evaluate-llm-summarization-18a040c3905d/)
10. [LLM-as-a-Judge explored](https://medium.com/online-inference/llm-as-a-judge-explored-2c6cd0d169fe)
11. [Jaccard Similarity – Text Similarity Metric in NLP](https://studymachinelearning.com/jaccard-similarity-text-similarity-metric-in-nlp/)
12. [What is Jaccard Similarity? | IBM](https://www.ibm.com/think/topics/jaccard-similarity)
13. [Benchmarking of Textual Models - Jaccard Similarity](https://www.traindex.io/blog/benchmarking-of-textual-models-jaccard-similarity-1c3i/)
14. [LLM Compression Techniques](https://www.projectpro.io/article/llm-compression/1179)
15. [Extending Context Window via Semantic Compression](https://arxiv.org/html/2312.09571v1)
