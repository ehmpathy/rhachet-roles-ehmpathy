# vision: kernelize skill

## the world before

kernel extraction is embedded within the compression pipeline. we measure "zero kernel loss" but have no independent validation of:

- **stability** — does the same brief produce the same kernels across runs?
- **accuracy** — do extracted kernels match what a human would identify?
- **coverage** — are all critical concepts captured?

the compression metric is circular: we extract kernels, compress, check retention — but if the extractor is inconsistent, "zero kernel loss" means little.

## the world after

kernel extraction is a first-class skill with independent evaluation:

```sh
# single extraction (default, fast)
npx rhachet run --skill kernelize --from path/to/brief.md

# consensus mode (N parallel runs, majority vote)
npx rhachet run --skill kernelize --from path/to/brief.md --consensus 3

# consensus with custom threshold (default: 0.5 = majority)
npx rhachet run --skill kernelize --from path/to/brief.md --consensus 5 --threshold 0.6

# output to specific file
npx rhachet run --skill kernelize --from path/to/brief.md --into path/to/kernels.json
```

### mode comparison

| mode | runs | latency | cost | stability | use case |
|------|------|---------|------|-----------|----------|
| **single** | 1 | fast | low | variable | iteration, drafts |
| **consensus** | N | N× | N× | high | production, high-value briefs |

### consensus parameters

| param | default | definition |
|-------|---------|------------|
| `--consensus N` | off | run N parallel extractions |
| `--threshold T` | 0.5 | kernel must appear in ≥T fraction of runs |

consensus reduces variance at cost of latency and tokens. use single mode for fast iteration, consensus for final compression of critical briefs.

output:
```json
{
  "source": "path/to/brief.md",
  "kernelCount": 7,
  "kernels": [
    { "id": "k1", "concept": "all procedures use (input, context) pattern", "category": "rule" },
    { "id": "k2", "concept": "dependency injection via context argument", "category": "principle" }
  ],
  "rationale": "...",
  "consensus": { "runs": 3, "threshold": 0.5 }
}
```

## mental model

kernels = atoms of semantic content

a brief is a molecule composed of kernels. compression removes structural filler (articles, verbose phrases, redundant examples) while kernel atoms remain intact.

the kernelizer identifies these atoms. if the kernelizer is unstable, the molecule definition changes run-to-run. if the kernelizer misses atoms, compression can silently drop critical content.

## evaluation metrics

| metric | definition | target |
|--------|------------|--------|
| **stability** | jaccard similarity of kernel sets across N runs | >0.8 |
| **precision** | % extracted kernels that are in ground truth | >0.9 |
| **recall** | % ground truth kernels that are extracted | >0.9 |
| **consensus lift** | stability improvement from consensus mode | measurable |

### eval matrix: consensus configurations

sweep across consensus parameters to find optimal settings:

| dimension | values to test |
|-----------|----------------|
| **runs (N)** | 1, 3, 5, 7 |
| **threshold (T)** | 0.3, 0.5, 0.7, 0.9 |
| **briefs** | representative set (rules, lessons, refs, demos) |

metrics per configuration:
- stability (jaccard across repeated evals)
- precision/recall vs ground truth
- latency (wall clock)
- cost (tokens consumed)

goal: find the sweet spot where stability plateaus vs cost. hypothesis: N=3, T=0.5 is likely optimal, but evals will validate.

## ground truth design

human-labeled kernel sets for representative briefs:

```
src/domain.roles/mechanic/skills/kernelize/.test/fixtures/ground-truth/
├── input-context-pattern.kernels.json
├── dependency-injection.kernels.json
├── forbid-gerunds.kernels.json
└── ...
```

each ground truth file:
```json
{
  "brief": "input-context-pattern",
  "source": "briefs/practices/code.prod/evolvable.procedures/rule.require.input-context-pattern.md",
  "kernels": [
    "all procedures must accept (input, context?) arguments",
    "input is a destructurable object with named keys",
    "context contains runtime dependencies",
    "positional arguments are forbidden"
  ],
  "labeledBy": "human",
  "labeledAt": "2026-02-13"
}
```

## risks

| risk | mitigation |
|------|------------|
| brain variance | consensus mode with N parallel runs |
| ground truth drift | version ground truth files, review on brief changes |
| over-extraction | precision metric catches spurious kernels |
| under-extraction | recall metric catches missed kernels |
| category confusion | category is informational, not used in retention check |

## success criteria

1. **stability** — jaccard similarity >0.8 across runs on standard briefs
2. **accuracy** — precision/recall >0.9 vs human-labeled ground truth
3. **consensus lift** — measurable improvement from consensus over single extraction
4. **coverage** — eval suite covers all brief categories (rules, lessons, demos, refs)
5. **observability** — perfeval results logged with full provenance

## dependencies

- `extractKernels` function (exists in `brief.compress/extractKernels.ts`)
- `extractKernelsWithConsensus` function (exists)
- brain access via `genContextBrain`
- perfeval infrastructure from `brief.compress` (can reuse pattern)

## out of scope

- compression (that's condensor skill)
- retention check (that's condensor skill)
- methodology selection (kernelize has one methodology: extract kernels)
