# research references: kernelize skill

## ref.1 — self-consistency for LLM outputs

**source**: Wang et al., "Self-Consistency Improves Chain of Thought Reasoning in Language Models", ICLR 2023
**link**: [arxiv.org/abs/2203.11171](https://arxiv.org/abs/2203.11171)

### lesson

> "Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer."

### method

> "It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths."

### results

> "Self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%)."

### relevance to kernelize

self-consistency via majority vote is the foundation of our consensus extraction mode. the paper validates that sampling multiple paths and selecting by consistency improves accuracy over single greedy decoding.

---

## ref.2 — LLMLingua prompt compression

**source**: Jiang et al., "LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models", EMNLP 2023
**link**: [arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736)

### lesson

> "A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios."

### method

> "Introduces a token-level iterative algorithm for fine-grained prompt compression that can better preserve the key information within the prompt by taking into account the conditional dependencies between tokens."

### results

> "The proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss."

### relevance to kernelize

LLMLingua demonstrates that semantic content can be preserved under aggressive compression. kernelize aims to identify the "key information" that must be preserved — the kernels — before compression occurs.

---

## ref.3 — inter-annotator agreement metrics

**source**: Artstein, "Inter-annotator Agreement", Handbook of Linguistic Annotation
**link**: [researchgate.net/publication/318176345](https://www.researchgate.net/publication/318176345_Inter-annotator_Agreement)

### lesson: Cohen's kappa

> "Cohen's kappa statistic measures how often two raters agree with each other after accounting for the likelihood they'd agree by chance."

### lesson: Fleiss' kappa

> "Fleiss' kappa measures the consistency between multiple annotators and is an extension of the classic Cohen's kappa coefficient."

### interpretation

> "Cohen's kappa ranges from 1, representing perfect agreement between raters, to -1, meaning the raters choose different labels for every sample. A value of 0 means the raters agreed exactly as often as if they were both randomly guessing."

### relevance to kernelize

inter-annotator agreement metrics provide the framework for our stability evaluation. if we treat each extraction run as an "annotator", jaccard similarity serves a similar role to kappa — measuring agreement beyond chance.

---

## ref.4 — information extraction evaluation

**source**: LangChain, "Extraction Benchmarking"
**link**: [blog.langchain.com/extraction-benchmarking](https://blog.langchain.com/extraction-benchmarking/)

### lesson

> "Structured information extraction evaluation typically uses metrics like recall, precision, and F1 for matched entities."

### challenge

> "A major challenge in information extraction is quantitatively assessing extraction quality in the absence of labeled datasets, particularly when ground truth generation requires vast expertise and time."

### approach

> "Creating synthetic ground truth by artificially inserting complex information into documents to test LLM extraction efficiency."

### relevance to kernelize

we face the same ground truth challenge. our approach: human-labeled kernel sets for representative briefs, validated by precision/recall against model extractions.

---

## ref.5 — prompt engineering guide on self-consistency

**source**: DAIR.AI, "Self-Consistency"
**link**: [promptingguide.ai/techniques/consistency](https://www.promptingguide.ai/techniques/consistency)

### lesson

> "Self-consistency aims to replace the naive greedy decoding used in chain-of-thought prompting. The idea is to sample multiple, diverse reasoning paths through few-shot CoT, and use the generations to select the most consistent answer."

### key insight

the guide establishes self-consistency as a standard prompting technique, not a novel research contribution. this validates our use of consensus extraction as a best practice.

---

## ref.6 — ground truth annotation for extraction

**source**: Gupta et al., "LLMs Accelerate Annotation for Medical Information Extraction", 2023
**link**: [arxiv.org/html/2312.02296v1](https://arxiv.org/html/2312.02296v1)

### lesson

> "Combining Large Language Models with human expertise to create an efficient method for generating ground truth labels while reducing human annotation burden."

### benchmark

> "Human annotators also serve as benchmarks—the accuracy of a second human annotator serves as the benchmark for 'human-level performance'."

### relevance to kernelize

this suggests a hybrid approach: use LLM to propose kernels, human to validate. the human validation becomes ground truth for future evals.

---

## terms established

| term | definition | source |
|------|------------|--------|
| **self-consistency** | sample multiple paths, select by majority | ref.1 |
| **semantic integrity** | preservation of content under compression | ref.2 |
| **inter-annotator agreement** | consistency between multiple labelers | ref.3 |
| **Cohen's kappa** | pairwise agreement metric, chance-corrected | ref.3 |
| **Fleiss' kappa** | multi-annotator agreement metric | ref.3 |
| **ground truth** | human-labeled correct answers | ref.4, ref.6 |
| **precision** | fraction of extractions that are correct | ref.4 |
| **recall** | fraction of correct items that are extracted | ref.4 |

---

## concepts to leverage

1. **majority vote** — established technique for variance reduction (ref.1)
2. **coarse-to-fine** — extract high-level first, refine (ref.2)
3. **agreement metrics** — kappa family for stability measurement (ref.3)
4. **synthetic ground truth** — create test cases with known answers (ref.4)
5. **human-LLM hybrid** — LLM proposes, human validates (ref.6)

---

## citations

1. Wang et al., [Self-Consistency Improves Chain of Thought Reasoning](https://arxiv.org/abs/2203.11171), ICLR 2023
2. Jiang et al., [LLMLingua: Compressing Prompts](https://arxiv.org/abs/2310.05736), EMNLP 2023
3. Artstein, [Inter-annotator Agreement](https://www.researchgate.net/publication/318176345_Inter-annotator_Agreement), Handbook of Linguistic Annotation
4. LangChain, [Extraction Benchmarking](https://blog.langchain.com/extraction-benchmarking/)
5. DAIR.AI, [Self-Consistency](https://www.promptingguide.ai/techniques/consistency), Prompt Engineering Guide
6. Gupta et al., [LLMs Accelerate Annotation](https://arxiv.org/html/2312.02296v1), 2023
