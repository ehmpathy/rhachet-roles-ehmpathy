# ejection: distill later via bhrain librarian

**date**: 2026-02-09
**phase**: post-kernelization
**decision**: eject with intent to return

---

## current state

kernelization phase complete:
- 35 probe files researched and emitted
- 35 kernel files extracted and classified
- ~1,060 kernels across [FACT], [SUMP], [KHUE], [HYPO]
- comprehensive catalog created at `3.2.probe.comprehend.kernelize.catalog.md`
- 7 domain clusters mapped
- 4 cross-domain relation chains identified
- treestruct hierarchy index built

## decision

**eject now, distill later**

the kernels are raw and comprehensive but not yet distilled into a reusable knowledge corpus. that distillation requires:
- kernel deduplication across probes
- citation consolidation (same fact from multiple sources)
- representative kernel synthesis
- corpus structure for retrieval

this is librarian work, not mechanic work.

## method comparison conclusion

for the immediate task (brief minification), the research supports:

| method | recommendation |
|--------|----------------|
| **LLMLingua** | validated, production-ready, 20x compression with ~1.5% loss |
| **TSC** | faster but unvalidated for instruction compliance |
| **summarization** | abstractive methods lose constraint tokens |

**LLMLingua wins** on empirical evidence. TSC is a hypothesis worth test but not yet proven for mechanic briefs.

## future work: bhrain librarian

on return to distill:

1. **converge kernels** â€” merge duplicates across probes, preserve all citations
2. **structure corpus** â€” organize by treestruct hierarchy for retrieval
3. **validate assumptions** â€” flag [SUMP] kernels that need empirical test
4. **prioritize hypotheses** â€” rank [HYPO] kernels by testability and impact
5. **emit knowledge artifacts** â€” briefs, refs, lessons for mechanic consumption

the librarian should consume:
- `kernels/q*.kernels.md` (35 files)
- `3.2.probe.comprehend.kernelize.catalog.md` (synthesis)

## artifacts ready for distillation

```
.research/v2025_02_09.brief-minification/
â”œâ”€â”€ probe/                          # 35 raw research responses
â”‚   â””â”€â”€ q*.probe.research.response.v1.i1.md
â”œâ”€â”€ kernels/                        # 35 kernel extractions
â”‚   â””â”€â”€ q*.kernels.md
â”œâ”€â”€ 3.2.probe.comprehend.kernelize.catalog.md  # synthesis catalog
â””â”€â”€ 7.ejection.distill-later-via-librarian.md  # this file
```

## takeaway for now

LLMLingua is the empirically validated choice for brief compression. TSC is compelling but needs validation before adoption. the full kernel corpus awaits librarian distillation when the time is right.

---

ğŸ¢ shell yeah, good research session. catch you on the flip side ğŸŒŠ
