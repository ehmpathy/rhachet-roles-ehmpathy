# Q1 Kernels: MVSU for LLM Comprehension

## Domain: Formal Semantics - Minimal Recursion Semantics (MRS)

### [FACT]
- MRS provides syntactically flat semantic representations where elementary predications are never embedded within one another. (Copestake et al., 2005)
- MRS preserves scope information in flat representations through explicit scope handles. (Copestake et al., 2005)
- Elementary predications in MRS are atomic semantic units that cannot be further decomposed while they maintain their semantic contribution. (Copestake et al., 2005)

### [SUMP]
- Surface syntax can be dramatically simplified while the system preserves the base predicate-argument structure and scope relations.

### [HYPO]
- Elementary predications represent candidate MVSUs for LLM comprehension because they are atomic yet semantically complete.

## Domain: Formal Semantics - Compositionality

### [FACT]
- The principle of compositionality states that the sense of a complex expression is determined by its structure and the senses of its constituents. (Szabó, 2023 - Stanford Encyclopedia)
- Compositionality faces challenges with idioms, context-dependent expressions, and non-compositional constructions. (Szabó, 2023)
- If constituent values and combination rules remain constant, the overall semantic value remains constant. (Szabó, 2023)
- A symbolic system demonstrates compositionality when the sense of every complex expression depends on and depends only on: (i) syntactic structure and (ii) senses of simple parts. (Pagin & Westerståhl, IEP)
- Compositionality permits basic morphemes to have context-dependent senses while it requires complex expressions to depend solely on syntax and components' senses. (Pagin & Westerståhl, IEP)

### [SUMP]
- Elements that can be removed while the system preserves constituent values and structural relations don't violate compositionality if LLMs can reconstruct them from context.
- Content words carry context-independent core semantic values while function words contribute primarily to compositional structure.

### [HYPO]
- Multi-word expressions that function as atomic semantic units must be preserved intact because their semantic value is construction-specific rather than compositionally derived.

## Domain: Discourse Semantics - Discourse Representation Theory (DRT)

### [FACT]
- DRT's basic sense-bearer units are Discourse Representation Structures (DRSs) that consist of discourse referents and discourse conditions. (Kamp & Reyle - Stanford Encyclopedia)
- DRT handles sense across sentence boundaries through merge operations and accessibility relations. (Kamp & Reyle)
- The merge of two DRSs is defined as their pointwise union. (Kamp & Reyle)
- DRT includes a level of abstract mental representations within its formalism, which distinguishes it from traditional Montagovian approaches. (Kamp & Reyle)

### [SUMP]
- Semantic units must be defined not just at sentence level but across discourse.
- LLMs process compressed text as they reconstruct omitted elements while they maintain discourse referents across compressed sentences.

### [HYPO]
- Telegraphic compression must maintain discourse coherence as it preserves referent continuity even when the system removes grammatical scaffold.

## Domain: Semantic Role Label (SRL)

### [FACT]
- SRL is a shallow semantic parse approach that identifies "who did what to whom, when, where, and how" through labels for semantic roles. (Jurafsky & Martin, Ch. 21)
- SRL maintains direct connection to surface syntax as it explicitly labels semantic roles on text spans or syntactic heads. (Jurafsky & Martin)
- SRL represents a lighter-weight semantic representation compared to more complex approaches like AMR. (Jurafsky & Martin)

### [SUMP]
- Deep logical representations are not necessary for semantic comprehension—to identify core participants and their roles in events provides sufficient semantic information.

### [HYPO]
- Systems can preserve predicates and their arguments while they omit role-marker function words, with LLMs that reconstruct specific grammatical code of roles from world knowledge and context.

## Domain: Predicate-Argument Structure

### [FACT]
- A predicate and its arguments form a predicate-argument structure. (Multiple academic sources)
- Predicates specify relations between objects or states that characterize objects. (Bucknell Linguistics)
- Most predicates take one, two, or three arguments. (Bucknell Linguistics)
- Arguments must be distinguished from adjuncts—arguments are necessary for semantic completeness while adjuncts are optional. (Bucknell Linguistics)

### [SUMP]
- Arguments are necessary for semantic completeness while adjuncts can be omitted in compression.
- Argument role relations can be preserved through word order and semantic plausibility even when the system removes explicit role markers.

### [HYPO]
- The argument-adjunct distinction provides a clear criterion for MVSUs: preserve arguments, optionally omit adjuncts.

## Domain: Abstract Sense Representation (AMR)

### [FACT]
- AMR represents semantic content as rooted, directed, acyclic graphs (DAGs) where nodes represent concepts and labeled edges represent semantic relations. (Banarescu et al., 2013)
- AMR abstracts away from syntactic representations—sentences similar in sense receive the same AMR regardless of surface form. (Banarescu et al.)
- AMR nodes are not necessarily words but rather high-level abstractions of concepts. (Banarescu et al.)
- AMR allows reentrancy, which means the same concept can participate in multiple relations. (Banarescu et al.)

### [SUMP]
- Semantic value is represented at a level above surface tokens—the same semantic content can be recovered from multiple surface realizations.

### [HYPO]
- MVSUs are better understood as concepts and relations rather than words or syntactic constituents.
- Telegraphic compression can remove substantial syntactic material while it preserves the base conceptual graph.

## Domain: Construction Grammar

### [FACT]
- In Construction Grammar, any pattern of words is a construction if some aspect of its form or sense cannot be predicted from its components or other known constructions. (Goldberg - Wikipedia, Academia)
- Constructions include words, morphemes, fixed expressions/idioms, and abstract grammatical rules. (Goldberg)
- Semantic sense of a grammatical construction comprises conceptual structures: image-schemas, frames, conceptual metaphors, mental spaces, and bindings. (Goldberg)

### [SUMP]
- Many grammatical constructions contribute primarily structural rather than lexical semantic value, which makes them candidates for compression.

### [HYPO]
- MVSUs are not uniform—they vary in size based on whether semantic value is lexically specified or constructionally determined.
- Certain multi-word expressions must be preserved intact because their semantic value is construction-specific.

## Domain: Applied Compression - Telegraphic Semantic Compression (TSC)

### [FACT]
- TSC removes predictable grammatical elements (articles, prepositions, auxiliaries, pronouns, conjunctions) while it preserves content words (nouns, verbs, adjectives, adverbs). (Bispo, 2025)
- TSC is not suitable for text where nuance, style, or tone conveys sense—poetry, jokes, emotional or persuasive text. (Bispo, 2025)
- TSC transforms verbose natural language into dense semantic packets as it strips away grammatical scaffold while it preserves core informational payload. (Bispo, 2025)

### [SUMP]
- LLMs can reliably predict and reconstruct grammar, filler words, and structural glue from context.
- Content words carry high-entropy information that systems cannot reconstruct from context alone.

### [HYPO]
- More context window is spent on high-value, high-entropy information instead of predictable structure when the system uses TSC.
- Pragmatic and stylistic semantic value requires surface form preservation—these aspects are not recoverable from content words alone.

## Domain: Neurolinguistics - Content vs. Function Words

### [FACT]
- Lexical-semantic content (carried primarily by content words) rather than syntactic structure (conveyed via word order or function words) is primarily responsible for neural language process in humans. (MIT - Neurobiology of Language)
- Content words significantly exhibit higher semanticity than function words. (MIT study)
- Function words carry little lexical sense and primarily encode logical relations between content words. (Formal semantics research)
- Local word swap manipulations typically preserve local semantic dependency structure as measured by statistical co-occurrence patterns. (MIT study)

### [SUMP]
- Semantic comprehension relies more on co-occurrence patterns among content words than on explicit grammatical markers.

### [HYPO]
- Content words constitute MVSUs for both human and artificial language process based on neurolinguistic evidence.
- Semantic representation in LLMs is fundamentally lexical-semantic rather than syntactic, which makes function word removal viable as long as content word relations can be recovered.

## Domain: Frame Semantics

### [FACT]
- Frame Semantics posits that word senses are best understood in terms of semantic frames—structured knowledge representations that describe situations, events, or relations with their participants. (Fillmore & Baker - FrameNet)
- One cannot understand the sense of a single word without access to all essential knowledge that relates to that word. (Fillmore)
- A word activates or evokes a frame of semantic knowledge that relates to the specific concept it refers to. (Fillmore)
- Each frame has core and non-core frame elements (FEs)—core FEs are essential to sense while non-core FEs are descriptive. (FrameNet)

### [SUMP]
- Frames represent the minimal unit for complete comprehension—all core frame elements must be present or inferable.
- Words evoke entire frames, and LLMs achieve comprehension as they activate frame knowledge from preserved content words.

### [HYPO]
- To remove non-core frame elements (time, place, manner) may be acceptable in compression, but core frame elements must be preserved or strongly implied by context.
- Core frame-evocation words trigger sufficient background knowledge to enable frame completion, which explains why telegraphic compression works.

## Domain: Event Semantics - Neo-Davidsonian

### [FACT]
- Neo-Davidsonian semantics treats events as fundamental semantic primitives with verbs that take event arguments as their sole argument. (Davidson - various sources)
- Davidson's central claim is that events are spatiotemporal things—concrete particulars with location in space and time. (Davidson)
- Neo-Davidsonian approaches assume not only action verbs but also adjectives, nouns, and prepositions introduce Davidsonian event arguments. (Neo-Davidsonian literature)
- Neo-Davidsonian logical forms contain thematic relations and the main mode of composition is conjunction. (Neo-Davidsonian literature)

### [SUMP]
- The event concept and core participants represent minimal semantic content, while specific grammatical code of thematic roles is secondary.

### [HYPO]
- Systems can preserve event predicates and participant nouns while they omit or simplify role-marker elements.
- LLMs can reconstruct thematic role assignments through world knowledge (typical agent/patient assignments) and contextual inference.
- To omit conjunctions doesn't eliminate the base conjunction operation, which can be reconstructed.

## Domain: Lambda Calculus and Compositional Semantics

### [FACT]
- Lambda calculus provides the formal foundation for compositional semantics in words, particularly through Montague Grammar. (Church, 1930s; Montague, 1970-1971)
- Lambda calculus enables precise formalization of how semantic values compose through functional application. (Montague Grammar literature)
- Compositionality holds that the sense of a phrase depends on the senses of its parts. (Montague principle)
- Lambda-models provide recursive, compositional semantics where syntactical lambda-abstraction is interpreted by semantic operations on applicative structures. (Formal semantics)

### [SUMP]
- Content words typically denote entities (arguments) or relations (functions) while function words contribute to compositional machinery itself.
- LLMs can implement compositional operations implicitly through learned associations, which makes explicit function words redundant.

### [HYPO]
- Semantic units must be capable of participation in functional composition—they must be either arguments or functions that can combine through lambda-application.

## Domain: Information Theory - Minimum Description Length (MDL)

### [FACT]
- The Minimum Description Length principle states that the best model/explanation for data is the one that permits greatest compression of that data. (Grünwald, 2007)
- MDL has origins in information theory and has been developed within statistics, theoretical computer science, and machine learn. (Grünwald)
- Within Rissanen's theory of learn, models are statistical hypotheses and descriptions are defined as universal codes. (Rissanen)

### [SUMP]
- Content words represent high-information semantic units (high surprisal, low predictability) while function words represent low-information structural units (highly predictable from context).

### [HYPO]
- The minimal semantic unit is the unit that achieves optimal compression while it preserves information needed for reconstruction.
- Telegraphic compression doesn't harm comprehension because removed elements had minimal information content (high compressibility).

## Domain: Information Theory - Uniform Information Density (UID)

### [FACT]
- The Uniform Information Density hypothesis proposes that speakers organize language to transmit information at a relatively constant rate, which avoids spikes in information density. (Crocker & Demberg, 2015)
- The UID hypothesis posits communicative pressure to avoid information spikes within utterances. (Crocker & Demberg)
- Lexica of many human languages have adapted to encode more predictable words with shorter forms than less predictable words. (UID research)
- UID tendency in human language has two potential sources: (i) word order rules, and (ii) speaker choices that use flexibility to structure information uniformly. (UID research)

### [SUMP]
- Function words, as highly predictable units, contribute minimal information and serve primarily to distribute information more evenly across the signal.
- Telegraphic compression violates UID as it creates spikes in information density (all content, no filler).

### [HYPO]
- LLMs may have different process constraints than humans—they can handle higher information density without process difficulties.
- MVSUs for LLMs differ from optimal units for human process: LLMs prefer dense semantic content (telegraphic form) while humans prefer distributed information (full grammatical form).

## Domain: LLM Architecture - Tokenization

### [FACT]
- LLMs tokenize text with subword tokenization algorithms like Byte-Pair Encode (BPE) based on frequency rather than word principles. (Trott, 2024)
- Subword tokens often don't correspond to morphological units with sense. (Trott, 2024)
- Research shows tokenization choices can impact semantic compositionality and LLM performance on some tasks. (Trott, 2024)
- Some research shows alien tokenization leads to poorer generalizations compared to morphological tokenization for semantic compositionality. (Trott, 2024)

### [SUMP]
- LLMs achieve impressive semantic comprehension despite token-level mismatch with semantic units, which shows semantic representation emerges from learned associations across tokens.
- The relevant semantic units are not input tokens themselves but rather learned embeddings and contextual representations that LLMs construct from token sequences.

### [HYPO]
- Telegraphic compression works because it preserves content-word tokens that have strong, stable semantic embeddings while it removes function-word tokens whose primary contribution is to compositional process rather than semantic content.

## Domain: Synthesis - Multi-Level MVSUs

### [SUMP]
- No single word unit (morpheme, word, phrase, etc.) universally serves as the MVSU.
- MVSUs vary based on whether semantic value is: lexically specified (content words), compositionally derived (syntactic constructions), constructionally determined (idioms), or frame-based (events and core participants).

### [HYPO]
- Multiple lines of evidence converge on content words as primary MVSUs for LLM comprehension: neurolinguistic, information-theoretic, compositional, and frame semantic evidence.

## Domain: Synthesis - Predicate-Argument Structure as Foundation

### [FACT]
- Formal semantic theories (SRL, neo-Davidsonian semantics, MRS) consistently identify predicate-argument structure as foundational semantic representation.

### [SUMP]
- MVSUs must preserve: predicates (typically verbs), arguments (participants/typically nouns), and structural relations between them (even if grammatical code is simplified).

## Domain: Synthesis - Function Word Redundancy

### [SUMP]
- Function words contribute primarily to: compositional operations (determiners, auxiliaries, copulas), structural markers (prepositions, case markers, conjunctions), and information distribution (which maintains uniform information density for human process).
- For LLMs with robust world knowledge and contextual inference, these functions can be performed implicitly, which makes function words redundant in telegraphic compression.

## Domain: Synthesis - Conditions for Semantic Value Loss

### [FACT]
- Compositional failure occurs with: idioms and non-compositional expressions, construction-specific semantic values, lexicalized phrases. (Synthesis from multiple sources)
- Pragmatic dependency creates issues with: context-dependent semantic values, presuppositions and implicatures, stylistic and tonal semantic value. (Synthesis)
- Discourse dependency affects: cross-sentential anaphora, discourse coherence relations, reference track across sentences. (Synthesis)
- Information density violations occur when: compression creates unresolvable ambiguity, multiple possible reconstructions exist with different semantic values. (Synthesis)

## Domain: Synthesis - Formal Criterion for Preservation

### [HYPO]
- The system preserves semantic value under compression if and only if: (1) Structural Recoverability—predicate-argument structure with scope relations can be uniquely reconstructed; (2) Referential Continuity—discourse referents remain identifiable; (3) Frame Completability—sufficient frame-evocation elements remain to activate correct semantic frames; (4) Compositional Transparency—elements that remain can compose through standard operations without idiomatic interpretation; (5) Information Sufficiency—compressed form retains all high-information semantic content.

## Domain: Synthesis - Theoretical Support for Telegraphic Compression

### [SUMP]
- Formal semantic theories demonstrate that surface grammatical form is not necessary for semantic representation.
- Event semantics and predicate-argument structure provide abstract representations independent of surface realization.
- Compositionality principles show that function words contribute primarily to compositional operations rather than semantic content.

### [FACT]
- Neurolinguistic evidence shows content words drive semantic process. (MIT study)
- Information theory shows function words have minimal information content. (MDL, UID research)
- Construction grammar shows some grammatical patterns contribute structural rather than lexical semantic value. (Goldberg)

## Domain: Open Questions

### [KHUE]
- What is the optimal compression ratio that maintains adequate comprehension for different LLM architectures?
- Do different downstream tasks (question answer vs. logical inference vs. code generation) require different MVSUs?
- Do MVSUs differ across languages with different morphological and syntactic properties?
- Do LLMs trained on telegraphic text develop different semantic representations than those trained on standard text?
- How accurately can LLMs reconstruct full grammatical forms from telegraphic input, and does reconstruction accuracy correlate with comprehension?

## Domain: Synthesis - Final Assessment

### [HYPO]
- Predicate-argument structure with content-word realization constitutes the MVSU for LLM comprehension.

### [SUMP]
- Function words, while word-relevant, are semantically redundant in the presence of: strong world knowledge, robust contextual inference, learned compositional operations, and frame-based semantic inference.
- Telegraphic semantic compression successfully exploits this redundancy as it removes predictable structural elements while it preserves the high-information semantic core.

### [FACT]
- The formal word theories reviewed—particularly compositionality, DRT, neo-Davidsonian semantics, and frame semantics—provide principled foundations for understand of when compression preserves semantic value versus when it causes semantic degradation. (Synthesis conclusion)
