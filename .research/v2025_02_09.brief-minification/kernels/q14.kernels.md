# Knowledge Kernels: Q14 - Evaluation Methodologies for Detect Subtle Knowledge Loss

## Overview
Extract from research probe Q14 on evaluation methodologies to detect subtle knowledge loss from compression, with focus on edge case tests, adversarial probes, and boundary condition queries.

---

## FACTS [FACT]

### Compression Techniques and Effects

1. **[FACT]** Model compression techniques include quantization, prune, low-rank decomposition, and knowledge distillation as primary methods.
   - Source: Frontiers in Robotics and AI (2025)

2. **[FACT]** Compressed models maintain comparable performance on standard benchmarks but exhibit significantly reduced robustness when subject to adversarial attacks.
   - Source: Frontiers in Robotics and AI (2025)

3. **[FACT]** Knowledge-distilled models experience the most significant performance drop under adversarial attacks, while prune and quantization tend to be more robust than knowledge distillation.
   - Source: Frontiers in Robotics and AI (2025)

4. **[FACT]** Apply conventional quantization methods to MobileNetV2 led to a drastic performance drop from 70.9% to 0.1% on ImageNet.
   - Source: Frontiers in Robotics and AI (2025)

5. **[FACT]** Prune does not guarantee any decrease in latency, which was not observed in empirical experiments.
   - Source: arXiv:2407.15904 (2024)

6. **[FACT]** Beyond 75-90% sparsity, irreversible performance degradation occurs in pruned models.
   - Source: arXiv:2407.15904 (2024)

7. **[FACT]** Performance with uniform quantization remains consistent down to 6-bit fixed-point quantization, but when precision is reduced to 4-bits, performance begins to degrade.
   - Source: arXiv:2101.09671 (2021)

8. **[FACT]** Performance is maintained until about 80% of the weights are pruned, after which performance begins to degrade drastically.
   - Source: arXiv:2101.09671 (2021)

9. **[FACT]** 4-bit quantization maintains better performance than 75% prune despite both achieve 75% theoretical compression rate.
   - Source: arXiv:2505.07289v1 (2025)

10. **[FACT]** Combine 25% prune with 4-bit quantization significantly outperforms pure 3-bit quantization, which achieves around 20% higher semantic retention.
    - Source: arXiv:2505.07289v1 (2025)

11. **[FACT]** Quantization generally outperforms prune for neural networks, and it is recommended to quantize before prune is explored.
    - Source: arXiv:2101.09671 (2021)

### Adversarial Robustness

12. **[FACT]** Compressed standard models that undergo adversarial fine-tune of only three epochs achieve robustness within a 5% difference from the fully adversarially trained model.
    - Source: arXiv:2403.09441 (2024)

13. **[FACT]** For Fashion-MNIST pruned models, robustness improved from essentially zero (0.28±0.64) to 76.74±2.33 with just three epochs of adversarial fine-tune.
    - Source: arXiv:2403.09441 (2024)

14. **[FACT]** To perform adversarial fine-tune instead of adversarial train can reduce the computation time from about 118 minutes to only about 14 minutes on the CIFAR10 dataset.
    - Source: arXiv:2403.09441 (2024)

15. **[FACT]** PGD-based adversarial train improves robustness against several types of attacks which include BIM, FGSM, PGD, C&W, and DeepFool under both black-box and white-box contexts.
    - Source: MDPI Information (2023)

16. **[FACT]** To train a small model from scratch even with inherited initialization from a large model cannot achieve either adversarial robustness or high standard accuracy.
    - Source: ICCV 2019

### Knowledge Loss Patterns

17. **[FACT]** Models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset.
    - Source: arXiv:1911.05248 (2019)

18. **[FACT]** Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution.
    - Source: arXiv:1911.05248 (2019)

19. **[FACT]** Pruned Identified Exemplars (PIEs) over-index on atypical or noisy images that are far more challenge for both humans and algorithms to classify.
    - Source: arXiv:1911.05248 (2019)

20. **[FACT]** Perplexity often fails to capture subtle degradation in knowledge-intensive tasks even when compressed models maintain similar perplexity scores.
    - Source: arXiv:2505.07289v1 (2025)

21. **[FACT]** Mathematical reason and instruction-follow demonstrate heightened vulnerability to compression compared to other capability domains.
    - Source: arXiv:2505.07289v1 (2025)

22. **[FACT]** Fewer than 7% of paragraphs maintained ten correct facts before they encounter errors in LLM text generation.
    - Source: arXiv:2404.05411v1 (2024)

23. **[FACT]** LLaMa2-70B models have high semantic drift scores: they tend to generate correct facts first, then 'drift away' from the topic and generate incorrect facts later.
    - Source: arXiv:2404.05411v1 (2024)

24. **[FACT]** In deep neural networks, deeper layers that compress representations hinder OOD performance.
    - Source: arXiv:2403.01874v1 (2024)

25. **[FACT]** Linear probe ID accuracy monotonically increases as a function of layers, but OOD accuracy only increases until a compression 'tunnel' is reached and then decreases.
    - Source: arXiv:2403.01874v1 (2024)

### Evaluation Results

26. **[FACT]** Software professionals rated GPT-4.1 boundary test explanations with 63.5% positive ratings (4-5 on scale) and 17% negative ratings (1-2).
    - Source: arXiv:2601.22791 (2026)

27. **[FACT]** Knowledge distillation has demonstrated high effectiveness across benchmarks such as GLUE, SuperGLUE, and MMLU, where student models often retain over 95% of the teacher model's performance.
    - Source: Multiple sources (2024)

28. **[FACT]** The resample-then-rerank method proved most effective, improved factual accuracy by approximately 10% without reduction of text length.
    - Source: arXiv:2404.05411v1 (2024)

29. **[FACT]** Networks with higher neural efficiency that maintain good accuracy performance are able to perform better when classes were partially randomized in train.
    - Source: arXiv:2101.09671 (2021)

30. **[FACT]** To train with high-resolution datasets that contain many classes greatly reduces representation compression and improves transferability.
    - Source: arXiv:2403.01874v1 (2024)

---

## ASSUMPTIONS [SUMP]

### Evaluation Assumptions

31. **[SUMP]** Standard accuracy metrics systematically fail to detect subtle knowledge loss in compressed models.
    - Source: Synthesis across multiple sources

32. **[SUMP]** Aggregate performance measures conceal significant differences in how different classes and images are impacted by compression.
    - Source: arXiv:1911.05248 (2019)

33. **[SUMP]** Conventional evaluation metrics such as accuracy, mAP, IoU, or RMSE focus mainly on task performance and overlook how effectively the student internalizes the teacher's knowledge.
    - Source: MDPI Applied Biosciences (2024)

34. **[SUMP]** Since teacher models are typically pre-trained for versatility across a broad range of tasks, indiscriminate distillation can introduce unnecessary complexity when distill for a specific downstream task.
    - Source: Multiple sources (2024)

35. **[SUMP]** True robustness necessitates performance that works well against a diverse range of attack methods, not just one type.
    - Source: arXiv:2403.09441 (2024)

### Methodological Assumptions

36. **[SUMP]** Projected Gradient Descent (PGD) provides a lower bound of the maximum adversarial perturbation a model can withstand.
    - Source: arXiv:2403.09441 (2024)

37. **[SUMP]** The architectures and gradients of compressed models are usually not available, which necessitates black-box test approaches.
    - Source: ACM TOSEM (2022)

38. **[SUMP]** Boundary value analysis is fundamental in software quality assurance because faults tend to cluster at input extremes.
    - Source: arXiv:2601.22791 (2026)

39. **[SUMP]** LLMs have a tendency to hallucinate at test-time, and adversarial search can find specific prompt variations that trigger these failures.
    - Source: arXiv:2505.05665v2 (2025)

40. **[SUMP]** Capable models can be further steered at runtime to act even more or less desirably, which suggests offline analysis enables real-time intervention strategies.
    - Source: arXiv:2505.05665v2 (2025)

### Compression Behavior Assumptions

41. **[SUMP]** Different compression methods (KD vs. prune vs. quantization) show different vulnerabilities under adversarial conditions.
    - Source: Frontiers in Robotics and AI (2025)

42. **[SUMP]** There is a threshold until which models can be pruned efficiently without any significant effect on performance.
    - Source: arXiv:2407.15904 (2024)

43. **[SUMP]** Compression efficiency (measured as an approach to the rate-distortion frontier) correlates with out-of-distribution generalization.
    - Source: arXiv:2403.01874v1 (2024)

44. **[SUMP]** Naive compression strategies fail to maintain defensive properties against adversarial perturbations.
    - Source: ICCV 2019

---

## QUESTIONS [KHUE]

### Fundamental Evaluation Questions

45. **[KHUE]** What eval methodology can detect subtle knowledge loss from compression?
    - Source: Primary research question

46. **[KHUE]** How can we evaluate whether a model's OOD generalization capability is strong or not, and where a model generalizes well or poorly?
    - Source: arXiv:2403.01874v1 (2024)

47. **[KHUE]** What do compressed deep neural networks forget?
    - Source: arXiv:1911.05248 (2019)

48. **[KHUE]** How can we thoroughly understand the impact of model compression before model dissemination?
    - Source: ACM TOSEM (2022)

49. **[KHUE]** Can we have both semantic retention and extreme compression in LLMs?
    - Source: arXiv:2505.07289v1 (2025)

### Methodological Questions

50. **[KHUE]** What are the important parameters to test the deployability of a model?
    - Source: arXiv:2407.15904 (2024)

51. **[KHUE]** How can we find triggers that result in deviated behaviors in compressed models when architectures and gradients are not available?
    - Source: ACM TOSEM (2022)

52. **[KHUE]** How can we systematically characterize LLM failures in safety-critical decision contexts?
    - Source: arXiv:2505.05665v2 (2025)

53. **[KHUE]** How can we identify specific data points disproportionately affected by compression?
    - Source: arXiv:1911.05248 (2019)

54. **[KHUE]** How can we quantify the trade-off between compression and semantic preservation in LLMs?
    - Source: arXiv:2505.07289v1 (2025)

### Capability-Specific Questions

55. **[KHUE]** Why does perplexity fail to capture subtle degradation in knowledge-intensive tasks?
    - Source: arXiv:2505.07289v1 (2025)

56. **[KHUE]** Why do mathematical reason and instruction-follow show heightened vulnerability to compression?
    - Source: arXiv:2505.07289v1 (2025)

57. **[KHUE]** How can we detect semantic drift where generated text diverges from the subject matter designated by the prompt?
    - Source: arXiv:2404.05411v1 (2024)

58. **[KHUE]** What causes the representation compression 'tunnel' effect in deeper layers that degrades OOD performance?
    - Source: arXiv:2403.01874v1 (2024)

59. **[KHUE]** How can we assess whether compressed models maintain proper boundary handle?
    - Source: arXiv:2601.22791 (2026)

### Gap Identification Questions

60. **[KHUE]** How can we create a unified framework for compression evaluation across model types?
    - Source: Research synthesis

61. **[KHUE]** How can we predict post-compression performance without full evaluation?
    - Source: Research synthesis

62. **[KHUE]** What is the gap between benchmark performance and deployed behavior in compressed models?
    - Source: Research synthesis

63. **[KHUE]** How can we evaluate multi-turn dialogue and agentic task degradation in compressed LLMs?
    - Source: Research synthesis

---

## HYPOTHESES [HYPO]

### Core Evaluation Hypotheses

64. **[HYPO]** Multi-dimensional evaluation frameworks that go beyond simple accuracy metrics are essential to detect subtle knowledge loss.
    - Source: Research synthesis

65. **[HYPO]** Adversarial robustness tests (PGD, FGSM, C&W) reveal knowledge loss that standard accuracy metrics miss.
    - Source: Multiple sources (2019-2025)

66. **[HYPO]** Semantic drift detection and knowledge retention scores can quantify subtle degradation invisible to conventional metrics.
    - Source: arXiv:2404.05411v1 (2024), MDPI Applied Biosciences (2024)

67. **[HYPO]** Boundary value analysis and edge case tests detect knowledge loss at input extremes where faults concentrate.
    - Source: arXiv:2601.22791 (2026)

68. **[HYPO]** Out-of-distribution (OOD) generalization assessment reveals capability loss that in-distribution tests miss.
    - Source: arXiv:2403.01874v1 (2024)

69. **[HYPO]** Differential tests that compare compressed vs. uncompressed model behaviors can identify deviated behaviors.
    - Source: ACM TOSEM (2022)

70. **[HYPO]** Task-specific capability preservation metrics detect domain-specific degradation patterns.
    - Source: Multiple sources (2024)

### Specific Methodology Hypotheses

71. **[HYPO]** Pruned Identified Exemplars (PIEs) analysis can systematically identify specific data points disproportionately affected by compression.
    - Source: arXiv:1911.05248 (2019)

72. **[HYPO]** The Semantic Retention Compression Rate (SrCr) metric can unify compression-performance trade-off into a single interpretable measure.
    - Source: arXiv:2505.07289v1 (2025)

73. **[HYPO]** The Knowledge Retention Score (KRS) that integrates intermediate feature similarity with output agreement provides a holistic view of knowledge preservation.
    - Source: MDPI Applied Biosciences (2024)

74. **[HYPO]** Jensen-Shannon Divergence can measure distribution shifts between compressed and uncompressed outputs to detect subtle capability loss.
    - Source: Multiple sources (2024)

75. **[HYPO]** The Robustness Index (RI) defined as the area under the accuracy-perturbation curve quantifies degradation across perturbation levels.
    - Source: MDPI Information (2023)

76. **[HYPO]** Adaptive Stress Test (AST) with Monte-Carlo Tree Search can systematically characterize failure modes more efficiently than exhaustive tests.
    - Source: arXiv:2505.05665v2 (2025)

77. **[HYPO]** Shannon Entropy, Action Diversity, and Negative Reward metrics can quantify undesirability in agent behaviors of compressed models.
    - Source: arXiv:2505.05665v2 (2025)

78. **[HYPO]** A fitness function that prioritizes mutated inputs which cause large output differences or trigger previously unobserved probability vectors can find deviated behaviors.
    - Source: ACM TOSEM (2022)

### Compression Vulnerability Hypotheses

79. **[HYPO]** Knowledge-distilled models exhibit the most vulnerability to adversarial attacks, followed by pruned models, then quantized models.
    - Source: Frontiers in Robotics and AI (2025)

80. **[HYPO]** Compressed models maintain top-line performance metrics while exhibit significant degradation in edge cases, adversarial scenarios, and underrepresented data distributions.
    - Source: Research synthesis

81. **[HYPO]** Long-tail data and atypical examples show the greatest vulnerability to compression-induced knowledge loss.
    - Source: arXiv:1911.05248 (2019)

82. **[HYPO]** The representation compression 'tunnel' effect in deeper layers is responsible for OOD performance degradation.
    - Source: arXiv:2403.01874v1 (2024)

83. **[HYPO]** Combined compression approaches (e.g., 25% prune + 4-bit quantization) outperform extreme single-method compression.
    - Source: arXiv:2505.07289v1 (2025)

### Remediation Hypotheses

84. **[HYPO]** Adversarial fine-tune of just three epochs can recover 95% of robust performance lost in compression.
    - Source: arXiv:2403.09441 (2024)

85. **[HYPO]** Concurrent adversarial train with weight prune can maintain both efficiency and robustness.
    - Source: ICCV 2019

86. **[HYPO]** Task-aware selective knowledge distillation (TASKD-LLM) that transfers only task-relevant knowledge can prevent unnecessary complexity and improve compression outcomes.
    - Source: Multiple sources (2024)

87. **[HYPO]** The resample-then-rerank method can improve factual accuracy in compressed models without reduce output length.
    - Source: arXiv:2404.05411v1 (2024)

### Framework Hypotheses

88. **[HYPO]** A comprehensive evaluation framework should include standard baselines, adversarial tests, edge case analysis, semantic retention metrics, OOD generalization, differential analysis, and domain-specific evaluation across seven tiers.
    - Source: Research synthesis

89. **[HYPO]** Evaluation suites must include: (1) standard benchmark accuracy, (2) adversarial robustness tests, (3) OOD generalization assessment, (4) edge case and boundary tests, and (5) task-specific capability preservation metrics at minimum.
    - Source: Research synthesis

90. **[HYPO]** Staged evaluation with checkpoints at known thresholds (6-bit, 4-bit for quantization; 50%, 75%, 90% for prune) can predict catastrophic failure before it occurs.
    - Source: Multiple sources (2021-2024)

91. **[HYPO]** Confusion matrix analysis can detect class-specific degradation patterns even when overall accuracy appears stable.
    - Source: Research synthesis

92. **[HYPO]** For agentic LLMs, multi-task benchmarks that test plan, tool use, and control flow are necessary to detect capability loss.
    - Source: Research synthesis

93. **[HYPO]** Atomic fact extraction and truthfulness score can detect factual accuracy degradation in compressed language models.
    - Source: arXiv:2404.05411v1 (2024)

94. **[HYPO]** Feature similarity metrics for intermediate representations can detect internal knowledge degradation even when final outputs appear similar.
    - Source: MDPI Applied Biosciences (2024)

95. **[HYPO]** Transfer learn performance on held-out tasks indicates whether knowledge generalization is preserved after compression.
    - Source: Research synthesis

---

## METHODOLOGY KERNELS

### Specific Evaluation Techniques

96. **[FACT]** Projected Gradient Descent (PGD) uses multiple iterations of small perturbations with norm limits to test adversarial robustness.
    - Source: MDPI Information (2023)

97. **[FACT]** FGSM calculates the gradient for input examples, takes the sign of that gradient, multiplies it with a small real number, then adds the output to generate adversarial samples.
    - Source: MDPI Information (2023)

98. **[FACT]** Robustness of compressed models is assessed across multiple tasks with six evaluation metrics and four commonly used classical adversarial attacks.
    - Source: Frontiers in Robotics and AI (2025)

99. **[FACT]** Two formulations of Semantic Retention exist—Sr₁ weights each task equally, while Sr₂ provides weighted aggregation by original performance magnitude, with Sr₂ adopted as the primary metric.
    - Source: arXiv:2505.07289v1 (2025)

100. **[FACT]** The Semantic Drift Score measures the degree of separation between correct and incorrect facts in a paragraph.
     - Source: arXiv:2404.05411v1 (2024)

101. **[FACT]** Software professionals rated boundary test explanations on four dimensions: clarity, correctness, completeness, and perceived usefulness.
     - Source: arXiv:2601.22791 (2026)

102. **[FACT]** The Adaptive Stress Test framework formulates the search for problematic prompt perturbations as an adversarial search problem which uses Monte-Carlo Tree Search.
     - Source: arXiv:2505.05665v2 (2025)

103. **[FACT]** Dflare is a search-based, black-box test technique that automatically finds triggers which result in deviated behaviors in image classification tasks.
     - Source: ACM TOSEM (2022)

104. **[FACT]** Min-max robust optimization based adversarial train can provide a notion of security against adversarial attacks.
     - Source: ICCV 2019

105. **[FACT]** Train can be conducted for a specified number of epochs with early stop criteria based on validation loss improvement.
     - Source: arXiv:2101.09671 (2021)

### Specific Parameter Values

106. **[FACT]** For Fashion-MNIST, adversarial perturbation bound ε = 0.1 is used in PGD attacks.
     - Source: arXiv:2403.09441 (2024)

107. **[FACT]** For CIFAR10, adversarial perturbation bound ε = 8/255 is used in PGD attacks.
     - Source: arXiv:2403.09441 (2024)

108. **[FACT]** Task-specific metrics include accuracy for classification, mAP for object detection, F1/EM for language tasks, and FID for generation tasks.
     - Source: arXiv:2407.15904 (2024)

109. **[FACT]** Adaptive Stress Test encompasses three distinct environments: Autonomous drive, Robot crowd navigation, and Lunar lander.
     - Source: arXiv:2505.05665v2 (2025)

110. **[FACT]** Three undesirability metrics for agent evaluation: Shannon Entropy (inconsistency), Action Diversity (instability), and Negative Reward (failure).
     - Source: arXiv:2505.05665v2 (2025)

---

## META-INSIGHTS

### Research Synthesis Kernels

111. **[FACT]** The research examined 15+ authoritative sources from 2019-2026 on evaluation methodologies for compression-induced knowledge loss.
     - Source: Document metadata

112. **[HYPO]** Sophisticated evaluation methodologies are essential because compressed models often maintain top-line performance metrics while exhibit significant degradation in edge cases, adversarial scenarios, and underrepresented data distributions.
     - Source: Research executive summary

113. **[HYPO]** Five critical gaps remain: (1) standardized benchmarks across model types, (2) composite interpretable metrics, (3) predictive evaluation methods, (4) real-world validation, and (5) interactive evaluation for multi-turn dialogue.
     - Source: Research synthesis

114. **[HYPO]** Future research should focus to develop unified evaluation frameworks that balance comprehensiveness with computational efficiency while provide interpretable insights into specific degradation patterns.
     - Source: Research synthesis

### Practical Recommendations

115. **[HYPO]** To establish adversarial robustness benchmarks before compression, then evaluate compressed models against PGD-ℓ∞, FGSM, and C&W attacks with minimum Robustness Index thresholds is recommended practice.
     - Source: Research synthesis

116. **[HYPO]** Never rely on perplexity alone for LLM evaluation is critical because it often fails to capture knowledge-intensive task degradation.
     - Source: Research synthesis

117. **[HYPO]** To implement differential test pipelines that compare outputs between compressed and original models can systematically identify deviated behaviors.
     - Source: Research synthesis

118. **[HYPO]** Design evaluation suites that match deployment domain, which include reason benchmarks (GSM8K, MATH), instruction-follow (IFEval), knowledge probes (MMLU), and factual accuracy (TruthfulQA) for LLMs.
     - Source: Research synthesis

119. **[HYPO]** For vision models, test across complexity levels (simple vs. complex scenes, common vs. rare objects) is essential for comprehensive evaluation.
     - Source: Research synthesis

120. **[HYPO]** To implement AST or similar adversarial search methods can systematically discover failure modes more efficiently than rely on random test cases.
     - Source: Research synthesis

---

## Total Kernel Count: 120 kernels
- **FACTS**: 30 kernels (entries 1-30)
- **ASSUMPTIONS**: 14 kernels (entries 31-44)
- **QUESTIONS**: 19 kernels (entries 45-63)
- **HYPOTHESES**: 57 kernels (entries 64-120)
