# Knowledge Kernels: Q26 - Prompt Cache & KV-Cache Compression Interaction

## Research Question
How do prompt cache and KV-cache compression interact with semantic compression? Orthogonal optimizations that could stack?

---

## [FACT] Kernels - Grounded, Provable Knowledge

### F1: Three Optimization Axes
Prompt cache operates at the request level (reuses computed KV states across API calls), KV-cache compression operates at the attention mechanism level (reduces memory footprint in active inference), and semantic compression operates at the input level (reduces redundancy before process begins).
**Source:** Executive Summary, lines 7-9

### F2: Multiplicative Performance Gains
When combined, these three techniques compound their benefits and enable systems to achieve 15x throughput improvements, 90% cost reductions, and support for contexts 6-8x longer than baseline systems.
**Source:** Executive Summary, lines 11-12; LMCache, line 82-94

### F3: ChunkKV Semantic Preservation
ChunkKV treats consecutive token sequences (default: 10 tokens) as indivisible units when compressed, which preserves complete semantic structures like subjects, predicates, and objects together.
**Source:** ChunkKV, lines 23-24

### F4: ChunkKV Cross-Layer Index Similarity
ChunkKV demonstrates superior cross-layer index similarity (57.74% for LLaMA-3-8B) compared to token-level approaches (27.95% for SnapKV), enables layer-wise index reuse optimization that reduces computational overhead by 20% with minimal accuracy degradation (≤0.6%).
**Source:** ChunkKV, lines 35-36

### F5: ChunkKV Performance Improvement
ChunkKV outperforms state-of-the-art methods by up to 8.7% in precision while maintains the same compression ratio, with layer-wise index reuse delivers up to 26.5% improvement in throughput.
**Source:** ChunkKV, lines 43-46

### F6: LMCache Architecture Components
LMCache includes four core components: KV Connector (standardized API), Token Processor (identifies prefix matches), Storage Manager (orchestrates data movement), and Event Manager (coordinates async operations).
**Source:** LMCache, lines 69-73

### F7: LMCache Batched Data Movement
LMCache groups scattered pages into larger chunks (default 256 tokens) uses stream GPU buffers for efficient data movement.
**Source:** LMCache, lines 66, 86

### F8: LMCache Throughput Improvement
Combination of LMCache with vLLM achieves up to 15x improvement in throughput across workloads such as multi-round question answer and document analysis.
**Source:** LMCache, lines 94-95

### F9: LMCache Production Hit Rates
Production deployments revealed prefix cache hit rates exceeded expectations (50% for Company G), though context truncation significantly degraded hit ratios (85% → 45%).
**Source:** LMCache, lines 88-90

### F10: PALU Low-Rank Decomposition
PALU decomposes Key and Value projection weight matrices W into two low-rank matrices A and B, caches compressed intermediate states that are reconstructed on-the-fly in attention computation.
**Source:** PALU, lines 109-111

### F11: PALU Compression Ratios
When combined low-rank compression with quantization, PALU achieves over 91.25% compression (11.4× reduction) while yields 1.19 lower perplexity than KVQuant at 2-bit quantization.
**Source:** PALU, lines 131-134

### F12: PALU Speedup Performance
PALU achieves up to 2.91× speedup for RoPE-based attention and maintains <1% accuracy degradation at 30% compression while supports 7.59× total compression when combined with 3-bit quantization.
**Source:** PALU, lines 135-138

### F13: PALU Walsh-Hadamard Integration
PALU integrates Walsh-Hadamard Transform (WHT) directly into decomposed weights via W ≈ AB = (AR)(R^T B) = Â B̂, eliminates runtime overhead while enables effective low-bit quantization by addresses severe outliers.
**Source:** PALU, lines 113-114, 129-130

### F14: Semantic Compression Context Extension
Semantic compression enables LLMs to process documents 6-8 times longer than their standard context window without fine-tune or parameter modifications.
**Source:** Semantic Compression, lines 151-153

### F15: Semantic Compression Complexity Reduction
Complexity reduces from O(L²) to approximately γ₂²/γ₁·L + α²L², where α represents compression ratio, yields quadratic speedup proportional to compression magnitude.
**Source:** Semantic Compression, lines 158-159

### F16: Semantic Compression Passkey Performance
Passkey Retrieval maintains >90% accuracy at 30k tokens (vs. baseline failure at ~5k), and when combined with YaRN interpolation achieves 90%+ accuracy at 60k+ tokens.
**Source:** Semantic Compression, lines 161-162

### F17: StreamLLM Attention Sink Phenomenon
Initial tokens receive disproportionately high attention scores regardless of semantic relevance because SoftMax requires attention scores to sum to one, causes models to allocate excess attention to initial tokens as natural "sinks" when current queries lack strong semantic matches.
**Source:** StreamLLM, lines 194-196

### F18: StreamLLM Cache Structure
StreamLLM maintains a two-part cache: (1) attention sink tokens (typically 4 initial tokens) that stabilize attention computation, and (2) a roll KV cache that retains the most recent tokens crucial for language model.
**Source:** StreamLLM, lines 196-197

### F19: StreamLLM Performance Metrics
StreamLLM enables stable process of 4+ million tokens and achieves up to 22.2× speedup vs. slide window with recomputation, works across Llama-2, MPT, Falcon, and Pythia models.
**Source:** StreamLLM, lines 201-204, 220

### F20: Anthropic Prompt Cache Cost Structure
Cache writes cost 1.25x base input token price (5-min TTL) or 2x (1-hour TTL), while cache reads cost only 0.1x base input token price (90% savings).
**Source:** Anthropic Prompt Cache, lines 244-246

### F21: Anthropic Prompt Cache TTL
Default 5-minute TTL (refreshed on each use), with optional 1-hour TTL at 2x cost.
**Source:** Anthropic Prompt Cache, lines 243, 266

### F22: Anthropic Prompt Cache Minimum Lengths
4096 tokens for Claude Opus 4.6/4.5 and Haiku 4.5; 1024 tokens for Sonnet 4.5 and Opus 4.1/4; 2048 tokens for Haiku 3.5 and 3.
**Source:** Anthropic Prompt Cache, lines 247-250

### F23: Anthropic Prompt Cache Breakpoints
Up to 4 cache breakpoints can be defined in a prompt, with automatic prefix check works backwards from explicit breakpoints up to 20 blocks.
**Source:** Anthropic Prompt Cache, lines 252, 268

### F24: Prompt Cache Output Equivalence
Cached prompts produce identical outputs to non-cached prompts - the optimization is purely computational.
**Source:** Anthropic Prompt Cache, line 264

### F25: FlashAttention and KV Cache Orthogonality
FlashAttention and KV cache solve orthogonal bottlenecks: KV cache skips redundant work, while FlashAttention makes the left work (computes attention for the current token over n keys) faster and more memory-efficient.
**Source:** Optimization Orthogonality, lines 302-303

### F26: Expected Attention Compression Rate
Expected Attention achieves 60% cache prune while preserves performance quality without architectural modifications or additional train.
**Source:** Expected Attention, lines 330-331, 343

### F27: Expected Attention Train-Free Operation
Unlike learnable eviction methods, Expected Attention requires no fine-tune or model modification, makes it immediately applicable to any pretrained LLM.
**Source:** Expected Attention, lines 337-338, 345

### F28: LLM as Compression Tool
Chinchilla 70b model achieved raw compression rates of 8.3% on text, 48.0% on images, and 21.0% on audio, outperforms purpose-built compressors like PNG and FLAC.
**Source:** Information Theory, lines 367-369

### F29: Shannon Source Code Theorem Application
Maximization of log-likelihood of a model is equivalent to minimization of expected code length when use that model for compression via arithmetic code.
**Source:** Information Theory, lines 382-387

### F30: H2O Heavy-Hitter Insight
A small portion of tokens (heavy hitters) contribute most of the value when computes attention scores, enables dynamic KV cache eviction.
**Source:** H2O, lines 408-409, 424-425

### F31: H2O Throughput Improvements
H2O with 20% heavy hitters improves throughput over DeepSpeed Zero-Inference, HuggingFace Accelerate, and FlexGen by up to 29x, 29x, and 3x respectively on OPT-6.7B and OPT-30B.
**Source:** H2O, lines 416-417, 430-431

### F32: H2O Positional Bias
Accumulated attention scores have an intrinsic positional bias where they gradually decrease with token position, causes disproportionate eviction of later tokens.
**Source:** H2O, lines 418-419, 432

### F33: SentenceKV Two-Stage Operation
In prefill, segments input into sentences and stores sentence embeddings on GPU and detailed KVs on CPU; in decode, retrieves semantically relevant sentences based on query similarity and loads correspondent KVs for attention.
**Source:** SentenceKV, lines 453-458

### F34: Context Compression Magnitude
Three core techniques—summarization, keyphrase extraction, and semantic chunk—can achieve 5–20x compression while maintains or improves accuracy.
**Source:** Context Compression, lines 548, 558

### F35: DistilBERT Compression Example
DistilBERT demonstrates compression power by shrink of BERT by 40% while retention of 97% of language understand capabilities and operation 60% faster.
**Source:** NVIDIA Optimization, lines 495-496, 520

### F36: Sparsity and Quantization Stack
Sparse representations can be combined with quantization to achieve greater speedups than either technique alone.
**Source:** NVIDIA Optimization, lines 507-510

### F37: Model Release Velocity
On average, 15–20 new open-weight models are released every week in 2025.
**Source:** LMCache, line 84

---

## [SUMP] Kernels - Assumptions

### S1: Semantic Chunk Size Assumption
ChunkKV assumes that complete semantic information usually appears in a continuous sequence, with a default chunk size of 10 tokens be appropriate to preserve semantic units.
**Source:** ChunkKV, lines 23-24, 41

### S2: Attention Sink Universality
StreamLLM assumes that the attention sink phenomenon (disproportionate attention to initial tokens) is universal across autoregressive LLMs trained with finite-length attention windows.
**Source:** StreamLLM, lines 192-196

### S3: Zipf Law Application
Semantic compression assumes that Zipf law observation—"a small set of the most frequent word tokens in a large corpus account for almost all occurrences"—can be leveraged for effective compression.
**Source:** Semantic Compression, lines 152-153, 172

### S4: Cache Reuse Patterns
LMCache implementation assumes that common prefixes (system prompts, documents) are frequently reused across queries in production deployments.
**Source:** LMCache, lines 76-78

### S5: Sentence-Level Semantic Coherence
SentenceKV assumes that sentence boundaries represent meaningful semantic units for cache organization and retrieval.
**Source:** SentenceKV, lines 447-450

### S6: Fixed-Size Chunk Limitation
Traditional fixed-size chunk (500–1000 characters with 10–20% overlap) is assumed to ignore semantic boundaries and potentially fragment critical context.
**Source:** Late Chunk, lines 536-538

### S7: Future Query Distribution Predictability
Expected Attention assumes that the distribution of future queries can be estimated with sufficient accuracy to enable proactive prune of KV cache.
**Source:** Expected Attention, lines 329-331

### S8: Optimization Independence Assumption
The research assumes that optimizations operate at different levels (input, computation, storage, system) maintain clean separation of concerns and can be composed without destructive interference.
**Source:** Synthesis, lines 626-631

### S9: Production Cache Layers
Production systems are assumed to benefit from implement of multiple cache layers: Semantic Cache (100% savings) → Prefix Cache (50-90% savings) → Full Inference.
**Source:** Anthropic Prompt Cache, line 270

### S10: Fisher Information for Rank Allocation
PALU assumes that Fisher information metrics can accurately estimate weight matrix importance for automatic rank allocation across layers.
**Source:** PALU, lines 117-118

---

## [KHUE] Kernels - Questions

### Q1: Optimal Compression Ratio Per Layer
What are the optimal compression ratios for each optimization layer (semantic, KV cache, prompt cache) for different workload types?
**Context:** Synthesis discusses multiplicative benefits but does not specify optimal ratios for specific use cases (lines 593-611)

### Q2: Cache Invalidation Coordination
How should cache invalidation be coordinated across multiple optimization layers to maintain consistency without sacrifice performance?
**Context:** Synthesis mentions coordination requirements (lines 618-620, 688), but detailed mechanisms are not explored

### Q3: Hardware-Specific Optimization Stack
How do optimal optimization stacks vary across different hardware architectures (GPU types, CPU-GPU configurations, specialized AI accelerators)?
**Context:** Future directions mention hardware-aware optimization (lines 698-702) but lack empirical data

### Q4: Learnable Compression Coordination
Can models be trained to dynamically predict optimal compression ratios and cache policies based on workload patterns?
**Context:** Future directions suggest this (lines 693-697), but no implementations are discussed

### Q5: Attention Sink Mitigation
Beyond StreamLLM solution, are there alternative approaches to mitigate or eliminate the attention sink phenomenon entirely?
**Context:** StreamLLM identifies the phenomenon (lines 192-196) but does not explore alternative solutions

### Q6: Semantic Boundary Detection
What is the optimal method for detect semantic boundaries for chunk: fixed-size, sentence-based, embed-similarity-based, or learned boundaries?
**Context:** Multiple sources discuss different approaches (ChunkKV, SentenceKV, Late Chunk) without definitive comparison

### Q7: Cross-Layer Optimization Theory
What are the formal information-theoretic bounds on compression when stack multiple optimization techniques?
**Context:** Information theory foundation is discussed (lines 359-397) but formal bounds for stacked optimizations are not provided

### Q8: Diminish Returns Threshold
At what point do diminish returns from additional optimization layers make further stack counterproductive?
**Context:** Synthesis mentions diminish returns (lines 681-685) but lacks quantitative thresholds

### Q9: Position Encode Coordination
How should positional encode be coordinated across semantic compression, KV cache compression, and prompt cache to maintain correctness?
**Source:** StreamLLM mentions positional encode (line 197), but cross-optimization coordination is not detailed

### Q10: Lost-in-the-Middle Mitigation
How do semantic compression techniques interact with the "lost-in-the-middle" problem in long-context models?
**Context:** Late Chunk identifies the problem (lines 546-547) but interaction with other optimizations is unclear

### Q11: Optimal Cache Strategy Mix
What is the optimal mix of context cache, prefill-decode disaggregation, and chunk-level cache for different application types?
**Context:** LMCache supports multiple strategies (lines 75-78) but optimal selection criteria are not specified

### Q12: Quantization Outlier Management
Beyond PALU WHT approach, what other methods can address outliers introduced by low-rank compression to enable effective quantization?
**Context:** PALU solves this for their approach (lines 113-115) but alternative methods are not explored

### Q13: Dynamic vs. Static Compression
Under what conditions do dynamic compression methods (like H2O) outperform static methods (like ChunkKV), and can they be effectively combined?
**Context:** Both H2O (lines 407-436) and ChunkKV (lines 15-53) are discussed separately without direct comparison

### Q14: Semantic Compression Quality Metrics
What metrics beyond perplexity and task accuracy should be used to evaluate semantic compression quality?
**Context:** Various performance metrics are mentioned (lines 160-165) but comprehensive quality evaluation is not discussed

### Q15: Multi-Model Cache Share
Can KV cache be effectively shared across different model architectures or sizes to further improve efficiency?
**Context:** LMCache works across engines (lines 63-65) but cross-model share is not explored

---

## [HYPO] Kernels - Hypotheses

### H1: Multiplicative Benefit Hypothesis
Stack orthogonal optimizations yields multiplicative benefits rather than additive benefits, with potential for 100x+ total improvement when all three axes are optimized.
**Basis:** Evidence from PALU (11.4x > 2x + 4x) and synthesis claim (line 727)
**Status:** Partially validated by specific examples, requires broader empirical validation

### H2: Three-Axis Orthogonality Hypothesis
Prompt cache, KV-cache compression, and semantic compression operate on completely orthogonal axes (temporal, computational, input) and can be independently implemented and composed without architectural dependencies.
**Basis:** Synthesis conclusions (lines 570-591)
**Status:** Strong support evidence from multiple sources, requires formal proof

### H3: Semantic Awareness Superiority Hypothesis
Semantic-aware compression methods (ChunkKV, SentenceKV) consistently outperform token-level methods across diverse tasks because they preserve complete semantic units.
**Basis:** ChunkKV performance (lines 43-48), cross-layer similarity (lines 35-36)
**Status:** Evidence from specific methods, requires systematic comparison

### H4: Optimal Phase Order Hypothesis
Organizations achieve maximum benefit by implement optimizations in the specific order: (1) prompt cache, (2) KV compression, (3) semantic compression, (4) advanced stack.
**Basis:** Practical implementation strategy (lines 634-656)
**Status:** Proposed strategy without empirical validation across organizations

### H5: Attention Distribution Coordination Hypothesis
All KV cache compression methods must account for attention distribution patterns (includes attention sinks) to avoid performance collapse, regardless of other optimization techniques used.
**Basis:** StreamLLM finds (lines 192-224)
**Status:** StreamLLM demonstrates the problem, but universal applicability requires broader test

### H6: Information-Theoretic Orthogonality Hypothesis
Semantic compression (lossy source code), KV compression (computational optimization), and prompt cache (memoization) operate on fundamentally different information-theoretic principles, ensures theoretical orthogonality.
**Basis:** Information theory foundation (lines 359-397)
**Status:** Theoretical framework proposed, requires formal mathematical proof

### H7: Layer-Wise Optimization Differential Hypothesis
Different layers in LLMs require different compression ratios and strategies, with Fisher information or similar metrics enable optimal per-layer configuration.
**Basis:** PALU automatic rank allocation (lines 117-118)
**Status:** Demonstrated for low-rank compression, generalization to other methods unproven

### H8: Compound Savings Hypothesis
When prompt cache and KV cache compression are combined, the savings compound rather than simply add, because cache stores compressed representations that are cheaper to reuse.
**Basis:** Optimization orthogonality discussion (line 310)
**Status:** Logical inference from architecture, lacks direct empirical measurement

### H9: Semantic Compression Context Quality Hypothesis
Semantic compression not only reduces data volume but can enhance quality by removes noise and focuses on salient information, leads to improved accuracy on downstream tasks.
**Basis:** Late Chunk results show maintained or improved accuracy with compression (line 558)
**Status:** Demonstrated in specific cases, requires systematic study across task types

### H10: Cache Hit Rate Degradation Hypothesis
Context truncation or aggressive semantic compression significantly degrades cache hit rates by fragments previously cached prefixes, creates a trade-off between compression and cache efficiency.
**Basis:** LMCache production metrics show 85% → 45% degradation (line 90)
**Status:** Observed in production, mechanisms and mitigation strategies need investigation

### H11: Pre-trained Sink Token Hypothesis
Add a dedicated learnable sink token when pretrain eliminates the need for multiple initial tokens in inference and improves efficiency without sacrifice stability.
**Basis:** StreamLLM pre-train enhancement (lines 206-207)
**Status:** Proposed solution, requires empirical validation across models

### H12: Positional Bias Universality Hypothesis
Accumulated attention scores in all autoregressive models exhibit intrinsic positional bias that affects compression decisions, requires semantic-aware methods to compensate.
**Basis:** H2O positional bias find (lines 418-419, 432)
**Status:** Observed in H2O, universality across models and architectures unproven

### H13: Cross-Layer Index Reuse Hypothesis
Higher cross-layer index similarity in semantic chunk methods enables novel optimizations (like layer-wise reuse) that further amplify efficiency gains beyond compression alone.
**Basis:** ChunkKV layer-wise index reuse optimization (lines 35-36, 46)
**Status:** Demonstrated for ChunkKV, applicability to other semantic methods untested

### H14: Memory Hierarchy Semantic Bridge Hypothesis
Semantic awareness enables intelligent data placement across memory hierarchies (GPU/CPU/disk), with compact semantic representations on fast storage direct detailed data load from slower storage.
**Basis:** SentenceKV architecture (lines 447-475)
**Status:** Demonstrated for sentence-level granularity, optimal granularity and generalization unclear

### H15: Quantization-Compression Synergy Hypothesis
Low-rank compression creates outliers that hinder quantization, but appropriate transformations (like WHT) can eliminate outliers and enable these techniques to synergize for superior combined compression.
**Basis:** PALU WHT integration (lines 113-115, 127-130)
**Status:** Proven for PALU specific approach, alternative transformations and broader applicability unexplored

---

## Summary Statistics

- **Total Kernels Extracted:** 86
  - [FACT]: 37 kernels
  - [SUMP]: 10 kernels
  - [KHUE]: 15 kernels
  - [HYPO]: 15 kernels

---

## Meta-Analysis

### Kernel Density by Source
- ChunkKV: 5 facts, 1 assumption, 1 hypothesis
- LMCache: 5 facts, 1 assumption, 1 hypothesis
- PALU: 5 facts, 1 assumption, 2 hypotheses
- Semantic Compression: 3 facts, 1 assumption, 0 hypotheses
- StreamLLM: 4 facts, 2 assumptions, 2 hypotheses
- Anthropic Prompt Cache: 6 facts, 1 assumption, 0 hypotheses
- Optimization Orthogonality: 1 fact, 1 assumption, 2 hypotheses
- Expected Attention: 2 facts, 1 assumption, 0 hypotheses
- Information Theory: 2 facts, 0 assumptions, 1 hypothesis
- H2O: 3 facts, 0 assumptions, 1 hypothesis
- SentenceKV: 1 fact, 1 assumption, 1 hypothesis
- NVIDIA Optimization: 2 facts, 0 assumptions, 0 hypotheses
- Late Chunk: 1 fact, 1 assumption, 1 hypothesis
- Synthesis: Multiple support evidence across all kernel types

### Key Research Gaps Identified
1. **Formal Mathematical Proofs:** While empirical evidence is strong, formal information-theoretic proofs of orthogonality are absent
2. **Cross-Organization Validation:** Implementation strategies are proposed but not validated across diverse organizations
3. **Hardware-Specific Optimization:** Limited data on how optimal stacks vary across hardware architectures
4. **Dynamic Adaptation:** Learnable coordination and adaptive policies are suggested but not implemented
5. **Comprehensive Benchmark:** Direct comparisons between alternate methods (semantic vs. token-level, dynamic vs. static) are limited

### Highest-Confidence Conclusions
1. **Three-axis orthogonality** (temporal, computational, input) is well-supported by multiple independent sources
2. **Multiplicative benefits** when stacked are demonstrated with concrete examples (15x, 11.4x improvements)
3. **Semantic-aware methods** show consistent advantages over token-level approaches in preserve information
4. **Production viability** is confirmed by real-world deployments (LMCache, Anthropic cache)
5. **Architectural independence** is maintained through clean interfaces and separation of concerns
