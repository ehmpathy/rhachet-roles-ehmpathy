# Knowledge Kernels: Q13 - Define Semantic Preservation for Agent Briefs

**Source**: /home/vlad/git/ehmpathy/_worktrees/rhachet-roles-ehmpathy.vlad.brief-minification/.research/v2025_02_09.brief-minification/probe/q13.probe.research.response.v1.i1.md

**Date Extracted**: 2026-02-09

---

## FACTS [FACT]

### F1: Statistical Independence of Dimensions
Constraint compliance and semantic accuracy are statistically independent (r=0.193, p=0.084). Systems can produce semantically correct outputs while they violate constraints, and vice versa.
**Source**: Source 1 - arXiv:2512.17920v1

### F2: Constraint Violation Magnitude
Constraint violation magnitudes are 2.9× larger than semantic change magnitude across compression levels. Instruction-follow failures dominate over knowledge degradation.
**Source**: Source 1 - arXiv:2512.17920v1

### F3: Inter-rater Reliability for Constraint Compliance
Inter-rater reliability shows almost perfect agreement on constraint compliance (Fleiss' κ=0.90) versus fair agreement on semantic accuracy (κ=0.25). This validates constraint compliance as objectively measurable.
**Source**: Source 1 - arXiv:2512.17920v1

### F4: Constraint Violations Peak at Medium Compression
97.2% of experiments show constraint compliance peaks at both extremes (c=0.0 and c=1.0) and declines to a trough at medium compression (c=0.5).
**Source**: Source 1 - arXiv:2512.17920v1

### F5: RLHF as Root Cause of Constraint Violations
RLHF ablation experiments remove helpfulness signals and improve constraint compliance by 598% at c=0.5, with 79% achievement of perfect compliance. RLHF-trained behaviors are the dominant cause of constraint violations.
**Source**: Source 1 - arXiv:2512.17920v1

### F6: Token Compression Ratios
Symbolic metalanguage compression achieves 62-81% token reduction across task families.
**Source**: Source 2 - arXiv:2601.07354

### F7: Semantic Equivalence Rates by Model
Gemini 2.5 Flash achieves 75% semantic equivalence on selection tasks (highest observed among tested models).
**Source**: Source 2 - arXiv:2601.07354

### F8: Constraint Composition Failure
Constraint composition fails across all tested models with near-zero equivalence. Models do not reliably interpret compound constraints (∩ operations).
**Source**: Source 2 - arXiv:2601.07354

### F9: Performance Paradox with Symbolic Instructions
Kimi K2 achieves 100% accuracy on selection tasks with MetaGlyph prompts versus 90.8% with natural language. Symbolic instructions can outperform prose for certain model-task combinations.
**Source**: Source 2 - arXiv:2601.07354

### F10: Entity Preservation Rates
xRAG compression recovers approximately 66% semantic similarity but preserves only 28% of entities on average. Dates and numerical values show particularly poor retention (22% and 26% respectively).
**Source**: Source 6 - arXiv:2503.19114

### F11: Multi-token Integration Challenge
xRAG struggles to integrate information across multiple compressed tokens. Performance drops 20-30 BERTScore points.
**Source**: Source 6 - arXiv:2503.19114

### F12: LLM Default to Surface Similarity
LLMs generalize poorly beyond observed inputs and default to surface similarity rather than semantic preservation in code translation tasks.
**Source**: Source 3 - UC Berkeley EECS-2025-174

---

## ASSUMPTIONS [SUMP]

### S1: Multi-Dimensional Nature of Semantic Preservation
Semantic preservation for agent briefs is a multi-dimensional construct that combines behavioral equivalence, constraint compliance, and observable interaction patterns, which are statistically independent.
**Source**: Synthesis across Sources 1-15

### S2: Verification Independence Requirement
Behavioral equivalence and constraint compliance must be verified independently because they are orthogonal dimensions.
**Source**: Source 1 - arXiv:2512.17920v1

### S3: Context-Dependent Behavior
Agent behaviors emerge from system-level integration rather than solely from model architecture. Context is essential to behavioral analysis.
**Source**: Source 15 - arXiv:2506.06366v2

### S4: Observational Sufficiency
Observable behaviors (input-output maps and externally visible side effects) are sufficient to determine semantic equivalence without examination of internal process.
**Source**: Source 4 - Pitts (2000), Source 11 - PLS Lab

### S5: Metamorphic Relations as Ground Truth
Metamorphic relations (e.g., commutativity, idempotence, monotonicity) can serve as reliable proxies for semantic correctness when absolute oracles are unavailable.
**Source**: Source 10 - Segura & Towey

### S6: Refinement Map Completeness
With addition of auxiliary variables (history/prophecy variables), any valid implementation can be proven to refine its specification through refinement maps.
**Source**: Source 5 - Abadi & Lamport (1991)

### S7: Formal Specification Representability
Agent briefs can be adequately represented in formal specification languages that support semantic denotation and verification.
**Source**: Synthesis across Sources 3, 4, 7

### S8: Test Suite Representativeness
Finite test suites with equivalence class samples can adequately represent infinite input spaces for I/O equivalence tests.
**Source**: Source 3 - UC Berkeley EECS-2025-174, Source 12 - Katalon

### S9: Constraint Salience Preservation
Maintain constraints at specification extremes (very short or very long) rather than medium lengths to preserve their salience and reduce ambiguity.
**Source**: Source 1 - arXiv:2512.17920v1

### S10: Compositional Proof Validity
Modular, compositional proof techniques can be combined to verify semantic preservation of complex brief transformations.
**Source**: Source 14 - ScienceDirect (2015)

---

## QUESTIONS [KHUE]

### Q1: Constraint Composition Problem
How can we enable LLMs to reliably interpret compound constraints (intersection operations) when current models show near-zero equivalence on constraint composition tasks?
**Source**: Source 2 - arXiv:2601.07354

### Q2: Liveness Property Verification
How can we verify liveness properties (agent eventually does X) when current verification methods focus primarily on safety properties (agent must not do Y)?
**Source**: Source 5 - Abadi & Lamport (1991), Synthesis

### Q3: Prophecy Variable Discovery
What practical methods can be developed to discover necessary prophecy variables to establish refinement maps in agent brief verification?
**Source**: Source 5 - Abadi & Lamport (1991)

### Q4: Scalability of Contextual Equivalence
How can we make exhaustive contextual equivalence tests computationally tractable for complex agents with large context spaces?
**Source**: Synthesis across Sources 4, 11

### Q5: Task-Adaptive Verification
What strategies can adapt verification approaches to different task types given that preservation rates vary dramatically by task (75% for selection, near-zero for constraint composition)?
**Source**: Source 2 - arXiv:2601.07354

### Q6: Emergent Behavior Prediction
How can we predict emergent behaviors that arise from agent-environment interaction through brief analysis alone?
**Source**: Source 15 - arXiv:2506.06366v2

### Q7: RLHF Constraint Interference
How can RLHF train be modified to avoid the 598% increase in constraint violations while it maintains helpfulness?
**Source**: Source 1 - arXiv:2512.17920v1

### Q8: Entity Preservation Enhancement
How can entity preservation rates (currently 22-28% for dates/numbers) be improved when we compress briefs to meet practical requirements (≥50%)?
**Source**: Source 6 - arXiv:2503.19114

### Q9: Optimal Compression Length
What is the optimal brief length that balances compression efficiency with constraint salience and avoids the medium-length ambiguity zone?
**Source**: Source 1 - arXiv:2512.17920v1

### Q10: Cross-Model Semantic Equivalence
Why do different models show vastly different semantic equivalence rates (0-100%) for the same task, and how can verification account for model-specific behaviors?
**Source**: Source 2 - arXiv:2601.07354

### Q11: Oracle Problem in Agent Tests
How can we address the oracle problem for agent behaviors where determination of "correct" outputs is inherently difficult or subjective?
**Source**: Source 9 - IEEE TSE (2014)

### Q12: Formal Specification Language Design
What properties should a domain-specific language for agent briefs have to optimally support semantic denotation, constraint specification, and automated verification?
**Source**: Synthesis across Sources 3, 4, 7, 14

---

## HYPOTHESES [HYPO]

### H1: Three-Dimensional Verification Framework
A verification framework that tests behavioral equivalence, constraint compliance, and contextual equivalence independently will achieve higher semantic preservation detection rates than single-dimensional approaches.
**Source**: Synthesis across Sources 1-15

### H2: Extreme Length Strategy
Agent briefs that are designed at specification extremes (very concise <5 words or detailed >100 words) will show significantly better constraint compliance than medium-length briefs (~27 words).
**Source**: Source 1 - arXiv:2512.17920v1

### H3: Metamorphic Relations as Oracle Replacement
Comprehensive metamorphic relations (commutativity, idempotence, monotonicity) can replace absolute test oracles for agent brief verification with comparable effectiveness.
**Source**: Source 10 - Segura & Towey

### H4: Symbolic Augmentation for Constraints
Augment natural language briefs with symbolic operators for critical constraints to improve constraint compliance without sacrifice of semantic accuracy.
**Source**: Source 2 - arXiv:2601.07354

### H5: Auxiliary Variable Necessity
Agent briefs will require auxiliary variables (history or prophecy) to establish complete refinement maps between abstraction levels in at least 30% of cases.
**Source**: Source 5 - Abadi & Lamport (1991)

### H6: Behavioral-Constraint Trade-off
There exists a trade-off curve between behavioral equivalence and constraint compliance when we optimize briefs, with an optimal operation point around 85% behavioral equivalence and 95% constraint satisfaction.
**Source**: Synthesis across Sources 1, 2, 6

### H7: Context-Dependent Preservation Rates
Semantic preservation rates will vary by at least 30% across different operational contexts, even for identical brief transformations.
**Source**: Source 15 - arXiv:2506.06366v2

### H8: CSP-Based Constraint Verification
Formalization of agent brief constraints as Constraint Satisfaction Problems will enable automated verification with >95% accuracy for detection of constraint violations.
**Source**: Source 13 - Semantic Scholar

### H9: Reconstruction Test Correlation
Reconstruction test performance (ability to regenerate original brief from compressed version) will correlate strongly (r>0.7) with downstream task preservation.
**Source**: Source 6 - arXiv:2503.19114

### H10: RLHF Removal Trade-off
Removal of RLHF helpfulness signals will improve constraint compliance by >500% but reduce semantic accuracy by 10-20% on complex reason tasks.
**Source**: Source 1 - arXiv:2512.17920v1

### H11: Equivalence Partition Efficiency
Use of equivalence partition to reduce test case count by 80% will maintain >95% defect detection capability for behavioral equivalence tests.
**Source**: Source 12 - Katalon

### H12: Multi-Stage Verification Effectiveness
A multi-stage verification pipeline (constraint check → I/O test → metamorphic test → contextual check) will detect 90%+ of semantic preservation failures with 40% lower computational cost than exhaustive tests.
**Source**: Synthesis across Sources 1, 9, 10, 12

### H13: Task-Specific Threshold Requirements
Different task types will require different acceptance thresholds: selection tasks (75% behavioral equivalence), extraction tasks (62%), constraint composition (90%+).
**Source**: Source 2 - arXiv:2601.07354

### H14: Formalization Benefit
Agent briefs expressed in formal specification languages will show 25-50% higher semantic preservation rates for transformation compared to informal natural language briefs.
**Source**: Synthesis across Sources 3, 7, 14

### H15: Entity Track as Preservation Indicator
Entity preservation rate (dates, numbers, named entities) serves as a lead indicator of overall semantic preservation and predicts behavioral equivalence with r>0.6 correlation.
**Source**: Source 6 - arXiv:2503.19114

---

## OPERATIONAL DEFINITIONS

### OD1: Behavioral Equivalence
Two agent briefs B₁ and B₂ are behaviorally equivalent if and only if ⟦B₁⟧(x) = ⟦B₂⟧(x) for all inputs x in the operational domain, where ⟦·⟧ denotes the semantic denotation that includes all observable actions and side effects.
**Source**: Source 3 - UC Berkeley EECS-2025-174

### OD2: Constraint Compliance
An agent brief B preserves constraint compliance if and only if all explicit constraints C = {c₁, c₂, ..., cₙ} specified in the original brief remain satisfied in all agent behaviors. This is verified through CSP satisfaction checks.
**Source**: Source 1 - arXiv:2512.17920v1, Source 13 - Semantic Scholar

### OD3: Contextual Equivalence
Two briefs B₁ and B₂ are contextually equivalent if they are interchangeable in all observable contexts C without change to system behavior: ∀C, ⟦C[B₁]⟧ = ⟦C[B₂]⟧ (Morris-style contextual equivalence).
**Source**: Source 4 - Pitts (2000), Source 11 - PLS Lab

### OD4: Semantic Preservation (Comprehensive)
Semantic preservation is a multi-dimensional property that requires: (1) behavioral equivalence (input-output consistency), (2) constraint compliance (boundary adherence), and (3) contextual equivalence (substitutability across contexts). Each dimension is verified independently.
**Source**: Synthesis across Sources 1-15

### OD5: Refinement Map
A refinement map from a lower-level specification to a higher-level one is a map from the lower-level specification's state space to the higher-level specification's state space. It maps steps of the lower-level state machine to steps of the higher-level state machine and maps behaviors allowed by the lower-level specification to behaviors allowed by the higher-level one.
**Source**: Source 5 - Abadi & Lamport (1991)

### OD6: Metamorphic Relation
A metamorphic relation is a rule that describes how a change in the input should predictably affect the output. It represents a fundamental property that must hold true for semantically equivalent implementations.
**Source**: Source 10 - Segura & Towey

### OD7: Test Oracle
A test oracle is a mechanism used in software tests to determine whether a test case has passed or failed through verification of system output correctness. It provides information that describes correct output based on the input of a test case.
**Source**: Source 9 - IEEE TSE (2014)

### OD8: Observational Equivalence
Two programs in the same language are called observationally equivalent whenever they are interchangeable in all observable contexts.
**Source**: Source 4 - Pitts (2000), Source 11 - PLS Lab

### OD9: I/O Equivalence
Test or input-output equivalence is a correctness metric where the list of possible inputs for which ⟦s⟧(x) = ⟦t⟧(x) must be maintained is limited to a finite list of inputs (either selected randomly or given by the user).
**Source**: Source 3 - UC Berkeley EECS-2025-174

### OD10: Equivalence Partition
The core principle states that if one input from a defined equivalence class behaves correctly, all other inputs within that class are expected to behave similarly. This enables efficient tests through representative samples.
**Source**: Source 12 - Katalon

---

## VERIFICATION METHODS

### VM1: I/O Equivalence Test
Execute both briefs with representative input test suites and verify identical outputs: ⟦B₀⟧(x) ?= ⟦B₁⟧(x) for test inputs x.
**Source**: Source 3 - UC Berkeley EECS-2025-174

### VM2: Metamorphic Test
Define metamorphic relations (commutativity, idempotence, monotonicity) and verify they hold across brief transformations through source and follow-up test case execution.
**Source**: Source 10 - Segura & Towey

### VM3: Constraint Satisfaction Problem Solve
Formalize constraints as CSP and verify satisfiability with automated CSP solvers.
**Source**: Source 13 - Semantic Scholar

### VM4: Mechanical Verification
Use automated tools to check constraint adherence with high inter-rater reliability (κ=0.90).
**Source**: Source 1 - arXiv:2512.17920v1

### VM5: Reconstruction Test
Verify that compressed/transformed briefs can regenerate behaviors of original briefs. Prompt the target LLM to recreate original text and compare with BERTScore and ROUGE metrics.
**Source**: Source 6 - arXiv:2503.19114

### VM6: Context Substitution Test
Test brief substitutability across diverse operational contexts to verify contextual equivalence.
**Source**: Source 4 - Pitts (2000), Source 11 - PLS Lab

### VM7: Refinement Map Verification
Establish formal maps between abstraction levels with auxiliary variables (history/prophecy) to prove that lower-level specifications correctly implement higher-level ones.
**Source**: Source 5 - Abadi & Lamport (1991)

### VM8: Entity Preservation Track
Measure percentage of named entities (persons, locations, organizations, dates, numerical values) that appear in transformed briefs compared to original.
**Source**: Source 6 - arXiv:2503.19114

### VM9: Equivalence Partition
Partition input space into equivalence classes and test representative samples from each class to achieve systematic coverage with reduced test cases.
**Source**: Source 12 - Katalon

### VM10: Model Check
Perform exhaustive search through all possible states of the model to ensure the system adheres to its specification.
**Source**: Source 7 - MDPI (2024)

### VM11: Theorem Prove
Prove mathematically that the system adheres to its specification. This often requires human intervention and insights to guide the proof, unlike automated model checks.
**Source**: Source 7 - MDPI (2024)

### VM12: Boundary Test
Explicitly test at constraint boundaries to verify that limits are respected.
**Source**: Synthesis

---

## EMPIRICAL BENCHMARKS

### EB1: Behavioral Equivalence Acceptance
≥75% output equivalence on selection tasks, ≥62% on extraction tasks per empirical benchmarks.
**Source**: Source 2 - arXiv:2601.07354

### EB2: Constraint Compliance Acceptance
100% critical constraint satisfaction, ≥95% for non-critical constraints.
**Source**: Synthesis

### EB3: Entity Preservation Target
≥50% preservation rate for dates, numbers, and named entities.
**Source**: Source 6 - arXiv:2503.19114

### EB4: Context Substitutability Target
≥90% context-substitutability rate across operational contexts.
**Source**: Synthesis

### EB5: Semantic Similarity Threshold
≥66% BERTScore for reconstruction tests indicates adequate semantic preservation.
**Source**: Source 6 - arXiv:2503.19114

---

## DESIGN PRINCIPLES

### DP1: Separation of Concerns
Explicitly separate behavioral specifications from constraint specifications in brief structure to enable independent verification.
**Source**: Source 1 - arXiv:2512.17920v1, Source 7 - MDPI (2024)

### DP2: Design for Verifiability
Structure briefs to support mechanical constraint checks and systematic behavioral tests.
**Source**: Synthesis

### DP3: Avoid Ambiguity Zones
Keep briefs either very concise (<5 words) or sufficiently detailed (>100 words); avoid medium-length (~27 words).
**Source**: Source 1 - arXiv:2512.17920v1

### DP4: Maintain Constraint Salience
Keep constraints explicit and prominent to prevent degradation when we compress.
**Source**: Source 1 - arXiv:2512.17920v1

### DP5: Formalize Critical Constraints
Express safety-critical constraints in formal languages amenable to CSP solve.
**Source**: Source 13 - Semantic Scholar

### DP6: Provide Test Oracles
Include or reference test cases and metamorphic relations for verification.
**Source**: Source 9 - IEEE TSE (2014), Source 10 - Segura & Towey

### DP7: Use Symbolic Operators Carefully
Apply symbolic representations for simple operations but avoid for constraint composition where near-zero equivalence is observed.
**Source**: Source 2 - arXiv:2601.07354

### DP8: Compositional Proof Structure
Enable modular verification when we structure briefs to support compositional proof techniques.
**Source**: Source 14 - ScienceDirect (2015)

### DP9: Account for RLHF Effects
Recognize that RLHF-induced helpfulness can cause 598% increase in constraint violations and design accordingly.
**Source**: Source 1 - arXiv:2512.17920v1

### DP10: Multi-Metric Evaluation
Task performance alone is insufficient; measure specific information preservation that includes behavioral equivalence, constraint compliance, and entity preservation.
**Source**: Source 6 - arXiv:2503.19114

---

## IMPLEMENTATION FRAMEWORK

### IF1: Four-Phase Verification Process
Phase 1: Specification and Baseline Establishment; Phase 2: Transformation and Compression; Phase 3: Multi-Dimensional Verification; Phase 4: Iterative Refinement.
**Source**: Synthesis across all sources

### IF2: Multi-Stage Test Pipeline
Stage 1: Mechanical constraint compliance check (fast, high-agreement); Stage 2: I/O equivalence test with representative test suite; Stage 3: Metamorphic relation verification; Stage 4: Contextual equivalence spot checks.
**Source**: Synthesis across Sources 1, 9, 10, 12

### IF3: Failure Analysis Categories
For verification failures, identify root causes in three categories: behavioral failures (absent information, semantic drift, surface similarity bias), constraint failures (constraint salience loss, RLHF interference, ambiguity zones), contextual failures (context-specific dependencies, emergent property changes).
**Source**: Synthesis

### IF4: Targeted Fix Strategies
Add auxiliary variables or history track if needed; enhance constraint salience; augment with symbolic elements for simple operations; rebalance RLHF if constraint compliance suffers.
**Source**: Synthesis across Sources 1, 2, 5

### IF5: Regression Verification
Re-run all verification phases after application of fixes to ensure changes don't break other dimensions.
**Source**: Standard software practice

---

## OPEN CHALLENGES

### OC1: Constraint Composition Problem
No current LLM reliably interprets compound constraints (∩ operations). Alternative representations are required.
**Source**: Source 2 - arXiv:2601.07354

### OC2: Emergent Behavior Prediction
Behaviors that emerge from agent-environment interaction may not be predictable from brief analysis alone.
**Source**: Source 15 - arXiv:2506.06366v2

### OC3: Liveness Property Verification Gap
Current methods focus on safety; liveness verification (agent eventually does X) needs more attention.
**Source**: Source 5 - Abadi & Lamport (1991)

### OC4: Scalability of Exhaustive Tests
Exhaustive contextual equivalence tests are computationally prohibitive for complex agents.
**Source**: Synthesis

### OC5: Prophecy Variable Discovery
Practical methods to discover necessary prophecy variables remain elusive.
**Source**: Source 5 - Abadi & Lamport (1991)

### OC6: Task-Specific Adaptation
Preservation rates vary dramatically by task type; task-adaptive verification strategies are needed.
**Source**: Source 2 - arXiv:2601.07354

### OC7: Entity Preservation Enhancement
Current methods preserve only 22-28% of dates and numerical values, below practical requirements.
**Source**: Source 6 - arXiv:2503.19114

### OC8: Cross-Model Consistency
Different models show vastly different semantic equivalence rates (0-100%) for identical tasks.
**Source**: Source 2 - arXiv:2601.07354

---

## TOTAL KERNEL COUNTS

- **FACTS**: 12
- **ASSUMPTIONS**: 10
- **QUESTIONS**: 12
- **HYPOTHESES**: 15
- **OPERATIONAL DEFINITIONS**: 10
- **VERIFICATION METHODS**: 12
- **EMPIRICAL BENCHMARKS**: 5
- **DESIGN PRINCIPLES**: 10
- **IMPLEMENTATION FRAMEWORK**: 5
- **OPEN CHALLENGES**: 8

**TOTAL KERNELS EXTRACTED**: 99

---

## METADATA

**Research Question**: How do we define 'semantic preservation' for agent briefs operationally?

**Key Result**: Semantic preservation is a multi-dimensional construct that requires independent verification of behavioral equivalence, constraint compliance, and contextual equivalence.

**Confidence Level**: High - results are consistently supported across 15+ authoritative sources that span formal verification, software tests, LLM research, and constraint satisfaction domains.

**Research Methodology**: Systematic web search with 12 distinct search queries, analysis of 20+ authoritative sources.

**Kernelization Date**: 2026-02-09

**Kernelized By**: Claude Sonnet 4.5
