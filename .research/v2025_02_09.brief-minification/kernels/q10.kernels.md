# Q10 Knowledge Kernels: Token-Level vs Telegraphic vs Summarization Compression

## Extracted Kernels

### [FACT] K001-token-level-20x-compression
**claim**: Token-level compression (LLMLingua) achieves up to 20x compression with minimal performance loss
**source**: "allows for up to 20x compression with little performance loss" (Jiang et al. 2023, EMNLP)
**domain**: prompt-compression, large-language-models

### [FACT] K002-extractive-outperforms-abstractive
**claim**: Extractive compression achieves +7.89 F1 points advantage over abstractive methods on multi-hop reasoning at equivalent compression ratios
**source**: "extractive reranker-based compression achieved +7.89 F1 points on 2WikiMultihopQA at 4.5x compression, while abstractive compression at similar ratios decreased performance by 4.69 F1 points"
**domain**: compression-evaluation, reasoning-tasks

### [FACT] K003-hard-prompts-preserve-token-information
**claim**: Hard-prompt approaches (token-level) produce less hallucination than soft-prompts because compressed input retains direct original text references
**source**: "LLMLingua's hard-prompt approach produces less hallucination since compressed input retains direct original text references" (Understanding Information Preservation, EMNLP 2025)
**domain**: hallucination-reduction, behavioral-equivalence

### [FACT] K004-llmlingua2-speed-advantage
**claim**: LLMLingua-2 achieves 3x-6x faster inference than prior prompt compression methods through BERT-level token classification
**source**: "3x-6x faster than prior prompt compression methods" with "1.6x-2.9x" faster latency at compression ratios of "2x-5x" (Pan et al. 2024, ACL)
**domain**: computational-efficiency, compression-methods

### [FACT] K005-exit-sentence-extraction-performance
**claim**: EXIT (extractive sentence compression) achieves 37.0 EM and 48.3 F1 on HotpotQA 70B reader, outperforming 33.7 EM baseline
**source**: "30.6 EM, 41.5 F1 (vs. 28.1 EM baseline)" on 8B and "37.0 EM, 48.3 F1 (vs. 33.7 EM baseline)" on 70B reader (Xu et al. 2024, ACL)
**domain**: multi-hop-reasoning, information-retrieval

### [FACT] K006-soft-prompt-compression-limit
**claim**: Soft-prompt methods (GIST) achieve 26x compression but with 50% exact match rate for seen tasks and 10% for out-of-domain tasks
**source**: "produced identical outputs to baselines approximately 50% of the time for seen tasks, declining to ~10% for OOD tasks" (Mu et al. 2023, NeurIPS)
**domain**: behavioral-equivalence, soft-prompt-compression

### [FACT] K007-abstractive-citation-accuracy-loss
**claim**: Abstractive compression reduces properly cited sentences by 50% and increases verification time by 3x compared to extractive methods
**source**: "As outputs become more abstractive, perceived utility improves by as much as 200%, while the proportion of properly cited sentences decreases by as much as 50% and users take up to 3 times as long to verify cited information"
**domain**: faithfulness, verifiability, abstractive-methods

### [FACT] K008-exit-inference-latency
**claim**: EXIT achieves 0.8 second latency compared to 8.5 seconds (CompAct) and 28.1 seconds (Refiner) on same document sets while maintaining superior accuracy
**source**: "EXIT achieves 0.8s latency" vs. CompAct's "8.5s" and Refiner's "28.1s" while maintaining superior accuracy (37.9 EM vs. 34.2 and 34.4) (Xu et al. 2024, ACL)
**domain**: efficiency-comparison, abstractive-compression

### [FACT] K009-llmlingua2-bidirectional-context
**claim**: LLMLingua-2 formulates compression as token classification using full bidirectional context rather than unidirectional entropy calculations
**source**: "formulate[s] prompt compression as a token classification problem to guarantee the faithfulness of the compressed prompt to the original one" using "full bidirectional context" (Pan et al. 2024, ACL)
**domain**: compression-methodology, token-classification

### [FACT] K010-longllmlingua-question-aware-improvement
**claim**: LongLLMLingua achieves 21.4% performance improvement on NaturalQuestions with 4x compression through question-aware coarse-to-fine compression
**source**: "boosts performance by up to 21.4% with around 4x fewer tokens in GPT-3.5-Turbo, leading to substantial cost savings" (Jiang et al. 2024, ACL)
**domain**: question-answering, task-aware-compression

### [FACT] K011-taco-rl-task-aware-gains
**claim**: Task-aware compression (TACO-RL) achieves 8%-189% performance improvements across text summarization, question answering, and code summarization over task-agnostic methods
**source**: "improve task performance by 8%-189% across text summarization, question answering, and code summarization scenarios over state-of-the-art compression techniques" (Wang et al. 2024, ACL)
**domain**: task-aware-compression, optimization-methods

### [FACT] K012-gist-storage-efficiency
**claim**: GIST tokens enable caching 26x more prompts than full instruction caching using identical storage allocation
**source**: "caching 26x more prompts than full instruction caching, using the same amount of storage" (Mu et al. 2023, NeurIPS)
**domain**: storage-optimization, soft-prompt-methods

### [FACT] K013-symbolic-compression-semantic-equivalence
**claim**: Symbolic metalanguage compression (MetaGlyph) achieves 75% semantic equivalence with 62-81% token reduction on selection tasks
**source**: "Gemini 2.5 Flash achieves 75% semantic equivalence on selection tasks—symbolic and prose instructions produce identical outputs three-quarters of the time" with "62–81% token reduction" (van Gassen 2025)
**domain**: symbolic-compression, behavioral-equivalence

### [FACT] K014-extractive-grounding-reliability
**claim**: Extractive compression methods demonstrate 90% constraint compliance reliability compared to lower semantic accuracy reliability
**source**: "The differential reliability of constraint compliance (κ = 0.90) versus semantic accuracy (κ = 0.25)" (Semantic Compression and Behavioral Equivalence Testing)
**domain**: behavioral-equivalence, constraint-compliance

### [FACT] K015-soft-prompt-grounding-degradation
**claim**: Soft-prompt compression induces substantial groundedness degradation with 30-point drop on HotpotQA and 50-point drops on conversational QA tasks
**source**: "Compression induces substantial groundedness degradation - a 30-point drop on HotpotQA and 50-point drops on conversational QA tasks" (Understanding Information Preservation, EMNLP 2025)
**domain**: soft-prompt-limitations, grounding-metrics

### [FACT] K016-icae-context-compression-ratio
**claim**: ICAE achieves 4x context compression with only approximately 1% additional parameters on Llama models
**source**: "effectively achieves 4× context compression based on Llama while introducing only approximately 1% additional parameters to the base model" (Ge et al. 2024, ICLR)
**domain**: soft-prompt-methods, parameter-efficiency

### [FACT] K017-tsc-removal-principles
**claim**: Telegraphic-style compression removes articles, prepositions, and auxiliary verbs while preserving nouns, verbs, numbers, and entity names
**source**: "removes what an LLM can reliably predict; grammar, filler words, and structural glue; while preserving the information it cannot reconstruct from context" (Bispo 2025, Medium)
**domain**: linguistic-compression, tsc-methodology

### [FACT] K018-exit-zero-shot-transfer
**claim**: EXIT trained solely on HotpotQA effectively addresses both single-hop (NQ, TQA) and multi-hop (2WIKI) queries under out-of-domain conditions
**source**: "EXIT trained solely on HotpotQA effectively addresses both single-hop (NQ, TQA) and multi-hop (2WIKI) queries under out-of-domain conditions" (Xu et al. 2024, ACL)
**domain**: generalization, zero-shot-transfer

### [FACT] K019-llmlingua-budget-controller
**claim**: LLMLingua employs a budget controller to maintain semantic integrity under high compression ratios while modeling token interdependencies
**source**: "a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment" (Jiang et al. 2023, EMNLP)
**domain**: compression-methodology, semantic-preservation

### [FACT] K020-information-preservation-evaluation
**claim**: Information preservation measurement reconstructs original text from compressed representations and compares using BERTScore and ROUGE metrics
**source**: "For information preservation, researchers prompt the target LLM to reconstruct original text from compressed representations, then compare using similarity metrics like BERTScore and ROUGE" (Understanding Information Preservation, EMNLP 2025)
**domain**: evaluation-methodology, fidelity-measurement

### [SUMP] S001-tsc-faithfulness-assumption
**claim**: Telegraphic-style compression preserves faithfulness through selective preservation of high-entropy facts
**source**: "removes predictable grammatical structure while preserving the high-entropy, fact-rich details that actually carry meaning" (Bispo 2025, Medium)
**domain**: tsc-methodology
**note**: This assumes LLM entropy estimation reliably identifies meaningful information, not validated in peer-review

### [SUMP] S002-hard-prompt-generalization-assumption
**claim**: Hard-prompt methods generalize better across task distributions than soft-prompt methods
**source**: "robust generalization ability across different LLMs" and "evaluated on both in-domain and out-of-domain datasets" (Pan et al. 2024, ACL)
**domain**: generalization
**note**: Implies hard-prompts preserve task-invariant information, but not directly proven

### [SUMP] S003-bidirectional-context-superiority
**claim**: Bidirectional context processing in LLMLingua-2 enables better information preservation than unidirectional approaches
**source**: "Uses full bidirectional context rather than relying solely on unidirectional entropy calculations" (Pan et al. 2024, ACL)
**domain**: compression-methodology
**note**: Assumes bidirectionality inherently improves token selection, not explicitly validated

### [KHUE] Q001-tsc-behavioral-equivalence-validation
**claim**: Unknown whether telegraphic-style compression preserves behavioral equivalence for instruction-following tasks
**source**: "TSC lacks rigorous evaluation on behavioral equivalence" and "no peer-reviewed evaluation on instruction-following tasks"
**domain**: tsc-validation, behavioral-equivalence
**gap**: Requires controlled experiments comparing TSC vs LLMLingua-2 on instruction-following benchmarks

### [KHUE] Q002-compression-ratio-behavioral-ceiling
**claim**: Unknown maximum compression ratio at which behavioral equivalence degrades below acceptable thresholds for different task types
**source**: "evaluate specific task impact - may degrade behavioral equivalence for complex instructions" at 20x compression
**domain**: compression-limits, task-dependency
**gap**: Requires systematic evaluation of behavioral equivalence degradation curves across compression ratios

### [KHUE] Q003-soft-prompt-information-encoding
**claim**: Unknown whether soft-prompt methods (GIST, ICAE) can encode constraint tokens and instruction details at high compression ratios
**source**: "Memory slots that accurately and comprehensively represent the original context" but limited to 4x compression (Ge et al. 2024, ICLR)
**domain**: soft-prompt-limitations, constraint-preservation
**gap**: Requires analysis of what information types soft-prompts can and cannot encode

### [HYPO] H001-extractive-superiority-for-constraints
**claim**: Extractive compression methods (token-level) preserve instruction constraints better than abstractive methods because constraint tokens remain explicitly accessible
**source**: "Extractive compression is recommended for 80% of use cases — safest, fastest, often accuracy-improving" and constraint compliance κ = 0.90 (Extractive vs Abstractive Trade-offs)
**domain**: behavioral-equivalence, constraint-compliance
**provenance**: Testable through constraint extraction and compliance measurement on compressed vs original prompts

### [HYPO] H002-symbolic-compression-optimal-range
**claim**: Symbolic compression (TSC, MetaGlyph) achieves maximum behavioral equivalence at 60-80% token reduction with diminishing returns beyond
**source**: "75% semantic equivalence on selection tasks" with "62–81% token reduction" suggesting optimal range (van Gassen 2025)
**domain**: symbolic-compression, optimal-ratios
**provenance**: Testable through evaluation of semantic equivalence across token reduction percentages

### [HYPO] H003-task-awareness-roi-threshold
**claim**: Task-aware compression methods provide positive ROI over task-agnostic methods only when task distribution is narrow enough to justify fine-tuning cost
**source**: "the fine-tuning process is sensitive to the choice of reward function, base model, and task-specific prompts" (Wang et al. 2024, ACL)
**domain**: task-aware-optimization, cost-benefit
**provenance**: Testable by comparing fine-tuning cost vs performance gains across task distribution widths

### [HYPO] H004-grounding-metric-causality
**claim**: Downstream task performance degradation in compressed prompts is caused by information loss, measurable through grounding metrics rather than task-only evaluation
**source**: "Downstream performance alone does not reveal a method's limitations or assess information loss" (Understanding Information Preservation, EMNLP 2025)
**domain**: evaluation-methodology, causality
**provenance**: Testable through causal analysis comparing information preservation metrics with performance degradation

### [HYPO] H005-soft-prompt-hallucination-risk
**claim**: Soft-prompt compression increases hallucination risk proportionally to information loss due to continuous encoding limitations
**source**: "LLMLingua's hard-prompt approach produces less hallucination since compressed input retains direct original text references" (Understanding Information Preservation, EMNLP 2025)
**domain**: hallucination-risk, soft-prompt-limitations
**provenance**: Testable through hallucination measurement on soft-prompt vs hard-prompt compression at equivalent compression ratios

### [HYPO] H006-grammar-removal-instruction-disruption
**claim**: Telegraphic-style compression disrupts instruction semantics through removal of grammatical structure that encodes constraint relationships
**source**: "not suitable for text where nuance, style, or tone conveys meaning" and removal targets include "prepositions ('of', 'in')" which encode relationships (Bispo 2025, Medium)
**domain**: tsc-limitations, instruction-semantics
**provenance**: Testable by comparing instruction comprehension on TSC-compressed instructions vs token-level compression

### [HYPO] H007-compression-generalization-tradeoff
**claim**: Task-agnostic compression methods sacrifice task-specific performance for cross-task generalization, with breakeven point around 4x compression
**source**: "Task-agnostic prompt compression techniques model compression as a token classification problem that fails to capture task-specific information" but "evaluate on both in-domain and out-of-domain datasets" (Wang et al. 2024 vs Pan et al. 2024, ACL)
**domain**: task-awareness, generalization
**provenance**: Testable through performance measurement on known vs unknown task types across compression methods

### [HYPO] H008-verification-time-abstractive-penalty
**claim**: Abstractive compression increases verification time due to reduced citability, creating 3x overhead for fact-checking
**source**: "users take up to 3 times as long to verify cited information" and "proportion of properly cited sentences decreases by as much as 50%" (Extractive vs Abstractive Trade-offs)
**domain**: verifiability, abstractive-limitations
**provenance**: Testable through measurement of fact-checking time and citation quality on abstractive vs extractive compression

---

## Kernel Statistics

- **FACT kernels**: 20 (empirically grounded findings from peer-reviewed research)
- **SUMP kernels**: 3 (architectural assumptions not directly validated)
- **KHUE kernels**: 3 (identified research gaps requiring exploration)
- **HYPO kernels**: 8 (testable hypotheses derived from evidence)

**Total kernels extracted**: 34

**Domain distribution**:
- compression-methodology: 6
- behavioral-equivalence: 7
- task-aware-compression: 4
- efficiency-comparison: 3
- soft-prompt-methods: 4
- evaluation-methodology: 3
- tsc-validation: 2
- other domains: 5
