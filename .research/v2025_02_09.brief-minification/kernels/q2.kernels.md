# Q2 Kernels: Function vs Content Words

## Domain: Linguistic Theory - Word Classification

### [FACT]
- Lexical categories consist of content words (nouns, verbs, adjectives, adverbs) and function words (articles, prepositions, conjunctions, auxiliary verbs), with semantic information embed in content words and syntactic information in function words. (Uno et al., 2024)
- Content words are open-class words that accept new additions, while function words are closed-class words; languages do not easily admit changes to this set. (Ashby et al., 2005)
- Function words include articles ('the', 'a'), prepositions ('of', 'in'), auxiliary verbs ('was', 'is'), and other low-information tokens. (Developer Service Blog, 2024)

## Domain: Human Language Development

### [FACT]
- Telegraphic speech emerges consistently across diverse linguistic environments in the early toddler period, typically between 18 and 30 months of age, a universal stage in child language acquisition where children produce short utterances that emphasize content words and omit function words. (Developer Service Blog, 2024)
- The syntactic-bootstrap theory proposes that verb learn succeeds because children use knowledge of syntax, as well as of the observed scene, to interpret sentences and thus to figure out the meanings of verbs. (Màrquez et al., 2008)

## Domain: Human Neuroscience - Word Process

### [FACT]
- Event-related potentials (ERPs) show that the N400 component (reflects semantic process) is considerably smaller for closed-class function words than for open-class content words of similar frequency. (Münte et al., 2001)
- Closed class words and open class words are proposed to be processed by different neural systems in the human brain. (Münte et al., 2001)
- Agrammatism is a symptom often observed in Broca's aphasia characterized by the omission of closed class words and morphological features of content words. (Münte et al., 2001)
- Patients with Broca's aphasia often show selective difficulty to produce or comprehend closed class words, which demonstrates neural separability of function word process. (Münte et al., 2001)
- Unexpected content words elicit a typically distributed N400 when displayed in the parafovea, followed by a longer-last, widely distributed positivity that starts around 300 ms once foveated. (Li et al., 2023)
- Unexpected function words elicit a left lateralized LAN-like component when presented in the parafovea, followed by a left lateralized, posteriorly distributed P600 when foveated. (Li et al., 2023)
- The N400 reflects context-sensitive lexical retrieval processes, while the P600 component indexes the integration of sense into the unfold utterance interpretation. (Li et al., 2023)
- Syntactic and semantic processes are supported by separable temporo-frontal networks strongly lateralized to the left hemisphere for syntax and less so for semantics. (Ashby et al., 2005)

### [FACT]
- Function words trigger P600/LAN components associated with syntactic integration, while content words trigger N400 associated with semantic retrieval, with temporal sequence that shows function words processed for grammatical role within 600ms of view. (Li et al., 2023)

## Domain: Human Read and Comprehension

### [FACT]
- Function words show frequency effects in first-fixation and gaze duration similar to content words, though clear differences in online process of function and content words emerge in later process measures. (Ashby et al., 2005)
- Comprehension models propose an early strategy that builds a framework of function words and morphemes before complete syntactic structure is assigned in a subsequent process. (Ashby et al., 2005)
- Language process is fast and largely incremental, with substantial syntactic, semantic, and pragmatic process of a word that occurs while the eyes are fixated on that word or while that word is be heard. (Ashby et al., 2005)

### [SUMP]
- Humans use function words as anchors for incremental parse—each function word helps constrain possible sentence structures as read progresses left-to-right. (Synthesis from Ashby et al., 2005)

## Domain: Clinical Linguistics - Agrammatism

### [FACT]
- Agrammatism is characterized by speech that contains mainly content words, with a lack of function words, where agrammatic speakers of English preserve word order but omit free functors like 'is' and inflections like '-ing' while retain a telegraphic skeleton. (Matchin et al., 2024)
- The basic signs of agrammatism are short phrase length, simplified syntax, errors and omissions of main verbs, and omission or substitution of grammatical morphemes such as plural markers or functors. (Matchin et al., 2024)
- People with agrammatism may have telegraphic speech, a unique speech pattern with simplified formation of sentences in which many or all function words are omitted. (Matchin et al., 2024)

### [HYPO]
- A rational agent would preserve the parts that carry the most information and omit the most redundant elements, which happen to be function words and inflectional morphological markers. (Matchin et al., 2024)
- Agrammatic speech represents a rational, information-theoretic strategy for communication under resource constraints (reduced work memory, process speed, or motor control). (Matchin et al., 2024)

### [SUMP]
- Agrammatic speakers produce telegraphic speech but often struggle to comprehend it from others, which suggests humans have limited "decompression" capabilities compared to their compression strategies. (Synthesis from Matchin et al., 2024)

## Domain: Information Theory - Linguistic Redundancy

### [FACT]
- Shannon's information theory provides a means to measure redundancy or efficiency of symbolic representation within a given language. Accord to Shannon, redundancy is calculated as the difference between the maximum possible entropy of a message and its actual entropy. (Wit & Gillette, 2024)
- For printed English, Shannon estimated approximately 50% redundancy for sequences up to eight letters, with higher values (~75%) when broader contexts like paragraphs are considered. (Wit & Gillette, 2024)
- A redundancy of 50 percent means that roughly half the letters in a sentence could be omitted and the message still be reconstructable. (Wit & Gillette, 2024)
- Zipf's Law states that the relative frequency of a word is inversely proportional to its rank, with the second most frequent word used only half as often as the most frequent word. (Wit & Gillette, 2024)
- The most frequently used words tend to be the shortest, and use of the shortest sequences for the most common words promotes greater communication efficiency. (Wit & Gillette, 2024)

### [FACT]
- Compression algorithms like gzip sometimes capture linguistically meaningful structures which coincide with lexical words or suffixes, but many compressed sequences are linguistically unintelligible or simply do not coincide with any linguistically meaningful structures. (Wit & Gillette, 2024)

### [SUMP]
- Function words contribute disproportionately to linguistic redundancy due to their high frequency and predictability. (Synthesis from Wit & Gillette, 2024)
- Predictable elements carry less information by definition; this makes high-frequency function words informationally sparse. (Synthesis from Shannon theory)

## Domain: LLM Architecture - Attention Mechanisms

### [FACT]
- In the fine-tune of a pre-trained BERT model for specific downstream tasks, attention scores are substantially altered based on the relationship between lexical categories and the given downstream task. (Uno et al., 2024)
- In downstream tasks that prioritize semantic information, attention scores centered on content words are enhanced, while in cases that emphasize syntactic information, attention scores centered on function words are intensified. (Uno et al., 2024)
- BERT layers consistently assign more bias to specific lexical categories, irrespective of the task; this highlights the presence of task-agnostic lexical category preferences. (Uno et al., 2024)
- BERT's attention heads exhibit patterns such as attend to delimiter tokens, specific positional offsets, or broadly attend over the whole sentence, with heads in the same layer that often exhibit similar behaviors. (Clark et al., 2019)
- Certain attention heads correspond well to linguistic notions of syntax and coreference, with heads that attend to direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. (Clark et al., 2019)
- Attention heads in BERT show strong correlation with specific syntactic roles and achieve >75% accuracy in some cases. (Clark et al., 2019)
- Substantial syntactic information is captured in BERT's attention, despite that the model was not explicitly trained on these tasks. (Clark et al., 2019)

### [SUMP]
- BERT can dynamically "deprioritize" function words in semantic tasks and "elevate" them in syntactic tasks; this shows flexibility humans lack in consistent function word process. (Synthesis from Uno et al., 2024)
- BERT has internalized where function words should appear and what roles they play through attention patterns learned in pre-train. (Synthesis from Clark et al., 2019)

## Domain: LLM Tokenization and Embed

### [FACT]
- Modern LLMs adopt subword tokenization, which strikes an optimal balance between word-level and character-level approaches, where tokens are often words and subwords depend on frequency of occurrence. (Daily Dose of DS, 2024)
- If a word is extremely frequent, the tokenizer will probably use a single token to represent it, but if a word is less frequent, the tokenizer might decompose the word into more common subwords. (Daily Dose of DS, 2024)
- Unlike single characters, subword tokens can represent meaningful chunks of words (e.g., 'cook' + 'ing' split from 'cooked', where each component carries part of the sense). (Daily Dose of DS, 2024)
- Tokens are converted into embeddings—dense numeric vectors that capture the semantic sense of the token, which includes semantic relationships, grammatical roles, and contextual nuances learned from extensive train data. (Daily Dose of DS, 2024)

### [FACT]
- Transformer language models exhibit a strong frequency bias due to their maximum likelihood train objective, which limits infrequent tokens from receive useful learn signals and thus hinders their ability to effectively encode linguistic information. (Zhang et al., 2024)
- Stop words are words that don't really have any value in a sentence because they are so common that they will not provide any differentiation in the word frequency model. (Zhang et al., 2024)

### [SUMP]
- Common function words like "the," "is," "of" almost always get single-token representations, while rare content words may be split into subwords. (Synthesis from Daily Dose of DS, 2024)
- Function words' embeddings might encode "this is a determiner" rather than rich semantic content; this explains why they're more droppable. (Synthesis from Daily Dose of DS, 2024)
- The high frequency of function words in train creates abundant learn signals and allows their embeddings to capture nuanced grammatical patterns, but their commonality makes them provide little differentiation in semantic tasks. (Synthesis from Zhang et al., 2024)

## Domain: Prompt Compression for LLMs

### [FACT]
- Telegraphic Semantic Compression (TSC) transforms verbose natural language into dense semantic packets; it strips away grammatical scaffold while preserves the core informational payload. (Developer Service Blog, 2024)
- Content words contain the unpredictable, fact-rich information an LLM cannot guess, while function words are predictable, low-information tokens that LLMs can infer from context. (Developer Service Blog, 2024)
- Use of its language understand, the LLM reconstructs fluent, human-readable text from compressed facts and reintroduces grammar and context as needed. (Developer Service Blog, 2024)
- Prompt compression reduces the size of the prompt by removal of less informative content. (Ahmed, 2024)
- Hard-prompt compression operates on the surface form and involves removal of unnecessary or low-information content by prune of parts of the original text or summarization of it. (Ahmed, 2024)
- Soft-prompt compression compresses the context into dense memory slots by learn of continuous representations of the information in the latent space; this allows for significantly higher compression rates as they operate in the embed space. (Ahmed, 2024)
- Current token-level compression solutions often rely on removal of intermediate tokens from a sentence, which may result in a non-coherent and grammatically incorrect sentence; this often hampers the semantics of the input prompt. (Ahmed, 2024)
- LLMLingua achieved up to 20x compression while preserved the original prompt's capabilities, particularly in in-context learn and reason. (Ahmed, 2024; Microsoft Research, 2023)

### [FACT]
- LLMLingua uses a compact language model to identify and remove non-essential tokens from prompts; it employs a coarse-to-fine compression strategy. (Microsoft Research, 2023)
- The method maintains semantic integrity even at high compression ratios through budget controllers that prevent over-compression. (Microsoft Research, 2023)
- By removal of non-essential tokens, LLMLingua reduces context window costs while maintains or even improves LLM performance on downstream tasks. (Microsoft Research, 2023)

### [HYPO]
- LLMs possess a unique capability compared to humans: they can reconstruct full grammatical sentences from telegraphic input. (Synthesis from Developer Service Blog, 2024)
- Function words may sometimes introduce noise or dilute attention away from semantic content; this explains why compression can maintain or improve performance. (Synthesis from Microsoft Research, 2023)

### [SUMP]
- While human children naturally produce telegraphic speech in development, adults typically require function words for comprehension. (Synthesis from Developer Service Blog, 2024)
- A smaller LM can identify which tokens are "non-essential" for a larger LM's comprehension—and these largely correspond to function words and grammatical markers. (Synthesis from Microsoft Research, 2023)

## Domain: LLM Robustness and Adversarial Examples

### [FACT]
- Function words are the only words that reduce adversarial success rates without cause a significant performance drop; this confirms that proper removal of function words could potentially defend models against attacks. (Wang et al., 2024)
- LLMs remain vulnerable to word-level perturbations that include misspell (level 1), swap of words (level 2), and replacement of words with synonyms (level 3). (Wang et al., 2024)
- Adversarial attacks like TextFooler and BERT-Attack identify the most important and vulnerable words that alter model prediction the most and then replace those words. (Wang et al., 2024)

### [HYPO]
- Removal of function words actually improves robustness to adversarial attacks; this suggests function words may be noise vectors for LLM decision-make in some contexts. (Wang et al., 2024)
- Function words may introduce ambiguity or distraction in LLM attention patterns—their high frequency might cause over-attention relative to their information content. (Synthesis from Wang et al., 2024)

### [SUMP]
- Function words don't fall into the category of "important and vulnerable words" targeted by adversarial attacks; this confirms their peripheral role in LLM comprehension. (Synthesis from Wang et al., 2024)

## Domain: Semantic Role Label

### [FACT]
- Semantic role label (also called shallow semantic parse or slot-fill) is the process that assigns labels to words or phrases in a sentence that indicates their semantic role in the sentence, such as that of an agent, goal, or result. (Màrquez et al., 2008)
- The task of SRL is to label shallow semantic relations in a sentence as predicate argument structures (PAS), where a predicate usually refers to a word that indicates an event or a relation, and arguments (ARGs) refer to syntactic constituents that represent different semantic roles. (Màrquez et al., 2008)
- Since semantic roles are formed by syntactic constituents in the sentence, an effective parser, as well as an effective syntactic feature set are essential to build a practical SRL system. (Màrquez et al., 2008)
- The syntactic structure of a sentence provides information about the number and type of syntactic arguments that surround the verb; a semantic analysis of the syntactic structure yields a semantic event structure. (Màrquez et al., 2008)

### [SUMP]
- Semantic roles (the "who did what to whom" that constitutes core sense) are primarily encoded in content words (predicates and arguments), with function words that serve to mark grammatical relationships. (Synthesis from Màrquez et al., 2008)
- Modern neural SRL systems can achieve high accuracy even with imperfect syntactic parse; this suggests that semantic roles can be inferred largely from content words when sufficient context is available. (Synthesis from Màrquez et al., 2008)

## Domain: Cross-Linguistic Communication

### [FACT]
- Minimal English is a highly reduced version of English designed to be as simple and cross-translatable as possible; it is intended for use by non-specialists and for a wide and open-ended range of functions. (Goddard & Wierzbicka, 2024)
- Minimal English lexicon consists of a core of about 250 words, with the core vocabulary that comprises semantic primes and some associated grammatical words. (Goddard & Wierzbicka, 2024)
- The start point is semantic primes and universal semantic molecules, because these senses are, as far as we know, cross-translatable into all or most languages of the world. (Goddard & Wierzbicka, 2024)
- The approach uses English 'function words' like about, to, for, and with, in grammatical ways, but with restrictions to ensure cross-translatability. (Goddard & Wierzbicka, 2024)
- When use of Minimal English occurs, grammar must be kept as simple as possible, especially to avoid use of English grammar structures known to be non-translatable into many languages. (Goddard & Wierzbicka, 2024)

### [SUMP]
- Many function words and grammatical constructions are language-specific conveniences rather than semantic necessities. (Synthesis from Goddard & Wierzbicka, 2024)
- If sense can be conveyed with ~250 words to humans across languages, LLMs with statistical knowledge of billions of word combinations can certainly infer absent function words. (Synthesis from Minimal English research)
- Certain function word constructions aren't cross-translatable; this reveals their arbitrary nature as conventions of English rather than universal semantic requirements. (Synthesis from Goddard & Wierzbicka, 2024)

## Domain: Comparative Analysis - LLMs vs Humans

### [FACT]
- LLMs process entire sequences in parallel through self-attention, while humans process sequentially in real-time read. (Synthesis from multiple sources)
- Humans use function words for real-time sentence structure build within temporal constraints (600ms process windows), while LLMs can build sentence representations that consider all words simultaneously. (Synthesis from Li et al., 2023)

### [HYPO]
- LLMs have fundamentally different information requirements than humans: they've learned statistical patterns so thoroughly in pre-train that grammatical markers become partially redundant. (Synthesis from research)
- The "syntactic bootstrap" concept in child language acquisition—use of syntax to learn verb senses—is reversed in LLMs: they use semantic knowledge (from massive pre-train) to infer syntax. (Synthesis from multiple sources)
- LLMs have superior "decompression" capabilities—they can reconstruct absent grammatical structure more reliably than humans can. (Synthesis from multiple sources)

### [SUMP]
- While humans process function words for syntactic scaffold consistently across contexts, BERT can dynamically adjust attention to word classes based on task requirements. (Synthesis from Uno et al., 2024)
- Humans lack the cross-linguistic statistical knowledge that multilingual LLMs possess; this makes humans more dependent on explicit function words for parse. (Synthesis from multiple sources)
- LLMs process semantics through distributed attention over content words, while humans use function words as structural anchors. (Synthesis from multiple sources)
- Humans need syntax to understand semantics (bottom-up process), while LLMs can infer syntax from semantics (top-down process). (Synthesis from SRL and attention research)

## Domain: Practical Applications - Compression Guidelines

### [HYPO]
- Function words are highly droppable for LLM comprehension: articles (the, a, an), copular verbs (is, am, are, was, were), auxiliary verbs (have, has, had, do, does), most prepositions in predictable contexts, and conjunctions in simple coordination carry minimal information loss. (Synthesis from research)
- Function words are moderately droppable (context-dependent): prepositions that indicate non-standard relationships, subordinate conjunctions (that, which, who), and modal auxiliaries that change sense (can, must, might). (Synthesis from research)
- Function words are less droppable (higher information content): negations (not, never) are semantically critical, quantifiers (some, many, all) specify scope, pronouns serve as coreference markers, and prepositions that indicate temporal or causal relationships. (Synthesis from research)

### [FACT]
- Content words (nouns, main verbs, adjectives, adverbs), named entities, numbers and measurements, and domain-specific terminology should never be dropped. (Synthesis from compression research)

## Domain: Empirical Performance Metrics

### [FACT]
- 20x compression ratios are achievable while maintain semantic content in LLM applications. (Microsoft Research, 2023; Ahmed, 2024)
- BERT attention heads achieve >75% accuracy in identification of syntactic relationships without explicit function words. (Clark et al., 2019)
- Shannon estimated 50-75% redundancy in English text depend on context window size. (Wit & Gillette, 2024)

## Domain: Open Questions

### [KHUE]
- What is the optimal compression ratio for different types of LLM tasks (semantic comprehension vs syntactic parse vs generation)?
- How does function word removal affect different LLM architectures (BERT vs GPT vs other transformer variants)?
- Can explicit train on telegraphic input further improve LLM robustness and efficiency?
- What are the limits of semantic preservation at extreme compression ratios beyond 20x?
- How do multimodal LLMs (vision-language models) handle function word absence compared to text-only models?
- What is the relationship between compression ratio tolerance and model size/capability?
- Can human comprehension of telegraphic text be improved through train and narrow the gap with LLMs?

### [KHUE]
- Do different human languages show differential tolerance for function word omission based on their typological features (e.g., pro-drop languages vs non-pro-drop)?
- How do LLMs trained on multiple languages handle function word absence differently than monolingual models?
- What neural mechanisms enable LLMs to reconstruct grammatical structure that humans cannot reliably infer?

## Domain: Theoretical Implications

### [HYPO]
- The asymmetry in function word necessity between LLMs and humans reveals fundamental differences in linguistic process architectures: parallel vs sequential, statistical inference vs rule-based parse, top-down semantic-to-syntactic vs bottom-up syntactic-to-semantic.
- Optimization of input for LLMs means strip away the grammatical scaffold humans require for parse and leave dense semantic content that LLMs process more efficiently than verbose grammatical sentences.
- LLMs' massive pre-train (orders of magnitude beyond human language exposure) enables statistical inference of absent function words that humans cannot replicate.

### [SUMP]
- For semantic comprehension tasks that dominate LLM applications (question answer, summarization, generation), function words are less critical than for explicit syntactic tasks (parse, grammatical judgment).
- The scale of LLM train creates capabilities for reconstruction of grammatical structure from content words alone, a capability humans lack at comparable reliability.

## Domain: Synthesis - Core Insights

### [FACT]
- Function words are significantly more droppable for LLMs than for humans, but this asymmetry arises from fundamental differences in architecture, train, and process strategies. (Synthesis from all sources)

### [HYPO]
- Function words are mathematically redundant for semantic content transmission, and LLMs optimized for information process can largely ignore them for comprehension tasks. (Synthesis from information theory and compression research)
- LLMs' parallel process architecture makes them less dependent on explicit syntactic markers (function words) that humans need for sequential parse. (Synthesis from attention mechanisms and human read research)

### [SUMP]
- The fundamental insight is that LLMs and humans have different linguistic needs—what optimizes communication for humans (grammatical completeness) differs from what optimizes process for LLMs (semantic density). (Synthesis from all research domains)
