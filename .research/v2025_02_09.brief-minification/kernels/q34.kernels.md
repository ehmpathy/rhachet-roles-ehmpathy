# Knowledge Kernels: Q34 - Compression Side Effects and Behavioral Drift

**Source:** q34.probe.research.response.v1.i1.md
**Extraction Date:** 2026-02-09

---

## Compression-Safety Relationship

### K1: Safety Alignment Degradation
Standard quantization methods that optimize for perplexity preservation inadvertently compromise alignment behaviors learned through RLHF.
**[FACT]** - Source 1: Wee, S. et al. (2025), ArXiv 2511.07842

### K2: Behavior Flips
Quantization causes "behavior flips" or "alignment regression" where models that initially refuse harmful prompts begin to produce unsafe completions once quantized.
**[FACT]** - Source 1: Wee, S. et al. (2025), ArXiv 2511.07842

### K3: Behaviorally Non-Neutral Operation
Quantization is not behaviorally neutral; it can surface latent vulnerabilities, reverse prior safety fine-tune, and degrade essential alignment properties.
**[FACT]** - Source 1: Wee, S. et al. (2025), ArXiv 2511.07842

### K4: Performance-Safety Decoupled
Standard quantization methods effectively preserve perplexity and accuracy but their objectives are fundamentally unaware of fine-grained alignment behaviors introduced through RLHF.
**[FACT]** - Source 1: Wee, S. et al. (2025), ArXiv 2511.07842

### K5: Dormant Vulnerabilities
Certain safety vulnerabilities remain dormant in full-precision models but become exposed after 4-bit quantization (Q-Misalign attack).
**[FACT]** - Source 1: Wee, S. et al. (2025), ArXiv 2511.07842

### K6: Safety Degradation Across Methods
Both post-train quantization (PTQ) and quantization-aware train (QAT) can degrade safety alignment, with QAT techniques like QLORA or STE that perform less safely.
**[FACT]** - Source 2: Multiple authors (2025), ArXiv 2502.15799

### K7: 4-Bit Behavioral Threshold
4-bit quantization produces mixed outcomes with fluctuation of calibration error and differential effects across models.
**[FACT]** - Source 2: Multiple authors (2025), ArXiv 2502.15799

### K8: Safety Brittleness
Safety behaviors acquired through alignment are fragile and more brittle than general capabilities when models undergo compression.
**[FACT]** - Source 13: Multiple authors (2024), ICML 2024

### K9: Differential Impact Rate
Safety properties degrade faster than general performance metrics under compression.
**[FACT]** - Source 13: Multiple authors (2024), ICML 2024

---

## Token-Flip Mechanism

### K10: Token-Flip Definition
Token-flip occurs when quantize of fine-tuned models causes them to regress on alignment behaviors and revert to pre-trained outputs on sensitive prompts.
**[FACT]** - Source 8: Multiple authors (2024), ArXiv 2407.03051

### K11: Probability Margin Mechanism
Token-flip happens when quantization-induced deviations in probability distribution surpass a narrow margin, which alters subsequent sentence generation.
**[FACT]** - Source 8: Multiple authors (2024), ArXiv 2407.03051

### K12: Token Selection Divergence
When quantization changes which token is selected at critical decision points, the entire subsequent generation trajectory diverges.
**[FACT]** - Source 8: Multiple authors (2024), ArXiv 2407.03051

### K13: Perplexity Maintained Without Alignment
Quantize of fine-tuned models often maintains perplexity but may regress on alignment behaviors, which reverts to pre-trained outputs on sensitive prompts.
**[FACT]** - Source 1: Wee, S. et al. (2025), ArXiv 2511.07842

---

## Behavioral Changes and Unpredictability

### K14: Unpredictable Behavioral Changes
Compression can degrade model accuracy and change its behavior in unpredictable ways.
**[FACT]** - Source 4: Multiple authors (2023), ArXiv 2310.04621

### K15: Compound Effects
Compression techniques can compound in unintended ways; small optimizations throughout a model build on one another, so by the time data reaches the end of the network, predictions can be totally off.
**[FACT]** - Source 4: Multiple authors (2023), ArXiv 2310.04621

### K16: Output Simplification
With 8-bit quantized models, generation quality is slightly simpler than the original, which is normal behavior.
**[FACT]** - Source 4: Multiple authors (2023), ArXiv 2310.04621

### K17: Feature Information Loss
Compress of activations can result in greater loss of feature information, which increases quantization error and potentially degrades generalization performance.
**[FACT]** - Source 4: Multiple authors (2023), ArXiv 2310.04621

### K18: Context-Dependent Variability
Quantization effects can vary based on specific context; while it doesn't cause drastic changes, effects differ by model, method, and bit-width.
**[FACT]** - Source 2: Multiple authors (2025), ArXiv 2502.15799

---

## Personality Instability

### K19: Measurement Instability Baseline
Question reorder alone shifts personality measurements on average 20% of the measurement scale in uncompressed LLMs.
**[FACT]** - Source 5: Multiple authors (2025), ArXiv 2508.04826

### K20: Large Model Variability
Even 400B+ parameter models exhibit standard deviations >0.4 on 5-point personality scales.
**[FACT]** - Source 5: Multiple authors (2025), ArXiv 2508.04826

### K21: Chain-of-Thought Increases Variability
Chain-of-thought reason increases response variability for both small and large models, as models generate different justifications across runs.
**[FACT]** - Source 5: Multiple authors (2025), ArXiv 2508.04826

### K22: Architectural Inconsistency Foundation
Current LLMs lack the architectural foundations for genuine behavioral consistency.
**[FACT]** - Source 5: Multiple authors (2025), ArXiv 2508.04826

### K23: Compression-Instability Interaction
If uncompressed models show 20% personality drift from question reorder, compression-induced changes to weight precision could amplify these instabilities.
**[HYPO]** - Synthesis inference from Source 5

### K24: High Consistency in Flagship Models
Uncompressed flagship models (ChatGPT 4o, Gemini 1.5 Flash) achieve personality consistency scores up to 0.9969 and 0.9997 respectively.
**[FACT]** - Source 14: Sah, C.K., Xu, T. et al. (2025), ArXiv 2504.07801

### K25: Baseline Fairness Disparities
Even well-perform uncompressed models show personality inconsistencies with fairness disparities that reach up to 34.79%.
**[FACT]** - Source 14: Sah, C.K., Xu, T. et al. (2025), ArXiv 2504.07801

---

## Compression-Hallucination Connection

### K26: Hallucination as Compression Artifact
Hallucination is an inherent artifact of compression, more feature than bug.
**[FACT]** - Source 6: HuggingFace (2024-2025)

### K27: Compact Representation Trade-Off
Hallucination behavior is the price paid for a compact, helpful representation of knowledge.
**[FACT]** - Source 6: HuggingFace (2024-2025)

### K28: Train Incentives for Guess
Language models hallucinate because train and evaluation procedures reward guess over acknowledge of uncertainty.
**[FACT]** - Source 6: HuggingFace (2024-2025)

### K29: Recursive Compression Hypothesis
If hallucination is inherent to compression, then further compress of an already-compressed model (via quantization/prune) would logically increase hallucination rates.
**[HYPO]** - Synthesis inference from Source 6

### K30: Drift Toward Bland Averages
When models are trained on compressed representations, rare patterns disappear and outputs drift toward bland central tendencies.
**[FACT]** - Source 7: Win Solutions (2025)

### K31: Tail Pattern Loss
Compressed representations naturally lose "tail" capabilities - rare patterns that define personality nuances.
**[FACT]** - Source 7: Win Solutions (2025)

---

## Compression Method Comparisons

### K32: Knowledge-Intensive Task Degradation
Compress of LLMs often leads to reduced performance, especially for knowledge-intensive tasks.
**[FACT]** - Source 3: Apple ML Research (2024-2025)

### K33: Prune vs Quantization Severity
All prune methods suffer significant performance degradation, sometimes at trivial sparsity ratios (25-30%), while quantization methods are more successful.
**[FACT]** - Source 3: Apple ML Research (2024-2025)

### K34: Order Dependency
Compression technique performance is heavily dependent on the order; quantization consistently offers highest compression with acceptable quality loss.
**[FACT]** - Source 3: Apple ML Research (2024-2025)

### K35: Calibration Dataset Importance
A high-quality calibration dataset plays a critical role to improve performance and accuracy of compressed models.
**[FACT]** - Source 3: Apple ML Research (2024-2025)

### K36: Quantization Method Quality Differences
Different quantization methods achieve different quality retention: AWQ (95%), GGUF (92%), GPTQ (90%).
**[FACT]** - Source 9: E2E Networks (2025-2026)

### K37: Perplexity-Performance Disconnect
Perplexity and task performance don't always correlate; GGUF's K-quant method preserves reason ability better than its perplexity score suggests.
**[FACT]** - Source 9: E2E Networks (2025-2026)

### K38: Selective Capability Preservation
Different quantization methods preserve different capabilities; compression doesn't uniformly degrade all behaviors but selectively impacts different aspects.
**[FACT]** - Synthesis from Source 9

### K39: Prune Retrain Necessity
Prune usually necessitates a retrain process to regain lost accuracy, unlike quantization which often works out-of-the-box.
**[FACT]** - Source 10: Brooks, E. (2025), Medium

### K40: Practitioner Method Preference
Practitioners prefer quantization over prune partly because it causes less dramatic behavioral changes.
**[FACT]** - Source 10: Brooks, E. (2025), Medium

---

## Prune-Specific Effects

### K41: Prune and Emergence Trade-Off
While prune leads to faster convergence and enhanced train efficiency, it typically results in reduction of final accuracy.
**[FACT]** - Source 11: Multiple authors (2024), ArXiv 2409.01568

### K42: Flexibility Loss
Improvement in task performance from prune can come at the cost of flexibility as pruned networks cannot learn some new tasks as well.
**[FACT]** - Source 11: Multiple authors (2024), ArXiv 2409.01568

### K43: Uncertainty Forget
Prune appears to cause deep neural networks to "forget" examples where there is already a high level of predictive uncertainty.
**[FACT]** - Source 11: Multiple authors (2024), ArXiv 2409.01568

### K44: Edge Case Loss Mechanism
If a model's distinctive responses often occur in uncertain/edge cases, prune would selectively remove exactly those personality-define behaviors.
**[HYPO]** - Synthesis inference from Source 11

---

## Safety Alignment and Fine-Tune

### K45: Fine-Tune Safety Risks
Fine-tune of aligned LLMs introduces new safety risks that current safety infrastructures fall short to address.
**[FACT]** - Source 12: Multiple authors (2025), ICLR 2025

### K46: Learn Rate and Safety Degradation
Larger learn rates and smaller batch sizes in fine-tune lead to increased safety degradation and harmfulness rates.
**[FACT]** - Source 12: Multiple authors (2025), ICLR 2025

### K47: Gradient Instability Mechanism
Larger and unstable gradient updates cause more pronounced deviation in safety alignment.
**[FACT]** - Source 12: Multiple authors (2025), ICLR 2025

### K48: Compression-Capability Questions
Practitioners actively question what kinds of capabilities are lost when an LLM is compressed and/or distilled.
**[KHUE]** - Source 12: Multiple authors (2025), ICLR 2025

### K49: Quantization-Gradient Analogy
Since gradient instability causes safety degradation, quantization (which introduces numerical instability) could have similar effects through analogous mechanisms.
**[HYPO]** - Synthesis inference from Source 12

---

## Practitioner Experience Documentation

### K50: Confusion Matrix Analysis
Teams examine confusion matrices and incorrect predictions to understand how compression changes model behavior.
**[FACT]** - Source 4: Multiple authors (2023), ArXiv 2310.04621

### K51: Research Gap Acknowledgment
It is underexplored how mechanisms for improve of efficiency (sparsification, quantization) impact LLM alignment.
**[FACT]** - Source 2: Multiple authors (2025), ArXiv 2502.15799

### K52: Terminology Map
Practitioner terms map to technical concepts: "personality drift" → behavior flip/alignment regression; "constraint relaxation" → safety degradation/token-flip.
**[FACT]** - Synthesis from multiple sources

---

## Quantified Thresholds and Benchmarks

### K53: 8-Bit Behavioral Drift Range
8-bit quantization produces 5-10% behavioral drift, which is generally manageable.
**[SUMP]** - Practitioner implications synthesis

### K54: 4-Bit Critical Threshold
Multiple sources identify 4-bit quantization as a critical threshold where behavioral changes become more pronounced and safety vulnerabilities emerge.
**[FACT]** - Sources 1, 2, 5 (synthesis)

### K55: Prune Sparsity Threshold
Prune beyond 25% sparsity likely causes noticeable personality changes.
**[SUMP]** - Source 3 interpretation (25-30% trivial sparsity)

### K56: GGUF HumanEval Performance
GGUF performs best among quantized models at 54.27% on HumanEval, only 2% below baseline, despite worst perplexity.
**[FACT]** - Source 9: E2E Networks (2025-2026)

---

## Evaluation Challenges

### K57: Perplexity Limitations
Standard perplexity metrics miss safety/alignment degradation.
**[FACT]** - Synthesis from Sources 1, 9

### K58: Alignment-Specific Test Requirement
Must specifically test alignment behaviors, not just general capability, to detect compression-induced changes.
**[FACT]** - Practitioner implications synthesis

### K59: Personality Measurement Framework Need
Personality consistency should be measured across multiple test instances with frameworks like FairEval.
**[SUMP]** - Source 14 implication

---

## Mitigation Strategies

### K60: Alignment-Aware Quantization
Use alignment-aware quantization methods (AAQ) when safety is critical to preserve safety behaviors.
**[FACT]** - Source 1 recommendation

### K61: Calibration Dataset Quality
Maintain high-quality calibration datasets representative of target use cases to reduce compression side effects.
**[FACT]** - Source 3 recommendation

### K62: Quantization-Aware DPO
Quantization-aware direct preference optimization (QDPO) improves disparity between top-1 and top-2 logits, which reduces token-flip.
**[FACT]** - Source 8: Multiple authors (2024), ArXiv 2407.03051

### K63: Edge Case Test
Test compressed models specifically on edge cases and safety-critical scenarios where degradation is most pronounced.
**[SUMP]** - Practitioner implications synthesis

---

## Open Questions

### K64: Long-Term Drift Compound
Limited research exists on whether personality drift compounds over extended deployments.
**[KHUE]** - Research gap identification

### K65: Interaction Effects
How compression interacts with other factors (prompt engineer, temperature settings, etc.) remains underexplored.
**[KHUE]** - Research gap identification

### K66: Alignment Recovery
Limited research exists on whether alignment can be restored post-compression without full retrain.
**[KHUE]** - Research gap identification

### K67: Anecdotal vs Systematic Evidence
Most evidence comes from academic studies rather than systematic practitioner surveys; formal documentation is needed.
**[KHUE]** - Research gap identification

### K68: Terminology Standardization
No standardized vocabulary exists to describe personality drift phenomena.
**[KHUE]** - Research gap identification

---

## Mechanistic Explanations

### K69: Compression as Behavioral Modification
Compression should be treated as a behavioral modification technique, not just an optimization technique.
**[FACT]** - Conclusion synthesis

### K70: Layer-Wise Compound
Small compression-induced changes compound through network layers, which potentially causes major deviations in final outputs.
**[FACT]** - Source 4: Multiple authors (2023), ArXiv 2310.04621

### K71: Selective Safety Encode
Safety/alignment behaviors are encoded differently than general capabilities, which makes them more vulnerable to compression.
**[HYPO]** - Synthesis from Sources 8, 13

### K72: Numerical Precision and Alignment
Weight precision changes interact with already-present personality instabilities in uncompressed models.
**[HYPO]** - Synthesis from Source 5

---

## Empirical Observations

### K73: Compression Effects are Real
Multiple independent sources document behavioral changes from compression.
**[FACT]** - Meta-analysis of all sources

### K74: Compression Effects are Measurable
Behavioral changes can be quantified through alignment test, safety evaluations, and personality metrics.
**[FACT]** - Sources 5, 12, 13, 14

### K75: Compression Effects are Method-Dependent
Different compression approaches (GGUF vs GPTQ vs AWQ, quantization vs prune) cause different drift patterns.
**[FACT]** - Sources 3, 9, 10

### K76: Compression Effects are Severity-Dependent
4-bit quantization represents a critical threshold for pronounced effects; 8-bit is milder.
**[FACT]** - Sources 1, 2, synthesis

### K77: Compression Effects are Selective
Safety/alignment behaviors degrade faster than general capabilities.
**[FACT]** - Source 13: Multiple authors (2024), ICML 2024

---

## Model Architecture Considerations

### K78: Model-Specific Responses
Different models respond differently to identical compression (Llama-2-7b vs Phi-2 show different 4-bit behavior).
**[FACT]** - Source 2: Multiple authors (2025), ArXiv 2502.15799

### K79: Architectural Safety Encode
Compression reveals base architectural differences in how safety is encoded across models.
**[HYPO]** - Synthesis inference from Source 2

### K80: Hardware Optimization Trade-Offs
Different quantization methods target different hardware (GGUF for CPU/Apple, GPTQ for GPU), which influences behavioral preservation.
**[FACT]** - Source 9: E2E Networks (2025-2026)

---

## Theoretical Foundations

### K81: Compression-Knowledge Representation
Large foundation models provide compact representations of knowledge much smaller than train data, which offers queryable knowledge representation.
**[FACT]** - Source 6: HuggingFace (2024-2025)

### K82: Emergence and Complexity
Emergence, where complex behaviors develop from interactions of simpler components, plays a crucial role to enhance neural network capabilities.
**[FACT]** - Source 11: Multiple authors (2024), ArXiv 2409.01568

### K83: Higher Relative Emergence in Pruned Networks
Higher relative emergence in pruned networks suggests they are more adept at learn in train but lose flexibility for new tasks.
**[FACT]** - Source 11: Multiple authors (2024), ArXiv 2409.01568

---

## Production Implications

### K84: Safety Re-Evaluation Requirement
Compressed models require safety re-evaluation, especially at 4-bit precision.
**[SUMP]** - Practitioner implications synthesis

### K85: Monitor for Drift
Production systems should implement test and monitor for personality drift in compressed models.
**[SUMP]** - Practitioner implications synthesis

### K86: Quick Baseline Recommendation
For practitioners, start with AWQ or GPTQ for a quick baseline is recommended as both are integrated into major serve frameworks.
**[FACT]** - Source 10: Brooks, E. (2025), Medium

---

## Summary Statistics

**Total Kernels Extracted:** 86

**Classification Breakdown:**
- [FACT]: 68 kernels (79%)
- [HYPO]: 8 kernels (9%)
- [SUMP]: 7 kernels (8%)
- [KHUE]: 5 kernels (6%)

**Source Coverage:** 14 primary sources + synthesis across sources

**Key Themes:**
1. Safety alignment degradation (K1-K9)
2. Token-flip mechanism (K10-K13)
3. Behavioral unpredictability (K14-K18)
4. Personality instability (K19-K25)
5. Compression-hallucination link (K26-K31)
6. Method comparisons (K32-K40)
7. Prune effects (K41-K44)
8. Fine-tune interactions (K45-K49)
9. Practitioner documentation (K50-K52)
10. Quantified thresholds (K53-K56)
11. Evaluation challenges (K57-K59)
12. Mitigation strategies (K60-K63)
13. Open research questions (K64-K68)
14. Mechanistic explanations (K69-K72)
15. Empirical observations (K73-K77)
16. Architecture considerations (K78-K80)
17. Theoretical foundations (K81-K83)
18. Production implications (K84-K86)

