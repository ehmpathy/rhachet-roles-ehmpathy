# Research Question: Compression Training Data Bias and Concept Preservation

## Question Statement

**INVERSION: Does compression training data bias affect which concepts are preserved? If compressor was trained on code, will it preserve code concepts better?**

This research question investigates whether the training data distribution of a compression model influences which semantic concepts are preserved during compression. Specifically, it asks whether domain-specific training (e.g., on code) biases the compression process to preferentially preserve concepts within that domain.

---

## Research Findings Summary

### Key Insights

Recent research on model compression and training data bias reveals several critical findings:

1. **Compression Amplifies Existing Bias**: Compression techniques (pruning, quantization) amplify existing algorithmic biases present in the training data. Features that are underrepresented in training data experience disproportionate performance degradation during compression.

2. **Training Distribution Shapes Model Behavior**: Pretraining data distribution significantly affects which capabilities are learned and preserved. Models trained with different data distributions (books vs. web data vs. code) learn different balances of knowledge and generalization.

3. **Domain-Specific Performance Variation**: Large language models exhibit varying performance across different domains. Models trained on code struggle with underrepresented programming languages and domain-specific code generation, suggesting that training domain influences what concepts are captured and preserved.

4. **Selective Concept Preservation**: During compression, certain features are preserved better than others—particularly those well-represented in the original training data. Underrepresented concepts suffer disproportionate loss.

5. **Quantization vs. Pruning Trade-offs**: Different compression techniques have different effects on bias. Quantization tends to be more faithful to the original model's bias profile, while pruning can amplify biases, suggesting that compression method interacts with training distribution effects.

6. **Phase Transition Phenomenon**: Language models exhibit "phase transitions" in compression where capabilities abruptly collapse beyond certain compression thresholds, and this threshold appears to be task/domain-specific.

7. **Domain Adaptation Challenges**: When adapting models to specialized domains through compression or fine-tuning, models face challenges preserving concepts from both the original domain and the new specialized domain simultaneously—indicating that domain-specific training data is critical for preservation.

### Implications for Code-Specific Compression

If a compressor is trained on code:
- It would likely preserve code-related concepts better than natural language concepts
- However, this could come at the cost of degrading natural language understanding
- The type of code in the training data matters: models trained on Python preserve Python concepts better than C++ concepts
- Multi-lingual and multi-domain compression presents a challenging trade-off: you cannot equally preserve concepts from all domains

---

## Sources

1. [Characterising Bias in Compressed Models](https://arxiv.org/abs/2010.03058)
   - Establishes that compression amplifies existing algorithmic bias and disproportionately impacts underrepresented features in training data.

2. [Understanding the Effect of Model Compression on Social Bias in Large Language Models](https://aclanthology.org/2023.emnlp-main.161/)
   - Analyzes how compression techniques affect social bias in LLMs, showing varying effects between quantization and pruning approaches.

3. [Bias and Fairness in Large Language Models: A Survey](https://direct.mit.edu/coli/article/50/3/1097/121961/Bias-and-Fairness-in-Large-Language-Models-A)
   - Comprehensive survey covering how training data distribution creates and perpetuates bias in language models.

4. [A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models](https://aclanthology.org/2023.acl-long.878/)
   - Compares pruning, distillation, and quantization impacts on fairness, showing that compression methods interact differently with training data biases.

5. [Phase transitions in large language model compression](https://www.nature.com/articles/s44387-026-00072-8)
   - Describes phenomenon where compressed LLMs experience abrupt capability loss at certain thresholds, with domain-specific implications.

6. [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/html/2505.07289v1)
   - Examines the trade-off between compression and semantic concept preservation, introducing the Semantic Retention Compression Rate metric.

7. [Exploring Multi-Lingual Bias of Large Code Models in Code Generation](https://arxiv.org/html/2404.19368)
   - Shows that code models exhibit language-specific biases, preserving concepts for languages well-represented in training data better than others.

8. [Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data](https://arxiv.org/html/2407.14985v4)
   - Traces model capabilities directly back to pretraining data distribution, establishing causal link between training data and preserved concepts.

9. [Language Models Improve When Pretraining Data Matches Target Tasks](https://arxiv.org/html/2507.12466v1)
   - Demonstrates that models trained on domain-specific data preserve task-relevant concepts better than generalist models.

10. [Pruning vs Quantization: Which is Better?](https://arxiv.org/html/2307.02973v2)
    - Provides comparative analysis showing different compression methods have different effects on preserving concepts from training data distribution.

11. [Beyond Size and Accuracy: The Impact of Model Compression on Fairness](https://journals.flvc.org/FLAIRS/article/view/135617)
    - Analyzes how compression impacts fairness across different demographic groups, with implications for concept preservation in underrepresented domains.

12. [The Effect of Model Compression on Fairness in Facial Expression Recognition](https://arxiv.org/abs/2201.01709)
    - While focused on vision, demonstrates that compression effects vary across demographic groups, analogous to domain-specific concept preservation.

13. [You Never Know: Quantization Induces Inconsistent Biases in Vision-Language Foundation Models](https://arxiv.org/html/2410.20265v1)
    - Shows that compression effects are inconsistent across modalities, suggesting that compression interacts complexly with training data composition.

---

## Research Implications

The evidence suggests that **training data bias does significantly affect which concepts are preserved during compression**. A compressor trained on code would likely exhibit:

- **Better preservation of code semantics** due to more effective compression of code-specific patterns
- **Potentially degraded natural language preservation** due to reduced training emphasis on language concepts
- **Language-specific biases** in code (e.g., Python concepts preserved better than C++)
- **Domain-specific phase transitions** where compression thresholds differ based on concept domain

This inversion concept—that the training data of the compressor itself biases what it preserves—is an important consideration for developing specialized compression systems and understanding their limitations.
