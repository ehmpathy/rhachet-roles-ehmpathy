# Research Question: Minimum Viable Semantic Unit (MVSU) for LLM Comprehension

## Question
What is the minimum viable semantic unit (MVSU) for LLM comprehension? Are there formal linguistic theories (e.g., minimal recursion semantics) that define when meaning is preserved vs lost?

---

## Synthesis of Findings

### Key Conceptual Areas

The research question touches on several interconnected domains in computational linguistics and semantic theory:

#### 1. **Traditional Linguistic Foundations**
The classical answer in linguistics identifies the **morpheme** as the minimum unit of meaning—the smallest grammatical unit that carries semantic content and cannot be further subdivided. Examples include word roots (run) and affixes (-ed for past tense). However, this traditional view operates at the lexical level and does not fully address meaning preservation in compositional systems.

#### 2. **Formal Semantic Frameworks**
**Minimal Recursion Semantics (MRS)** provides a computational framework for representing underspecified semantic structures. Rather than proposing a minimum unit size, MRS identifies **elementary predications (EPs)** as the fundamental building blocks—each EP represents a single lexeme with its associated arguments. Importantly, MRS is not itself a semantic theory but a meta-level language for describing semantic structures. The framework maintains a key advantage: it includes treatment of scope that relates to conventional logical representations while using flat syntax (EPs are never nested within one another).

#### 3. **Propositional and Logical Units**
In propositional logic and semantics, an **atomic proposition** serves as the foundational unit—a declarative statement expressing a single idea that cannot be decomposed into simpler statements. Each atomic proposition can be assigned a truth value (true/false). Recent research on LLMs suggests that atomic propositions may need to be represented in the base parameters of language models rather than emerging as byproducts of token prediction, though this remains an unsolved challenge in 2025.

#### 4. **Compositional Semantics**
The **principle of compositionality** states that the meaning of a complex expression is determined by the meanings of its constituent expressions and the rules combining them. This principle is fundamental to how humans understand novel linguistic expressions and underpins most contemporary semantic theory. However, compositionality faces challenges with contextuality, idiomatic expressions, quotations, and figurative language (e.g., sarcasm cannot be inferred purely from component meanings).

#### 5. **Discourse and Elementary Units**
In discourse parsing, **Elementary Discourse Units (EDUs)** represent minimal functional meaning units—typically clauses, but ranging from nominalized noun phrases to complete sentences. Each EDU denotes a single event or type of event and serves as a distinct unit of information. EDUs function as the atomic units upon which discourse relations are built.

#### 6. **Frame-Based Semantics**
**FrameNet** operationalizes semantic frames as minimal meaning structures. The framework identifies two key minimal units: (a) **Lexical Units (LUs)**—words that evoke a frame (e.g., fry, bake, broil in the Apply_heat frame); and (b) **Frame Elements (FEs)**—semantic roles within frames (core and non-core). This approach grounds meaning in structured conceptual scenarios rather than isolated propositions.

#### 7. **Semantic Roles and Thematic Structure**
**Thematic roles** (Agent, Patient, Instrument, etc.) represent the fundamental semantic relationships between predicates and their arguments. These roles are distinct from grammatical roles (subject/object) and represent the minimal relational units that compose event semantics. Semantic role labeling (SRL) answers "who did what to whom"—arguably the most reduced form of meaningful semantic content.

#### 8. **Distributional Semantics and Embeddings**
Word embeddings grounded in **distributional semantics** represent meaning through learned vector spaces where meaning emerges from contextual co-occurrence patterns. The minimal unit here is the individual word vector, with meaning preserved through proximity to similarly-situated vectors. This approach differs fundamentally from compositional approaches by encoding meaning latently rather than explicitly.

#### 9. **LLM-Specific Meaning Preservation**
Recent work on **semantic chunking for LLMs** identifies that meaning preservation requires chunks to be "semantically coherent units"—balancing smallness (for efficiency) with largeness (for context retention). The challenge is determining the threshold where semantic coherence is maintained. Additionally, **atomic fact extraction** from complex claims requires decomposing them into atomic facts while preserving semantic intent—a task that conventional static strategies fail at because they prioritize syntactic fragmentation over contextual understanding.

#### 10. **The Meaning Preservation Paradox**
Current research reveals a fundamental tension: meaning cannot be reduced to atomic units without loss of context, yet sufficiently large units become computationally intractable. The minimum viable semantic unit appears to be context-dependent, varying based on:
- The specific semantic phenomenon being represented (events, attributes, relations)
- The computational architecture of the system (token-based, embedding-based, or symbolic)
- The downstream task (understanding, reasoning, verification)
- The linguistic form (literal vs. figurative language)

---

## Research Sources (11+)

### 1. Minimal Recursion Semantics: An Introduction
**URL:** https://www.cl.cam.ac.uk/~aac10/papers/mrs.pdf
**Contribution:** Foundational reference defining MRS framework. Explains how elementary predications serve as fundamental units, how flat syntax with scope treatment preserves meaning without nesting, and how MRS functions as a meta-level language for describing semantic structures. Critical for understanding formal approaches to semantic granularity.

### 2. Minimal Recursion Semantics - Wikipedia
**URL:** https://en.wikipedia.org/wiki/Minimal_recursion_semantics
**Contribution:** Accessible overview of MRS as a standard formalism used in large-scale HPSG (Head-Driven Phrase Structure Grammar) grammars. Clarifies how MRS handles underspecification—representing ambiguity without enumerating all readings—which is relevant to understanding how meaning can be preserved with minimal specification.

### 3. Natural Language Processing - Semantic Analysis (TutorialsPoint)
**URL:** https://www.tutorialspoint.com/natural_language_processing/natural_language_processing_semantic_analysis.htm
**Contribution:** Systematic overview of semantic units in NLP practice. Distinguishes lexical semantics (word meaning) from compositional semantics (phrase/sentence meaning). Explains semantic analysis's role in extracting machine-understandable meaning from natural language.

### 4. Understanding Semantic Analysis - NLP (GeeksforGeeks)
**URL:** https://www.geeksforgeeks.org/nlp/understanding-semantic-analysis-nlp/
**Contribution:** Detailed breakdown of semantic elements in NLP including lexical items, word sense disambiguation, and thematic roles. Explains how semantic units at different levels (words, phrases, sentences) combine to form meaningful expressions.

### 5. ChunkKV: Semantic-Preserving KV Cache Compression for Efficient Long-Context LLM Inference
**URL:** https://arxiv.org/html/2502.00299v5
**Contribution:** Recent (2025) work directly addressing semantic preservation in LLMs. Demonstrates practical approaches to maintaining meaning while compressing representations, showing that semantic coherence can be preserved through selective retention of key semantic elements.

### 6. Chunking Strategies for LLM Applications (Pinecone)
**URL:** https://www.pinecone.io/learn/chunking-strategies/
**Contribution:** Practical framework for semantic chunking in LLM applications. Explains how semantic chunking identifies logical breaks in text based on meaning rather than arbitrary boundaries, addressing the practical challenge of defining minimum coherent semantic units for RAG systems.

### 7. Principle of Compositionality - Wikipedia
**URL:** https://en.wikipedia.org/wiki/Principle_of_compositionality
**Contribution:** Comprehensive treatment of compositionality—the principle that complex meaning emerges from component meanings and combination rules. Essential for understanding how meaning is preserved when moving between different levels of semantic granularity.

### 8. Compositionality (Stanford Encyclopedia of Philosophy)
**URL:** https://plato.stanford.edu/entries/compositionality/
**Contribution:** Authoritative philosophical treatment of compositionality challenges and limitations. Discusses how meaning preservation breaks down for non-compositional phenomena (idioms, sarcasm, quotation), suggesting boundaries where minimal units cannot preserve full meaning.

### 9. On the Semantics of Large Language Models
**URL:** https://arxiv.org/html/2507.05448v1
**Contribution:** Contemporary research directly addressing how LLMs represent and understand meaning. Discusses challenges in representing atomic propositions and the gap between token-level processing and meaningful semantic units.

### 10. Fact in Fragments: Deconstructing Complex Claims via LLM-based Atomic Fact Extraction and Verification
**URL:** https://arxiv.org/html/2506.07446v1
**Contribution:** Addresses practical decomposition of complex semantic content into atomic units. Shows that conventional static decomposition strategies fail because they ignore contextual semantic dependencies, suggesting that true minimum viable units are context-dependent.

### 11. Semantic Role Labeling (Stanford NLP Course)
**URL:** https://web.stanford.edu/~jurafsky/slp3/slides/22_SRL.pdf
**Contribution:** Foundational treatment of semantic roles as minimal relational units (Agent, Patient, Instrument, etc.). Demonstrates how thematic structure provides a fundamental minimal unit for event semantics independent of syntactic realization.

### 12. Distributional Semantics and Linguistic Theory
**URL:** https://arxiv.org/pdf/1905.01896
**Contribution:** Scholarly treatment of how meaning emerges from distributional patterns. Provides theoretical grounding for understanding how meaning can be preserved through learned vector representations—an alternative to explicit compositional approaches.

### 13. Frame Semantics and FrameNet (Michael Brenndoerfer)
**URL:** https://mbrenndoerfer.com/writing/history-framenet-frame-semantics
**Contribution:** Overview of frame semantics as a structured approach to meaning. Explains how frames provide minimal semantic structures that bind lexical units and frame elements into coherent meaning units, offering an alternative to pure compositionality.

### 14. Minimal discourse units in spoken French: Uncovering genre-bound segmentation strategies
**URL:** https://www.semanticscholar.org/paper/Minimal-discourse-units-in-spoken-French:-Degand-Simon/32fb02d2cc0ab13b3fbcd0a316861fbbb831fc89
**Contribution:** Empirical linguistic research showing that minimal units vary by discourse genre and register. Demonstrates that the minimum viable semantic unit is not fixed but context and genre-dependent.

### 15. Atom of Thoughts: A Paradigm Shift in LLM Reasoning and Efficiency
**URL:** https://medium.com/@armankamran/atom-of-thoughts-a-paradigm-shift-in-llm-reasoning-and-efficiency-d2408e5ab663
**Contribution:** Novel LLM-specific framework treating atomic questions/propositions as minimal reasoning units. Demonstrates that decomposing complex problems into atomic states improves both reasoning and efficiency, suggesting practical validation for atomic units in LLM processing.

---

## Conclusion

The minimum viable semantic unit for LLM comprehension is not a fixed concept but emerges from the interaction of multiple theoretical frameworks:

- **Formally**: Minimal Recursion Semantics provides a framework (elementary predications) but not a minimum threshold
- **Logically**: Atomic propositions serve as irreducible units but require proper representation in model parameters
- **Linguistically**: Morphemes, words, thematic roles, and discourse units all serve as minimal units at different levels
- **Computationally**: Meaning preservation depends on architectural choices (token-based, embedding-based, symbolic)
- **Pragmatically**: The minimum unit varies by task, domain, and linguistic phenomenon

Meaning is preserved vs. lost based on:
1. Whether all necessary propositional content is captured
2. Whether semantic relationships (agent, patient, recipient) are maintained
3. Whether scope and modal properties are represented
4. Whether contextual dependencies are preserved
5. Whether the unit allows for compositional reconstruction of larger meanings

The research suggests that a truly "minimum viable" semantic unit requires context-awareness and cannot be defined in isolation—pointing toward dynamic, task-specific definitions of semantic granularity rather than universal fixed-size units.
