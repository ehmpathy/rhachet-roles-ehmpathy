# Probe Questions: Telegraphic Semantic Compression for Brief Minification

## Purpose
Structured exploration questions to research how telegraphic semantic compression can minify AI agent briefs without knowledge loss.

---

## Domain 1: Foundational Linguistics & Semantics

1. **What is the minimum viable semantic unit (MVSU) for LLM comprehension?**
   - Are there formal linguistic theories (e.g., minimal recursion semantics) that define when meaning is preserved vs lost?

2. **Which word classes are truly "droppable" for LLMs vs humans?**
   - Function words (articles, conjunctions, prepositions) vs content words (nouns, verbs, adjectives)
   - Do LLMs rely differently on these compared to human parsing?

3. **What role does syntactic structure play in LLM prompt interpretation?**
   - Can we drop all grammatical scaffolding if semantic payload is preserved?
   - Does "the user clicks the button" = "user click button" for LLMs?

4. **How do telegraphic/pidgin languages naturally achieve compression?**
   - What lessons from Tok Pisin, Chinese Pidgin English, or telegram-speak transfer to our domain?

---

## Domain 2: Information Theory & Redundancy

5. **What is the Shannon entropy of typical instruction text vs compressed brief text?**
   - How much "predictable" content exists in our mechanic briefs?

6. **What does Zipf's Law tell us about which tokens carry most information?**
   - Should high-frequency tokens be preferentially dropped?

7. **What is the theoretical compression limit for semantic-preserving text reduction?**
   - Information-theoretic bounds on semantic compression (ref: arxiv 2306.02305)

8. **How do LLMs perform as compressors vs decompressors?**
   - Given that LLM training ≈ finding redundancy, can we exploit their internal models to guide compression?

---

## Domain 3: Existing Tools & Techniques

9. **What compression ratios does LLMLingua achieve and at what performance cost?**
   - 20x compression with 1.5% accuracy drop (GSM8K) — can we match or exceed this for briefs?

10. **What is the difference between LLMLingua (token-level) vs TSC (telegraphic-style) vs summarization?**
    - Which preserves behavioral equivalence best for instruction-following tasks?

11. **How does LLMLingua-2's data distillation approach improve compression faithfulness?**
    - Is learned compression better than rule-based compression?

12. **What existing prompt compression benchmarks exist and are any applicable to instruction briefs?**
    - GSM8K, BBH, ShareGPT, NaturalQuestions — but what about role/instruction compliance?

---

## Domain 4: Evaluation & Measurement

13. **How do we define "semantic preservation" for agent briefs operationally?**
    - Behavioral equivalence: same actions given same inputs?
    - Constraint compliance: same boundaries respected?

14. **What eval methodology can detect subtle knowledge loss from compression?**
    - Edge case testing, adversarial probing, boundary condition queries?

15. **Can we use LLM-as-judge to compare compressed vs uncompressed brief behavior?**
    - Self-consistency checks, output comparison scoring?

16. **What is the acceptable loss threshold for brief compression?**
    - Is 1-2% accuracy drop acceptable? What if it affects safety constraints?

---

## Domain 5: Practical Application to Mechanic Briefs

17. **What are the distinct "zones" of a mechanic brief with different compression tolerance?**
    - Identity/role definitions vs behavioral constraints vs examples vs tone guidance?

18. **Are there brief-specific tokens that must NEVER be dropped?**
    - Negations ("do NOT"), quantifiers ("always", "never"), entity names, constraint keywords?

19. **How should structured formatting (JSON, YAML, bullets) affect compression strategy?**
    - Do structured formats already represent near-optimal compression?

20. **Can we create a brief-specific "stop word" list tailored to our domain?**
    - Different from general NLP stop words — domain-aware function words?

21. **How do we handle multi-language or technical jargon in compression?**
    - Domain terms may look redundant but carry critical meaning

---

## Domain 6: Divergent Explorations

22. **Could we use a smaller "compression" LLM to pre-process briefs for larger inference LLMs?**
    - LLMLingua uses GPT-2-small to compress for GPT-4 — viable for production?

23. **What about semantic hashing or embedding-based compression rather than lexical?**
    - Compress to embedding space, decompress on-demand?

24. **Could we train a brief-specific compression model on our own corpus?**
    - Custom fine-tuned compressor for mechanic brief dialect?

25. **What if we compression-expanded: adding redundancy strategically for robustness?**
    - Inverse: when does MORE text improve reliability?

26. **How do prompt caching and KV-cache compression interact with semantic compression?**
    - Orthogonal optimizations that could stack?

---

## Domain 7: Inversions & Failure Modes (Critical)

27. **INVERSION: What happens when compression destroys critical context?**
    - Case studies of compression failures — what broke and why?

28. **INVERSION: When is verbosity actually necessary?**
    - Are there brief sections where expansion improves compliance?

29. **INVERSION: Could aggressive compression introduce ambiguity that causes harm?**
    - "Don't help with X" compressed to "help X" — catastrophic?

30. **INVERSION: What if different tasks/models need different compression levels?**
    - One-size-fits-all compression vs adaptive compression?

31. **INVERSION: Does compression training data bias affect which concepts are preserved?**
    - If compressor was trained on code, will it preserve code concepts better?

---

## Domain 8: Real-World Evidence & Case Studies

32. **What are documented production uses of prompt compression with results?**
    - Microsoft's LLMLingua deployments, RAG optimization case studies

33. **Has anyone applied TSC specifically to system prompts / role instructions?**
    - Most research focuses on user queries / RAG context, not system instructions

34. **What do practitioners report about compression side effects?**
    - Anecdotal reports of "personality drift" or "constraint relaxation" post-compression?

35. **What A/B testing frameworks exist for prompt compression in production?**
    - How do teams measure compression impact at scale?

---

## Summary of Key Research Vectors

| Vector | Core Question | Risk Level |
|--------|---------------|------------|
| Linguistic | What can be dropped? | Medium |
| Information Theory | How much redundancy exists? | Low |
| Tooling | What works already? | Low |
| Evaluation | How do we know it worked? | High |
| Application | How does this apply to briefs? | High |
| Divergent | What else could work? | Medium |
| Inversions | What could go wrong? | Critical |
| Evidence | What's proven? | High |

---

## Sources Consulted

### Telegraphic Semantic Compression
- [TSC: A Semantic Compression Method for LLM Contexts](https://developer-service.blog/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts/)
- [TSC GitHub Repository](https://github.com/nunombispo/TelegraphicSemanticCompression-Article)
- [TSC on Medium](https://medium.com/django-unleashed/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts-45de3ebbae96)

### LLMLingua & Microsoft Research
- [LLMLingua Official Site](https://llmlingua.com/)
- [Microsoft Research: LLMLingua Blog](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)
- [LLMLingua GitHub](https://github.com/microsoft/LLMLingua)
- [LLMLingua-2 Paper](https://llmlingua.com/llmlingua2.html)
- [LLMLingua arXiv](https://arxiv.org/abs/2310.05736)

### Prompt Compression Techniques
- [Prompt Compression in LLMs - Making Every Token Count](https://medium.com/@sahin.samia/prompt-compression-in-large-language-models-llms-making-every-token-count-078a2d1c7e03)
- [Machine Learning Mastery: Prompt Compression](https://machinelearningmastery.com/prompt-compression-for-llm-generation-optimization-and-cost-reduction/)
- [How to Cut RAG Costs by 80%](https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb/)
- [DataCamp: Prompt Compression Guide](https://www.datacamp.com/tutorial/prompt-compression)

### Information Theory
- [Semantic Compression - Wikipedia](https://en.wikipedia.org/wiki/Semantic_compression)
- [Information-Theoretic Limits on Semantic Compression](https://arxiv.org/pdf/2306.02305)
- [Why GPT Still Listens to Shannon](https://medium.com/@satyamcser/why-gpt-still-listens-to-shannon-the-hidden-mathematics-of-language-compression-1c3ce6b62cd8)

### Linguistics & Function Words
- [Content Words, Function Words, and LLM Analysis](https://generic.wordpress.soton.ac.uk/skywritings/2023/09/16/content-words-function-words-and-llm-analysis-of-dictionaries/)
- [Morphology: Content Words and Function Words](https://thainlpbook.wordpress.com/2018/01/14/morphology-content-words-function-words-and-grammaticalization/)
- [Stop Words in NLP](https://botpenguin.com/glossary/stop-words)
