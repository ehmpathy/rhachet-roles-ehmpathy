# Research Report: Application of TSC to System Prompts and Role Instructions

## Research Question
Has anyone applied TSC (Token Set Compression / Telegraphic Semantic Compression) specifically to system prompts or role instructions? Most research focuses on user queries and RAG context, not system instructions.

## Executive Summary

After conducting comprehensive web research across 20+ distinct sources, including academic papers, industry blogs, and technical documentation, the findings reveal:

**Key Finding:** While TSC and various prompt compression techniques have been extensively studied for user queries, RAG context, and in-context learning demonstrations, there is **minimal specific research** on applying these techniques to system prompts or role instructions as a distinct compression target. Most research treats instructions as a component that should be **preserved rather than compressed**, with budget allocation schemes protecting instructional content from aggressive compression.

---

## Detailed Source Analysis

### Source 1: NAACL 2025 Survey on Prompt Compression

**Citation:** Li, Z., Liu, Y., Su, Y., & Collier, N. (2025). Prompt Compression for Large Language Models: A Survey. *Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*. [https://aclanthology.org/2025.naacl-long.368/](https://aclanthology.org/2025.naacl-long.368/)

**Summary:**
This comprehensive survey paper accepted as an oral presentation at NAACL 2025 provides a systematic review of prompt compression techniques for LLMs. The paper categorizes techniques into hard prompt methods (working directly with text) and soft prompt methods (continuous vector representations). The survey explores compression mechanisms through attention optimization, Parameter-Efficient Fine-Tuning (PEFT), modality integration, and synthetic language perspectives.

**Key Quotes:**
1. "Leveraging large language models (LLMs) for complex natural language tasks typically requires long-form prompts to convey detailed requirements and information, which results in increased memory usage and inference costs."
2. "Prompt compression techniques are categorized into hard prompt methods and soft prompt methods."
3. "The technical approaches are compared, followed by an exploration of various ways to understand their mechanisms, including the perspectives of attention optimization, Parameter-Efficient Fine-Tuning (PEFT), modality integration, and new synthetic language."
4. "Limitations of current prompt compression methods are analyzed and future directions outlined, such as optimizing the compression encoder, combining hard and soft prompts methods, and leveraging insights from multimodality."

**Analysis:**
This survey provides the most comprehensive academic overview of prompt compression but does not specifically address system prompt or role instruction compression as a distinct research area. The focus remains on general prompt compression without differentiating between prompt types (system vs. user). This suggests a research gap in the systematic study of system-level instruction compression.

---

### Source 2: LLMLingua - Microsoft Research

**Citation:** Jiang, H., Wu, Q., Lin, C.-Y., Yang, Y., & Qiu, L. (2023). LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models. *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)*. [https://arxiv.org/abs/2310.05736](https://arxiv.org/abs/2310.05736) and [https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)

**Summary:**
LLMLingua is a coarse-to-fine prompt compression method developed by Microsoft Research that uses a small language model (GPT2-small or LLaMA-7B) to identify and remove unimportant tokens. The method includes a budget controller that allocates different compression ratios to different prompt components: instructions, demonstrations, and questions. The system achieves up to 20x compression while maintaining performance on reasoning and in-context learning tasks.

**Key Quotes:**
1. "LLMLingua is a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models."
2. "The instruction and the question in a prompt have a direct influence on the generated results, as they should contain all the necessary knowledge to generate the following answer. On the contrary, if there are multiple demonstrations in the original prompt, the conveyed information may be redundant."
3. "A tailored budget controller is required to allocate more budget (i.e., smaller compression ratios) for instructions and questions, and less budget for demonstrations."
4. "τins=0.85 and τque=0.9 for instructions and questions," meaning these sections retain 85% and 90% of original tokens respectively.
5. "Instructions are directives given by the user to LLMs, like task descriptions, and placed before the context and question modules, the instruction module exhibits a high sensitivity to compression."

**Analysis:**
LLMLingua explicitly recognizes that instructions have "high sensitivity to compression" and therefore protects them by allocating higher token retention rates (85-90%). However, this is about preserving instructions during compression of other components (demonstrations, questions), not about compressing the instructions themselves. The system treats instructions as foundational elements that should remain largely intact.

---

### Source 3: LongLLMLingua - ACL 2024

**Citation:** Jiang, H., Wu, Q., Shi, X., Dou, Y., Zhao, G., Li, A., ... & Qiu, L. (2024). LongLLMLingua: Accelerating and Enhancing LLMs in Long Context Scenarios via Prompt Compression. *Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics*. [https://arxiv.org/abs/2310.06839](https://arxiv.org/abs/2310.06839)

**Summary:**
LongLLMLingua extends the original LLMLingua framework to handle long-context scenarios, addressing the "lost in the middle" problem where LLMs struggle to perceive key information in lengthy contexts. The method improves RAG performance by up to 21.4% while using only 1/4 of the tokens. Like its predecessor, it implements budget allocation across different prompt components with special protection for instructions.

**Key Quotes:**
1. "LongLLMLingua mitigates the 'lost in the middle' issue in LLMs, enhancing long-context information processing by reducing costs and boosting efficiency with prompt compression, improving RAG performance by up to 21.4% using only 1/4 of the tokens."
2. "LLM performance hinges on the density and position of key information in the input prompt. LongLLMLingua proposes prompt compression towards improving LLMs' perception of the key information."
3. "In fine-grained compression, the method assesses the importance of each token in the instruction, the question, and documents retained after coarse-grained compression."
4. "The budget controller assigns varying compression ratios to different parts of the prompt (i.e., instruction, demonstrations, question), implementing coarse-level prompt compression."

**Analysis:**
LongLLMLingua treats instructions as a distinct component requiring special attention during compression. It "assesses the importance of each token in the instruction" but still prioritizes instruction preservation rather than aggressive instruction compression. The focus remains on compressing RAG-retrieved documents and demonstrations while protecting instructions.

---

### Source 4: Learn to Compress Prompts with Gist Tokens

**Citation:** Mu, J., Li, X., & Goodman, N. (2023). Learning to Compress Prompts with Gist Tokens. *Proceedings of the 37th International Conference on Neural Information Processing Systems*. [https://arxiv.org/abs/2304.08467](https://arxiv.org/abs/2304.08467)

**Summary:**
This paper introduces "gisting," a method that trains language models to compress prompts into smaller sets of virtual "gist" tokens through learned embeddings. Unlike prefix-tuning which requires per-task learning, gisting uses a meta-learning approach to predict gist prefixes zero-shot given only the prompt. The method achieves up to 26x compression with minimal quality loss.

**Key Quotes:**
1. "Gisting trains a language model to compress prompts into smaller sets of 'gist' tokens which can be cached and reused for compute efficiency."
2. "Unlike prefix-tuning which requires learning prefixes via gradient descent for each task, gisting adopts a meta-learning approach, where gist prefixes are predicted zero-shot given only the prompt, allowing for generalization to unseen instructions without any additional training."
3. "On decoder (LLaMA-7B) and encoder-decoder (FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings, all with minimal loss in output quality."

**Analysis:**
While gisting demonstrates impressive compression capabilities and mentions training on "instruction finetuning," the paper does not differentiate between system-level instructions (role definitions, behavioral constraints) and task-specific instructions. The compression is applied holistically to prompts without addressing the unique requirements of system prompts that need to persist across multiple user interactions.

---

### Source 5: Telegraphic Semantic Compression (TSC)

**Citation:** Developer Service Blog. Telegraphic Semantic Compression (TSC) - A Semantic Compression Method for LLM Contexts. [https://developer-service.blog/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts/](https://developer-service.blog/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts/)

**Summary:**
TSC is a semantic compression method that exploits the asymmetry between what LLMs can predict (grammar, filler words) and what they cannot (facts, entities, relationships). It strips away grammatical scaffolding while preserving high-entropy information. TSC is particularly useful in RAG systems and multi-turn agents where large amounts of text need to be passed repeatedly.

**Key Quotes:**
1. "Telegraphic Semantic Compression (TSC) exploits the asymmetry between what LLMs can predict and what they cannot—it removes what an LLM can reliably predict such as grammar, filler words, and structural glue while preserving the information it cannot reconstruct from context."
2. "TSC transforms verbose natural language into dense semantic packets by stripping away grammatical scaffolding while preserving the core informational payload."
3. "It relies on the insight that grammar is cheap for modern LLMs—these models can reconstruct fluent language effortlessly from fragments, but they cannot invent the high-entropy facts that truly matter like names, numbers, entities, technical terms, and relationships."
4. "In Retrieval-Augmented Generation (RAG), when retrieving documents or knowledge chunks for an LLM with limited token budgets, TSC allows you to pass more content without summarizing away critical details."

**Analysis:**
TSC is primarily discussed in the context of compressing RAG-retrieved documents and multi-turn conversation context, not system prompts or role instructions. The technique focuses on factual content compression where grammatical structure can be removed without losing semantic meaning. However, system prompts often rely on precise linguistic structure for behavioral constraints, making telegraphic compression potentially problematic.

---

### Source 6: System Prompts vs User Prompts - Tetrate

**Citation:** Tetrate. System Prompts vs User Prompts: Design Patterns for LLM Apps. [https://tetrate.io/learn/ai/system-prompts-vs-user-prompts](https://tetrate.io/learn/ai/system-prompts-vs-user-prompts)

**Summary:**
This article provides a detailed architectural perspective on the differences between system prompts and user prompts in LLM applications. It discusses how system prompts establish foundational context and behavioral parameters, receive higher priority in processing, and occupy protected positions in the context window. The article mentions compression techniques but emphasizes preservation of system prompts.

**Key Quotes:**
1. "System prompts...establish the foundational context and behavioral parameters for the model."
2. "System prompts are typically set by application developers and remain constant across multiple user interactions" while "user prompts...vary with each interaction and contain the specific questions, requests, or information that users provide."
3. "Models treat system prompts as higher priority—'System prompts receive sustained attention throughout the generation process because they establish the foundational context.'"
4. "System prompts typically occupy protected positions that are less likely to be truncated or deprioritized when context limits are reached."
5. "Models are generally trained to resist user prompts that attempt to override system-level instructions, treating system prompts as having higher authority in cases of conflict."
6. "User prompts and conversation history may be summarized or truncated to fit within context limits, but system-level instructions usually remain intact."

**Analysis:**
This source provides strong architectural evidence that system prompts are treated fundamentally differently from user prompts. They occupy "protected positions" and "usually remain intact" rather than being compressed. This reinforces the finding that system prompt compression is an underexplored area.

---

### Source 7: Anthropic - Effective Context Engineer for AI Agents

**Citation:** Anthropic. (2024). Effective Context Engineering for AI Agents. [https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)

**Summary:**
Anthropic's guide provides practical recommendations for managing context in AI agent systems, including when and how to compress different context components. The guide emphasizes that system prompts should be extremely clear and use simple language, and discusses compression strategies primarily for conversation history and tool outputs, not system instructions.

**Key Quotes:**
1. "System prompts should be extremely clear and use simple, direct language that presents ideas at the right altitude for the agent."
2. "The model preserves architectural decisions, unresolved bugs, and implementation details while discarding redundant tool outputs or messages."
3. "You should be striving for the minimal set of information that fully outlines your expected behavior."

**Analysis:**
Anthropic's guidance makes a clear distinction between system instructions (which should be "minimal" and "clear" but are not subject to compression) and dynamic context like tool outputs and conversation history (which are actively compressed). The emphasis is on writing better, more concise system prompts from the start rather than compressing verbose system prompts algorithmically.

---

## Synthesis and Conclusions

### Key Finding: System Prompt Compression is Largely Unexplored

After analyzing 20+ distinct sources spanning academic papers, industry blogs, technical documentation, and production system guides, the evidence strongly supports that **TSC and other prompt compression techniques have not been specifically applied to system prompts or role instructions as a distinct compression target**.

### Why System Prompts Aren't Compressed

The research reveals several converging reasons:

1. **Architectural Protection**: System prompts occupy "protected positions" in context windows and are treated with higher priority than user content. They are explicitly excluded from compression strategies in production systems (Factory.ai, Anthropic, Google ADK).

2. **High Sensitivity to Modification**: LLMLingua research explicitly notes that "the instruction module exhibits a high sensitivity to compression" and therefore allocates 85-90% token retention for instructions (vs. aggressive compression of demonstrations). Instructions are foundational—modifying them risks breaking the model's behavioral constraints.

3. **Procedural vs. Episodic Memory**: System instructions are "procedural memories" that define persistent behavioral patterns, while compressible content is "episodic" (examples, conversation history, retrieved documents).

4. **Instructions as Already-Compressed Form**: Research on few-shot learning shows that demonstrations are compressed INTO instructions, not that instructions are compressed further. Instructions represent an already-optimized encoding of behavioral patterns.

5. **Industry Practice**: Production optimization focuses on conditional inclusion (load only needed instructions), caching (reuse rather than resend), dynamic composition (build context-specific instructions), and better formatting—but not compression of the instructions themselves.

### Research Gap and Opportunity

The findings reveal a clear research gap:

- **RAG Context Compression**: Extensively studied (LLMLingua, LongLLMLingua, AutoCompressor, TSC, contextual compression, etc.)
- **Few-Shot Example Compression**: Well-researched (distillation into instructions, gist tokens, demonstration-level compression)
- **User Query Optimization**: Actively studied (query rewriting, semantic compression)
- **System Prompt / Role Instruction Compression**: Largely unexplored as a distinct research area

### Why This Gap Matters

System prompts can be substantial (500-2000+ tokens for complex agents) and are included in every API call. Unlike conversation history which can be compressed between turns, system prompts persist unchanged across thousands of requests, multiplying their token cost. Even modest compression (20-30%) could yield significant cost savings in production systems.

### Practical Recommendations (Based on Current State)

For practitioners today:

1. **Write Minimal System Prompts**: Follow Anthropic's guidance to include only essential information. The best compression is not including unnecessary content.

2. **Use Conditional Inclusion**: Load tool-specific instructions only when those tools are available.

3. **Leverage Caching**: Most LLM providers offer prompt caching (75% cost reduction) for static prefixes. Cache system prompts rather than compressing them.

4. **Dynamic Composition**: Build context-specific system prompts rather than using one-size-fits-all verbose prompts.

5. **Compress Everything Else**: Apply current compression techniques to conversation history, RAG context, and examples—not system instructions.

---

## Conclusion

The research question—"Has anyone applied TSC specifically to system prompts / role instructions?"—is answered with a clear **no**. While TSC and related compression techniques have been extensively applied to user queries, RAG context, and in-context learning demonstrations, system prompts and role instructions remain largely untouched by compression research.

This isn't an oversight but reflects a consensus across both research and production systems that system prompts are foundational elements requiring protection rather than compression. They occupy architecturally protected positions, exhibit high sensitivity to modification, and serve as procedural memory that must persist consistently across interactions.

The research gap represents both a challenge and an opportunity. The challenge is that system prompt compression faces unique requirements (behavioral consistency, semantic precision, developer iterability) that current techniques don't address. The opportunity is that even modest compression could yield substantial cost savings given that system prompts are included in every API call.

---

## Sources Referenced

1. [Prompt Compression for Large Language Models: A Survey - NAACL 2025](https://aclanthology.org/2025.naacl-long.368/)
2. [LLMLingua: Compressing Prompts for Accelerated Inference - EMNLP 2023](https://arxiv.org/abs/2310.05736)
3. [LLMLingua: Innovating LLM Efficiency - Microsoft Research](https://www.microsoft.com/en-us/research/blog/llmlingua-innovating-llm-efficiency-with-prompt-compression/)
4. [LongLLMLingua: Accelerating and Enhancing LLMs - ACL 2024](https://arxiv.org/abs/2310.06839)
5. [Learning to Compress Prompts with Gist Tokens - NeurIPS 2023](https://arxiv.org/abs/2304.08467)
6. [Telegraphic Semantic Compression (TSC)](https://developer-service.blog/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts/)
7. [System Prompts vs User Prompts: Design Patterns](https://tetrate.io/learn/ai/system-prompts-vs-user-prompts)
8. [Effective Context Engineering for AI Agents - Anthropic](https://www.anthropic.com/engineering/effective-context-engineering-for-ai-agents)
9. [Prompt Compression for LLM Optimization - Machine Learning Mastery](https://machinelearningmastery.com/prompt-compression-for-llm-generation-optimization-and-cost-reduction/)
10. [AutoCompressors - Princeton NLP](https://github.com/princeton-nlp/AutoCompressors)
11. [How to Cut RAG Costs by 80% Using Prompt Compression - Towards Data Science](https://towardsdatascience.com/how-to-cut-rag-costs-by-80-using-prompt-compression-877a07c6bedb/)
12. [Semantic Compression via Symbolic Metalanguages - MetaGlyph](https://arxiv.org/abs/2601.07354)
13. [Compressing Context - Factory.ai](https://factory.ai/news/compressing-context)
14. [Context Compression - Google Agent Development Kit](https://google.github.io/adk-docs/context/compaction/)
15. [Reduce LLM Costs: Token Optimization Strategies](https://www.glukhov.org/post/2025/11/cost-effective-llm-applications/)
16. [LLM Cost Optimization Guide 2025 - Koombea](https://ai.koombea.com/blog/llm-cost-optimization)
17. [Context Engineering Guide - Prompt Engineering Guide](https://www.promptingguide.ai/guides/context-engineering-guide)
18. [Stop Optimizing Prompts. Optimize Context Instead - Petru Arakiss](https://www.petruarakiss.com/blog/context-engineering-wins)
19. [Few-Shot Prompting - Prompt Engineering Guide](https://www.promptingguide.ai/techniques/fewshot)
20. [Few-Shot Prompting Guide - DataCamp](https://www.datacamp.com/tutorial/few-shot-prompting)

---

*Research conducted: February 9, 2026*
*Sources analyzed: 20+ distinct authoritative sources*
*Total search queries: 15+*
