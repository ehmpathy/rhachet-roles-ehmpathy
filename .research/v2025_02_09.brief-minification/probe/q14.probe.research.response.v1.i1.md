# Research Question: What eval methodology can detect subtle knowledge loss from compression?

## Executive Summary

This research examines evaluation methodologies for detection of subtle knowledge loss from neural network and large language model (LLM) compression, with specific focus on edge case tests, adversarial probes, and boundary condition queries. Through analysis of 15+ authoritative sources from 2019-2026, this report identifies comprehensive approaches that include:

- **Multi-dimensional evaluation frameworks** that go beyond simple accuracy metrics
- **Adversarial robustness tests** with PGD, FGSM, and other attack methods
- **Semantic drift detection** and knowledge retention scores
- **Boundary value analysis** and edge case tests
- **Out-of-distribution (OOD) generalization** assessment
- **Differential tests** to identify deviated behaviors
- **Task-specific capability preservation** metrics

The research reveals that compressed models often maintain top-line performance metrics while they exhibit significant degradation in edge cases, adversarial scenarios, and underrepresented data distributions—which makes sophisticated evaluation methodologies essential.

---

## Source 1: Model Compression vs. Adversarial Robustness (2025)

**Citation:** Frontiers in Robotics and AI (2025). "A survey of model compression techniques: past, present, and future"
**URL:** https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1518965/full

### Summary
This comprehensive 2025 survey examines model compression techniques which include quantization, prune, low-rank decomposition, and knowledge distillation. The research reveals a critical vulnerability: compressed models maintain comparable performance on standard benchmarks but exhibit significantly reduced robustness when subjected to adversarial attacks.

### Key Quotes

1. "The goal of model compression is to significantly reduce the number of parameters, improve inference speed, and lower response latency, all while it maintains the model's generalization performance."

2. "Compressed models generally maintain comparable performance to their uncompressed counterparts, however, when subjected to adversarial attacks, compressed models exhibit significantly reduced robustness."

3. "Under adversarial attacks, compressed models exhibit reduced robustness, with knowledge-distilled models that experience the most significant performance drop, while prune and quantization tend to be more robust than knowledge distillation."

4. "Robustness of compressed models is assessed across multiple tasks with six evaluation metrics and four commonly used classical adversarial attacks."

5. "To apply conventional quantization methods to MobileNetV2 led to a drastic performance drop from 70.9% to 0.1% on ImageNet."

### Relevance to Research Question
This source demonstrates that adversarial robustness tests are essential for detection of subtle knowledge loss that standard accuracy metrics miss. The result that different compression methods (KD vs. prune vs. quantization) show different vulnerabilities under adversarial conditions highlights the need for multi-faceted evaluation approaches.

---

## Source 2: Comprehensive Study on Performance Evaluation and Optimization (2024)

**Citation:** arXiv:2407.15904 (2024). "Comprehensive Study on Performance Evaluation and Optimization of Model Compression: Bridge Traditional Deep Learn and Large Language Models"
**URL:** https://arxiv.org/html/2407.15904

### Summary
This study provides a systematic framework for evaluation of model compression across traditional deep learn and LLMs. It emphasizes multi-dimensional assessment which includes size reduction, inference latency, and accuracy retention while it identifies critical performance thresholds where degradation becomes significant.

### Key Quotes

1. "To test the deployability of a model the important parameters to know are its inference time, performance and the size."

2. "There is a threshold till which the models can be pruned efficiently without any significant effect on the performance."

3. "Prune does not guarantee any decrease in the latency, and neither is observed in our experiments."

4. "Dynamic range and float16 quantization maintain reasonable performance across image classification, object detection, and language models."

5. "Beyond 75-90% sparsity, irreversible performance degradation occurs."

### Relevance to Research Question
This source establishes the importance of task-specific metrics (accuracy for classification, mAP for detection, F1/EM for language, FID for generation) in detection of subtle degradation. The identification of critical thresholds (75-90% sparsity) demonstrates that comprehensive evaluation can predict catastrophic failure points before they occur.

---

## Source 3: Adversarial Fine-tune of Compressed Neural Networks (2024)

**Citation:** arXiv:2403.09441 (2024). "Adversarial Fine-tune of Compressed Neural Networks for Joint Improvement of Robustness and Efficiency"
**URL:** https://arxiv.org/html/2403.09441

### Summary
This research demonstrates that adversarial fine-tune can recover robustness lost from compression. The study uses Projected Gradient Descent (PGD) as the primary evaluation method and reveals dramatic improvements with minimal computational overhead.

### Key Quotes

1. "In practice, we use a simple and common strategy, called Projected Gradient Descent (PGD), to obtain a lower bound of the maximum."

2. "The compressed standard models, which undergo adversarial fine-tune, of only three epochs achieve robustness which is within a 5% difference from the fully adversarially trained model."

3. "For Fashion-MNIST pruned models, robustness improved from essentially zero (00.28±0.64) to 76.74±2.33 with just three epochs of adversarial fine-tune."

4. "To perform adversarial fine-tune instead of adversarial train can reduce the computation time from about 118 minutes to only about 14 minutes on the CIFAR10 dataset."

5. "We evaluated adversarial robustness only against PGD-ℓ∞ attacks. However, true robustness necessitates performance that works well against a diverse range of attack methods."

### Relevance to Research Question
This source provides concrete methodology for adversarial probe with PGD attacks with specific perturbation bounds (ε = 0.1 for Fashion-MNIST, ε = 8/255 for CIFAR10). The dramatic robustness improvements achieved through fine-tune demonstrate that adversarial evaluation can not only detect knowledge loss but also guide remediation strategies.

---

## Source 4: Semantic Retention and Extreme Compression in LLMs (2025)

**Citation:** arXiv:2505.07289v1 (2025). "Semantic Retention and Extreme Compression in LLMs: Can We Have Both?"
**URL:** https://arxiv.org/html/2505.07289v1

### Summary
This recent 2025 research introduces novel metrics for quantification of the trade-off between compression and semantic preservation in LLMs. The study reveals that traditional perplexity metrics fail to capture subtle degradation in knowledge-intensive tasks.

### Key Quotes

1. "Perplexity often fails to capture subtle degradation in knowledge-intensive tasks even when compressed models maintain similar perplexity scores."

2. "Two formulations exist—Sr₁ weights each task equally, while Sr₂ provides weighted aggregation by original performance magnitude, with Sr₂ adopted as the primary metric."

3. "4-bit quantization maintains better performance than 75% prune despite both achieve 75% theoretical compression rate."

4. "To combine 25% prune with 4-bit quantization significantly outperforms pure 3-bit quantization, which achieves around 20% higher semantic retention."

5. "Mathematical reason and instruction-follow demonstrate heightened vulnerability to compression compared to other capability domains."

### Relevance to Research Question
This source directly addresses the research question because it introduces the Semantic Retention Compression Rate (SrCr) metric specifically designed to detect subtle knowledge loss. The result that different capabilities (reason, instruction-follow) show differential vulnerability demonstrates the need for task-specific evaluation rather than monolithic metrics.

---

## Source 5: Semantic Drift Detection in Text Generation (2024)

**Citation:** arXiv:2404.05411v1 (2024). "Know When To Stop: A Study of Semantic Drift in Text Generation"
**URL:** https://arxiv.org/html/2404.05411v1

### Summary
This research introduces a Semantic Drift (SD) Score to quantify how generated text diverges from correct information over time. The study uses the FActScore task to evaluate biographical text generation, which reveals that even large models drift to incorrect facts early in generation.

### Key Quotes

1. "Semantic drift describes the phenomenon wherein generated text diverges from the subject matter designated by the prompt, which results in a growth in deterioration of relevance, coherence, or truthfulness."

2. "We measure the degree of separation between correct and incorrect facts in a paragraph."

3. "LLaMa2-70B have high semantic drift score: they tend to generate correct facts first, then 'drift away' from the topic and generate incorrect facts later."

4. "Fewer than 7% of paragraphs maintained ten correct facts before they encounter errors."

5. "The resample-then-rerank method proved most effective, improved factual accuracy by approximately 10% without reduction of text length."

### Relevance to Research Question
This source provides a concrete methodology for detection of semantic drift—a form of knowledge loss that manifests over sequential generation. The evaluation approach with atomic fact extraction and truthfulness score offers a fine-grained method for identification of boundary conditions where model knowledge degrades.

---

## Source 6: LLM Boundary Test Explanations (2026)

**Citation:** arXiv:2601.22791 (2026). "Understand on the Edge: LLM-generated Boundary Test Explanations"
**URL:** https://arxiv.org/html/2601.22791

### Summary
This very recent 2026 study examines LLM-generated explanations for boundary value tests with 27 software professionals. The research establishes evaluation criteria for boundary test quality and reveals critical requirements for detection of edge cases.

### Key Quotes

1. "Boundary value analysis and tests (BVT) is fundamental in software quality assurance because faults tend to cluster at input extremes."

2. "Software professionals rated GPT-4.1 explanations for boundary pairs on clarity, correctness, completeness and perceived usefulness."

3. "Overall, 63.5% of rates were positive (4-5 on the scale), while 17% were negative (1-2)."

4. "The ones that were fairly structured and very clearly described received higher rates from evaluators who seek predictable information sequences."

5. "References to back it up with requirements, standards, conventions were frequently requested to validate boundary logic independently."

### Relevance to Research Question
This source directly addresses boundary condition test methodology. The four-dimensional evaluation framework (clarity, correctness, completeness, usefulness) provides a structured approach for assessment of whether compressed models maintain proper boundary handle—a key area where subtle knowledge loss often manifests.

---

## Source 7: Adaptive Stress Test for Black-Box LLM Planners (2025)

**Citation:** arXiv:2505.05665v2 (2025). "Adaptive Stress Test of Black-Box LLM Planners"
**URL:** https://arxiv.org/html/2505.05665v2

### Summary
This research introduces an innovative framework that combines Adaptive Stress Test (AST) with Monte-Carlo Tree Search to systematically characterize LLM failures in safety-critical decision contexts. The methodology identifies edge cases through intelligent adversarial search rather than exhaustive tests.

### Key Quotes

1. "The researchers formulate the search for problematic prompt perturbations as an adversarial search problem, where they intelligently explore the space of possible input modifications rather than they exhaustively test all combinations."

2. "LLMs have a tendency to hallucinate at test-time, and their system finds specific prompt variations that trigger these failures."

3. "Shannon Entropy - Measures inconsistency in model outputs across multiple samples."

4. "Capable models can be further steered at runtime to act even more or less desirably, which suggests offline analysis enables real-time intervention strategies."

5. "Tests encompass three distinct environments: Autonomous drive, Robot crowd navigation, and Lunar lander."

### Relevance to Research Question
This source provides a sophisticated methodology for edge case detection through adversarial search. The three undesirability metrics (Shannon Entropy, Action Diversity, Negative Reward) offer concrete approaches for detection of knowledge loss in compressed models deployed as agents, particularly relevant for evaluation of behavior preservation under compression.

---

## Source 8: Adversarial Robustness vs. Model Compression (2019)

**Citation:** ICCV 2019. "Adversarial Robustness vs. Model Compression, or Both?"
**URL:** https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.pdf

### Summary
This seminal 2019 paper addresses the tension between adversarial robustness and model compression, which proposes joint optimization techniques. The research reveals that naive compression strategies fail to maintain defensive properties against adversarial perturbations.

### Key Quotes

1. "Min-max robust optimization based adversarial train can provide a notion of security against adversarial attacks."

2. "Weight prune is necessary to reduce network size under adversarial constraints."

3. "Train of a small model from scratch even with inherited initialization from the large model cannot achieve neither adversarial robustness nor high standard accuracy."

4. "The framework combines concurrent adversarial train with weight prune."

5. "Practitioners can deploy efficient models without sacrifice of their robustness to adversarial attacks."

### Relevance to Research Question
This foundational source establishes that adversarial robustness tests are essential for comprehensive evaluation of compressed models. The result that naive compression destroys robustness properties demonstrates that adversarial probe detects critical knowledge loss invisible to standard accuracy metrics.

---

## Source 9: Prune and Quantization Survey (2021)

**Citation:** arXiv:2101.09671 (2021). "Prune and Quantization for Deep Neural Network Acceleration: A Survey"
**URL:** https://arxiv.org/pdf/2101.09671

### Summary
This comprehensive survey examines prune and quantization techniques, provides detailed analysis of performance thresholds and degradation patterns. The research identifies critical sparsity levels where performance begins to degrade drastically.

### Key Quotes

1. "Performance with uniform quantization remains consistent down to 6-bit fixed-point quantization, but when precision is reduced to 4-bits, performance begins to degrade."

2. "As models are increasingly pruned at all explored precisions, performance is maintained until about 80% of the weights are pruned, after which performance begins to degrade drastically."

3. "Train can be conducted for a specified number of epochs with early stop criteria based on validation loss improvement."

4. "Networks with higher neural efficiency that maintain good accuracy performance are able to perform better when classes were partially randomized in train."

5. "Quantization generally outperforms prune for neural networks, and it is recommended to quantize neural networks when efficiency is required before prune is explored."

### Relevance to Research Question
This survey provides concrete thresholds (6-bit for quantization, 80% for prune) where degradation begins, offers practitioners specific checkpoints for evaluation. The discussion of early stop criteria and validation-based monitor provides practical methodologies for detection of subtle performance loss from compression.

---

## Source 10: What Do Compressed Deep Neural Networks Forget? (2019)

**Citation:** arXiv:1911.05248 (2019). "What Do Compressed Deep Neural Networks Forget?"
**URL:** https://arxiv.org/abs/1911.05248
**Additional reference:** https://wandb.ai/sauravmaheshkar/exploring-bias-and-compression/reports/What-Do-Compressed-Deep-Neural-Networks-Forget---Vmlldzo1NzA0NDY

### Summary
This influential 2019 paper introduces Prune Identified Exemplars (PIEs) to identify specific data points disproportionately affected by compression. The research reveals that compression impacts the long-tail distribution while it maintains aggregate performance.

### Key Quotes

1. "Models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset."

2. "Prune Identified Exemplars (PIEs) that are systematically more impacted by the introduction of sparsity."

3. "Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution."

4. "PIEs over-index on atypical or noisy images that are far more challenge for both humans and algorithms to classify."

5. "While these achieve high compression with little degradation to test set accuracy, this performance measure conceals significant differences in how different classes and images are impacted."

### Relevance to Research Question
This source directly answers the research question because it introduces a methodology (PIE analysis) specifically designed to detect subtle knowledge loss. The insight that aggregate metrics conceal significant disparate impacts on edge cases makes this approach essential for comprehensive evaluation of compressed models.

---

## Source 11: Out-of-Distribution Generalization Evaluation (2024)

**Citation:** arXiv:2403.01874v1 (2024). "A Survey on Evaluation of Out-of-Distribution Generalization"
**URL:** https://arxiv.org/html/2403.01874v1

### Summary
This survey categorizes OOD evaluation research into three paradigms: performance tests, performance prediction, and intrinsic property characterization. The research emphasizes that compression affects representation quality, particularly in deeper layers.

### Key Quotes

1. "Much less attention has been paid to the evaluation of OOD generalization, which aims not only to assess whether a model's OOD generalization capability is strong or not, but also to evaluate where a model generalizes well or poorly."

2. "In deep neural networks, deeper layers that compress representations hinder OOD performance."

3. "Linear probe ID accuracy monotonically increases as a function of layers, but OOD accuracy only increases until a compression 'tunnel' is reached and then decreases."

4. "Train with high-resolution datasets that contain many classes greatly reduces representation compression and improves transferability."

5. "Compression efficiency (measured as an approach to the rate-distortion frontier) correlates with out-of-distribution generalization."

### Relevance to Research Question
This source highlights OOD tests as a critical methodology for detection of knowledge loss, particularly the insight that compressed representations in deeper layers create a "tunnel" that degrades OOD performance. This provides a theoretical foundation for why compressed models might maintain in-distribution performance while they lose capability on edge cases and boundary conditions.

---

## Source 12: Knowledge Retention Metrics for Distillation (2024)

**Citation:** MDPI Applied Biosciences (2024). "A General-Purpose Knowledge Retention Metric for Evaluation of Distillation Models Across Architectures and Tasks"
**URL:** https://www.mdpi.com/2673-2688/6/10/273

### Summary
This research introduces the Knowledge Retention Score (KRS), a composite metric that integrates intermediate feature similarity and output agreement. The study addresses limitations of conventional evaluation metrics that focus only on task performance.

### Key Quotes

1. "Knowledge distillation (KD) compresses deep neural networks by transfer of knowledge from a high-capacity teacher model to a lightweight student model."

2. "Conventional evaluation metrics such as accuracy, mAP, IoU, or RMSE focus mainly on task performance and overlook how effectively the student internalizes the teacher's knowledge."

3. "The Knowledge Retention Score (KRS) is a composite metric that integrates intermediate feature similarity and output agreement into a single interpretable score to quantify knowledge retention."

4. "KRS provides a holistic view of how well knowledge is preserved in distillation."

5. "Evaluation frameworks remain inadequate, focus narrowly on efficiency or task-specific accuracy while they neglect essential LLM capabilities."

### Relevance to Research Question
This source provides a concrete methodology for evaluation of knowledge retention that goes beyond output accuracy. The KRS metric's integration of intermediate representations offers a way to detect subtle knowledge loss even when final outputs appear similar, which directly addresses the challenge of detection of hidden degradation in compressed models.

---

## Source 13: Task-Specific Capability Evaluation for Compressed LLMs (2024)

**Citation:** Multiple sources on Knowledge Distillation assessment
**URLs:**
- https://pmc.ncbi.nlm.nih.gov/articles/PMC12634706/
- https://link.springer.com/article/10.1007/s40747-025-02019-z

### Summary
This body of research examines task-specific capability preservation in compressed LLMs, introduces approaches like TASKD-LLM for selective knowledge distillation and Jensen-Shannon divergence for measurement of capability retention.

### Key Quotes

1. "Knowledge distillation methodologies in LLMs include rationale-based distillation, uncertainty-aware approaches, multi-teacher frameworks, dynamic and adaptive strategies, and task-specific distillation."

2. "Task-aware selective KD (TASKD-LLM), which transfers only task-relevant knowledge from the teacher to the student model."

3. "Since teacher models are typically pre-trained for versatility across a broad range of tasks, indiscriminate distillation can introduce unnecessary complexity when it distills for a specific downstream task."

4. "Jensen-Shannon (JS) Divergence serves as a robust measure for assessment of the preservation of both general and task-specific capabilities."

5. "Knowledge distillation has demonstrated high effectiveness across benchmarks such as GLUE, SuperGLUE, and MMLU, where student models often retain over 95% of the teacher model's performance."

### Relevance to Research Question
This source demonstrates that task-specific evaluation is essential for detection of capability loss in compressed models. The JS divergence approach provides a probabilistic method for measurement of whether compressed models maintain the same output distributions as uncompressed models across different capability domains.

---

## Source 14: Robustness Metrics with PGD and FGSM Attacks (2023)

**Citation:** MDPI Information (2023). "Evaluate the Robustness of Deep Learn Models against Adversarial Attacks: An Analysis with FGSM, PGD and CW"
**URL:** https://www.mdpi.com/2504-2289/8/1/8

### Summary
This research provides comprehensive analysis of adversarial attack methods (FGSM, PGD, CW) for evaluation of model robustness. The study introduces quantitative metrics which include success rate and Robustness Index.

### Key Quotes

1. "PGD performs multiple iterations of small perturbations on input data, limits the norm of perturbations at each iteration."

2. "FGSM calculates the gradient for input examples, takes the sign of that gradient, and multiplies it with a small real number, then adds the output to the input sample to generate the adversarial sample."

3. "Robustness is evaluated with metrics such as the success rate of adversary attacks, which measures the proportion of adversarial examples that manage to deceive the model."

4. "A quantitative metric called the Robustness Index (RI) is defined as the area under the accuracy–perturbation curve."

5. "PGD-based adversarial train improves the robustness of the model against several types of attack, such as BIM, FGSM, PGD, CW, and DeepFool attacks under both black-box and white-box contexts."

### Relevance to Research Question
This source provides concrete adversarial probe methodologies with specific attack parameters. The Robustness Index metric offers a quantitative approach to measurement of how compressed models degrade under increase of perturbation levels—a key methodology for detection of subtle knowledge loss through adversarial stress tests.

---

## Source 15: Differential Test for Compressed Models (2022)

**Citation:** ACM Transactions on Software Engineer and Methodology (2022). "Find Deviated Behaviors of the Compressed DNN Models for Image Classifications"
**URL:** https://dl.acm.org/doi/abs/10.1145/3583564

### Summary
This research introduces Dflare, a search-based black-box test technique to automatically find triggers that result in deviated behaviors in compressed models. The approach addresses the challenge that compressed model architectures and gradients are often unavailable.

### Key Quotes

1. "To help developers thoroughly understand the impact of model compression, it is essential to test these models to find those deviated behaviors before dissemination."

2. "This is a non-trivial task, because the architectures and gradients of compressed models are usually not available."

3. "Dflare is a search-based, black-box test technique to automatically find triggers that result in deviated behaviors in image classification tasks."

4. "The technique utilizes a novel fitness function to prioritize the mutated inputs that either cause large differences between two models' outputs or trigger previously unobserved models' probability vectors."

5. "Differential test is a useful approach that uses different implementations of the same algorithms and compares the results for software tests."

### Relevance to Research Question
This source provides a concrete methodology for differential tests—comparison of compressed vs. uncompressed model behaviors to detect deviations. The black-box approach is particularly valuable since it doesn't require access to model internals, which makes it applicable to proprietary or heavily optimized compressed models.

---

## Synthesis and Actionable Conclusions

### Key Result 1: Multi-Dimensional Evaluation is Essential

Standard accuracy metrics systematically fail to detect subtle knowledge loss in compressed models. The research consistently shows that models that maintain high aggregate performance may exhibit severe degradation on:

- **Edge cases and long-tail distributions** (PIEs - Prune Identified Exemplars)
- **Adversarial perturbations** (PGD, FGSM attacks reveal 60-87% accuracy drops)
- **Out-of-distribution data** (representation "tunnel" effect in compressed layers)
- **Boundary conditions** (cluster at input extremes where faults concentrate)
- **Task-specific capabilities** (reason and instruction-follow most vulnerable)

**Actionable Recommendation:** Implement evaluation suites that include at minimum: (1) standard benchmark accuracy, (2) adversarial robustness tests, (3) OOD generalization assessment, (4) edge case and boundary tests, and (5) task-specific capability preservation metrics.

### Key Result 2: Adversarial Probe Reveals Hidden Vulnerabilities

Adversarial robustness tests emerged as one of the most effective methodologies for detection of knowledge loss:

- **PGD attacks** with perturbation bounds (ε = 8/255 for CIFAR10, ε = 0.1 for Fashion-MNIST) provide lower bounds on model robustness
- **FGSM attacks** offer computationally efficient screens for vulnerabilities
- **Robustness Index (RI)** quantifies degradation as area under accuracy-perturbation curve
- Knowledge-distilled models show **most vulnerability**, followed by pruned, then quantized models

**Actionable Recommendation:** Establish adversarial robustness benchmarks before compression, then evaluate compressed models against PGD-ℓ∞, FGSM, and CW attacks. Set minimum Robustness Index thresholds based on deployment requirements. Consider adversarial fine-tune (3 epochs can recover 95% of robust performance) for models that fail thresholds.

### Key Result 3: Semantic and Knowledge Retention Metrics Detect Subtle Degradation

Novel metrics specifically designed for compression evaluation outperform traditional measures:

- **Semantic Retention Rate (Sr)** captures capability preservation across task sets
- **Semantic Retention Compression Rate (SrCr)** unifies compression-performance trade-off into single metric
- **Knowledge Retention Score (KRS)** integrates intermediate feature similarity with output agreement
- **Semantic Drift Score** quantifies degradation in sequential generation
- **Jensen-Shannon Divergence** measures distribution shifts between compressed and uncompressed outputs

**Actionable Recommendation:** Adopt composite metrics that integrate multiple evaluation dimensions. Prioritize SrCr or KRS based on whether you compress LLMs or traditional DNNs. Monitor semantic drift over generation length for autoregressive models.

### Key Result 4: Critical Thresholds Predict Catastrophic Failure

Research identifies specific compression levels where degradation accelerates:

- **Quantization:** Performance stable to 6-bit, degrades at 4-bit
- **Prune:** Stable to 75-80% sparsity, catastrophic beyond 90%
- **Combined methods:** 25% prune + 4-bit quantization outperforms pure 3-bit by ~20%
- **Perplexity:** Often is misleading—maintains similarity even with knowledge-intensive task degradation

**Actionable Recommendation:** Establish staged evaluation with checkpoints at known thresholds (6-bit, 4-bit for quantization; 50%, 75%, 90% for prune). Prioritize combined approaches over extreme single-method compression. Never rely on perplexity alone for LLM evaluation.

### Key Result 5: Differential Tests and PIE Analysis Identify Deviated Behaviors

Comparison of compressed vs. uncompressed model behaviors reveals subtle differences:

- **Dflare methodology** uses search-based mutation to find triggers that cause deviated outputs
- **PIE analysis** identifies specific examples disproportionately affected by compression
- **Long-tail data** shows greatest vulnerability—atypical and noisy examples affected first
- **Boundary value tests** reveal divergence at input extremes
- **Confusion matrix analysis** identifies class-specific degradation patterns

**Actionable Recommendation:** Implement differential test pipelines that compare outputs between compressed and original models. Systematically identify PIEs (inputs where behavior diverges) and analyze their characteristics. Use confusion matrices to detect class-specific degradation even when overall accuracy appears stable.

### Key Result 6: Task-Specific and Domain-Specific Evaluation Required

Different capabilities show differential vulnerability to compression:

- **Most vulnerable:** Mathematical reason, instruction-follow
- **Moderately vulnerable:** Knowledge-intensive tasks, factual recall
- **Most robust:** Simple classification, pattern recognition
- **Domain variation:** Medical image models more robust than general vision
- **Hardware interaction:** Edge deployment may expose latency issues that prune doesn't address

**Actionable Recommendation:** Design evaluation suites that match deployment domain. For LLMs, include reason benchmarks (GSM8K, MATH), instruction-follow (IFEval), knowledge probe (MMLU), and factual accuracy (TruthfulQA). For vision models, test across complexity levels (simple vs. complex scenes, common vs. rare objects).

### Key Result 7: Stress Tests and Adaptive Search Find Edge Cases

Systematic stress test methodologies outperform random sample:

- **Adaptive Stress Test (AST)** with MCTS intelligently explores failure modes
- **Three undesirability metrics:** Shannon entropy (inconsistency), action diversity (instability), negative reward (failure)
- **REST methodology** concatenates multiple problems to increase difficulty
- **Boundary value analysis** systematically tests input extremes (empty arrays, single elements, sorted data)
- **Semantic drift detection** monitors generation quality over sequence length

**Actionable Recommendation:** Implement AST or similar adversarial search methods to systematically discover failure modes rather than rely on random test cases. For LLMs, test with concatenated problems and monitor semantic drift over generation. For structured inputs, apply systematic boundary value analysis at all parameter limits.

### Key Result 8: Evaluation Must Include Capability Preservation

Beyond accuracy, compressed models must maintain broader capabilities:

- **Plan and tool use:** 4-bit quantization shows only 1-3% drop, but 10-15% drop in real-world tasks
- **Agentic behaviors:** Agent Compression Benchmark tests plan, control flow, multi-step reason
- **Factual accuracy:** FActScore methodology extracts atomic facts and validates truthfulness
- **Intermediate representations:** Feature similarity metrics detect internal knowledge degradation
- **Cross-task adaptability:** Transfer learn performance indicates knowledge generalization

**Actionable Recommendation:** For agentic LLMs, implement multi-task benchmarks that test plan (ALFWorld), tool use (API usage), and control flow (WebShop). For knowledge models, validate factual accuracy through atomic fact extraction. Always evaluate on held-out tasks to assess generalization preservation.

---

## Recommended Evaluation Framework

Based on the synthesized research, a comprehensive evaluation methodology for detection of subtle knowledge loss from compression should include:

### Tier 1: Standard Performance Baselines
- Benchmark accuracy on standard test sets (ImageNet, MMLU, etc.)
- Inference latency and throughput
- Model size and memory footprint
- Perplexity (for language models, with caveats)

### Tier 2: Adversarial and Robustness Tests
- PGD-ℓ∞ attacks (ε = 8/255 for vision, appropriate bounds for domain)
- FGSM attacks for efficient screen
- Robustness Index calculation across perturbation levels
- C&W attacks for comprehensive assessment

### Tier 3: Edge Case and Boundary Analysis
- Prune Identified Exemplars (PIE) analysis
- Boundary value tests at parameter extremes
- Long-tail distribution performance
- Confusion matrix analysis for class-specific degradation
- Adaptive Stress Test with MCTS

### Tier 4: Semantic and Knowledge Retention
- Semantic Retention Rate (Sr) across task sets
- Knowledge Retention Score (KRS) which includes intermediate features
- Jensen-Shannon Divergence for distribution shifts
- Semantic Drift Score for sequential generation
- Task-specific capability metrics (reason, instruction-follow, factual recall)

### Tier 5: Out-of-Distribution Generalization
- OOD dataset performance
- Cross-domain transfer evaluation
- Layer-wise representation analysis (tunnel detection)
- Distributional shift robustness

### Tier 6: Differential and Comparative Analysis
- Dflare-style differential tests
- Output distribution comparison (compressed vs. original)
- Behavioral deviation detection
- Feature space similarity analysis

### Tier 7: Domain-Specific Evaluation
- Agentic capabilities (plan, tool use) for LLM agents
- Medical and safety-critical metrics for specialized domains
- Real-world task performance vs. synthetic benchmarks
- Hardware-specific deployment metrics

---

## Critical Gaps and Future Directions

Despite extensive research, several gaps remain:

1. **Standardized benchmarks:** No unified framework exists for compression evaluation across model types
2. **Composite metrics:** Need for single interpretable scores that combine multiple dimensions
3. **Predictive evaluation:** Methods to predict post-compression performance without full evaluation
4. **Real-world validation:** Gap between benchmark performance and deployed behavior
5. **Interactive evaluation:** Limited work on multi-turn dialogue and agentic task degradation

Future research should focus on development of unified evaluation frameworks that balance comprehensiveness with computational efficiency, while they provide interpretable insights into specific degradation patterns that guide remediation strategies.

---

## Sources

- [A survey of model compression techniques: past, present, and future](https://www.frontiersin.org/journals/robotics-and-ai/articles/10.3389/frobt.2025.1518965/full)
- [Model Compression vs. Adversarial Robustness: An Empirical Study on Language Models for Code](https://arxiv.org/abs/2508.03949)
- [Comprehensive Study on Performance Evaluation and Optimization of Model Compression](https://arxiv.org/html/2407.15904)
- [Adversarial Fine-tune of Compressed Neural Networks](https://arxiv.org/html/2403.09441)
- [Semantic Retention and Extreme Compression in LLMs](https://arxiv.org/html/2505.07289v1)
- [Know When To Stop: A Study of Semantic Drift in Text Generation](https://arxiv.org/html/2404.05411v1)
- [Understand on the Edge: LLM-generated Boundary Test Explanations](https://arxiv.org/html/2601.22791)
- [Adaptive Stress Test of Black-Box LLM Planners](https://arxiv.org/html/2505.05665v2)
- [Adversarial Robustness vs. Model Compression, or Both?](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ye_Adversarial_Robustness_vs._Model_Compression_or_Both_ICCV_2019_paper.pdf)
- [Prune and Quantization for Deep Neural Network Acceleration: A Survey](https://arxiv.org/pdf/2101.09671)
- [What Do Compressed Deep Neural Networks Forget?](https://arxiv.org/abs/1911.05248)
- [A Survey on Evaluation of Out-of-Distribution Generalization](https://arxiv.org/html/2403.01874v1)
- [A General-Purpose Knowledge Retention Metric for Evaluation of Distillation Models](https://www.mdpi.com/2673-2688/6/10/273)
- [Knowledge distillation and dataset distillation of large language models](https://pmc.ncbi.nlm.nih.gov/articles/PMC12634706/)
- [Evaluate the Robustness of Deep Learn Models against Adversarial Attacks](https://www.mdpi.com/2504-2289/8/1/8)
- [Find Deviated Behaviors of the Compressed DNN Models](https://dl.acm.org/doi/abs/10.1145/3583564)
