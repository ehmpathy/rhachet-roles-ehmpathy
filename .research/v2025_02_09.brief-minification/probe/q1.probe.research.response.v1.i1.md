# Research Question 1: What is the Minimum Viable Semantic Unit (MVSU) for LLM Comprehension?

## Question Restatement

This research examines the fundamental question: What constitutes the minimum viable semantic unit (MVSU) for LLM comprehension? Specifically, we examine whether formal linguistic theories (such as minimal recursion semantics, predicate-argument structure, event semantics, and compositionality principles) provide frameworks to define when the system preserves versus loses semantic information in compression operations like telegraphic reduction.

---

## Source 1: Minimal Recursion Semantics (MRS) - Copestake et al.

**Full Citation:**
Copestake, Ann, Dan Flickinger, Carl Pollard, and Ivan A. Sag. "Minimal Recursion Semantics: An Introduction." Research on Language and Computation (2005).
URL: https://link.springer.com/article/10.1007/s11168-006-6327-9
PDF: https://www.cl.cam.ac.uk/~aac10/papers/mrs.pdf

### Summary
Minimal Recursion Semantics (MRS) represents a foundational framework in computational semantics that provides a syntactically flat representation of the content while it preserves scope information. MRS works for both parser and generator tasks and allows implementation in typed feature structure formalisms. The framework aims to achieve descriptively adequate semantic representations with minimal recursive structure, with a focus on elementary predications that the system never embeds within one another.

### Key Quotes

1. "A descriptively adequate representation with a nonrecursive structure may be achieved."

2. "MRS is a syntactically 'flat' representation, since the elementary predications are never embedded within one another."

3. "Unlike earlier approaches to flat semantics, MRS includes a treatment of scope which is straightforwardly related to conventional logical representations."

4. "A flat representation that preserves sufficient information about scope to be able to construct all and only the possible readings for the sentence is a central goal of the framework."

5. "MRS enables a simple formulation of the grammatical constraints on lexical and phrasal semantics, including the principles of semantic composition."

### Analysis
MRS provides crucial insights into minimal semantic units as it demonstrates that semantic representations can be "flattened" without loss of content preservation. The elementary predications in MRS represent one candidate for MVSUs—they are atomic semantic units that cannot be further decomposed while they maintain their semantic contribution. The framework's focus on how it preserves scope information even in flat representations indicates that certain structural relationships (like quantifier scope) must be maintained in minimal representations. For LLM comprehension and telegraphic compression, this shows that while surface syntax can be dramatically simplified, the system must preserve or be able to reconstruct the underlying predicate-argument structure and scope relationships.

---

## Source 2: Principle of Compositionality - Stanford Encyclopedia of Philosophy

**Full Citation:**
Szabó, Zoltán Gendler. "Compositionality." Stanford Encyclopedia of Philosophy (2023).
URL: https://plato.stanford.edu/entries/compositionality/

### Summary
The principle of compositionality, often attributed to Gottlob Frege, states that the content of a complex expression depends on the semantic values of its constituent expressions and the rules the system uses to combine them. This principle is fundamental to formal semantics and provides a framework for when compositional operations preserve versus break down the signal (as in idioms, context-dependent expressions, or non-compositional constructions).

### Key Quotes

1. "The meaning of a complex expression is determined by its structure and the meanings of its constituents."

2. "Once the meanings of the constituents and their mode of combination are fixed, the meaning of the whole is fixed."

3. "On the traditional view...this ensures systematic preservation of meaning through compositional rules."

4. "Among its most challenging problems there are the issues of contextuality, the non-compositionality of idiomatic expressions, and the non-compositionality of quotations."

5. "You cannot infer sarcasm purely on the basis of words and their composition, yet a phrase used sarcastically means something completely different from the same phrase uttered straightforwardly."

### Analysis
Compositionality provides a formal criterion for when the system preserves semantic value: if constituent values and combination rules remain constant, the overall semantic value remains constant. This has direct implications for how we determine MVSUs in telegraphic compression. Elements that the system can remove while it preserves constituent values and their structural relationships (like determiners, auxiliaries, and copulas) don't violate compositionality if the LLM can reconstruct them from context. However, the challenges to compositionality (idioms, context-sensitivity) reveal that MVSUs must sometimes include larger units than individual words—some multi-word expressions function as atomic semantic units that the system cannot decompose without loss of semantic value.

---

## Source 3: Compositionality in Language - Internet Encyclopedia of Philosophy

**Full Citation:**
Pagin, Peter, and Dag Westerståhl. "Compositionality in Language." Internet Encyclopedia of Philosophy.
URL: https://iep.utm.edu/compositionality-in-language/

### Summary
This comprehensive article examines compositionality from multiple philosophical and linguistic perspectives, and the authors distinguish between different types of compositionality (local vs. global, functional vs. mereological). The text explores the boundary conditions where compositionality fails. It provides detailed analysis of how the system maintains semantic value in compositional systems and identifies specific counterexamples that challenge compositional theories.

### Key Quotes

1. "A symbolic system demonstrates compositionality when the meaning of every complex expression E in that system depends on, and depends only on, (i) E's syntactic structure and (ii) the meanings of E's simple parts."

2. "If a language is compositional, then the meaning of a sentence S in that language cannot depend directly on the context that sentence is used in or the intentions of the speaker who uses it."

3. "If you take any expression E of L, and any morpheme M that occurs in E, and you replace M with a different morpheme M* of L that has the same meaning as M, then the result will have the same meaning as M."

4. "Idioms like 'kick the bucket' where literal component meanings don't determine overall meaning."

5. "The definition permits basic morphemes to have context-dependent meanings; compositionality requires only that complex expressions depend solely on their syntax and their components' meanings."

### Analysis
This source clarifies that MVSUs are not necessarily morphemes or words, but rather the level at which semantic decomposition stops to be productive. The key insight for telegraphic compression is that compositionality permits simple expressions to have context-dependent values while it requires complex expressions to be compositionally determined. This shows that content words (nouns, verbs, adjectives) can serve as MVSUs because they carry context-independent core semantic values, while function words contribute primarily to the compositional structure and can be omitted if that structure is recoverable from context.

---

## Source 4: Discourse Representation Theory - Stanford Encyclopedia of Philosophy

**Full Citation:**
Kamp, Hans, and Uwe Reyle. "Discourse Representation Theory." Stanford Encyclopedia of Philosophy.
URL: https://plato.stanford.edu/entries/discourse-representation-theory/

### Summary
Discourse Representation Theory (DRT), created by Hans Kamp in 1981, provides a framework to explore semantic value through formal semantics that explicitly includes a level of mental representations called Discourse Representation Structures (DRSs). These structures consist of discourse referents (which represent entities) and discourse conditions (which represent information about those entities). DRT's distinctive feature is its ability to handle semantic value across sentence boundaries through merge operations and accessibility relations.

### Key Quotes

1. "The basic meaning-carrying units in DRT are Discourse Representation Structures (DRSs), which are recursive formal meaning structures that have a model-theoretic interpretation and can be translated into first-order logic."

2. "Basic DRSs consist of discourse referents (e.g., x,y) representing entities in the discourse and discourse conditions (e.g., man(x), magazine(y)) representing information about discourse referents."

3. "One of the main differences between DRT-style approaches and traditional Montagovian approaches is that DRT includes a level of abstract mental representations (discourse representation structures, DRS) within its formalism, which gives it an intrinsic ability to handle meaning across sentence boundaries."

4. "The merge of two DRSs K and K' is defined as their pointwise union."

5. "A DRS is true iff there is an embedding function f such that Dom(f) = U_K and f verifies K in M."

### Analysis
DRT provides a crucial insight for MVSUs: semantic units must be defined not just at the sentence level but across discourse. The distinction between discourse referents and discourse conditions shows that MVSUs in telegraphic compression should preserve both entity introductions and predications about those entities. The merge operation demonstrates how semantic value can be accumulated incrementally, which aligns with how LLMs process compressed text—they can reconstruct omitted elements as they maintain discourse referents across compressed sentences. This shows that telegraphic compression must maintain discourse coherence as it preserves referent continuity even when it removes grammatical scaffold.

---

## Source 5: Semantic Role Labeling - Stanford NLP

**Full Citation:**
Jurafsky, Dan, and James H. Martin. "Semantic Role Labeling." Speech and Language Processing (Chapter 21).
URL: https://web.stanford.edu/~jurafsky/slp3/21.pdf

### Summary
Semantic Role Labeling (SRL) represents a "shallow" semantic parser approach that identifies predicate-argument structures within sentences as it assigns semantic roles (Agent, Patient, Theme, Goal, etc.) to constituents. SRL provides a practical framework to identify minimal semantic representations that capture "who did what to whom, when, where, and how" without deep logical analysis.

### Key Quotes

1. "Semantic role labeling (SRL) is a central natural language processing (NLP) task aiming to understand the semantic roles within texts, facilitating a wide range of downstream applications."

2. "The formalization of SRL involves specifying who did what to whom, when, where, and how, by detecting and labeling events, participants, and their roles in those events."

3. "SRL is also referred to as shallow semantic parsing, where labels are assigned to indicate roles like agent, goal, or result."

4. "SRL maintains a direct connection to surface syntax by explicitly labeling semantic roles on text spans or syntactic heads."

5. "SRL represents a lighter-weight semantic representation compared to more complex approaches like AMR."

### Analysis
SRL provides a practical framework to identify MVSUs through predicate-argument structure. The "shallow" nature of SRL shows that deep logical representations are not necessary for semantic comprehension—to identify core participants and their roles in events provides sufficient semantic information for comprehension. For telegraphic compression, this shows that the system can preserve predicates (typically verbs) and their arguments (typically nouns) while it omits role-marker function words (prepositions, case markers). The LLM can reconstruct the specific grammatical code of roles from world knowledge and context, as long as the core predicate-argument structure remains explicit.

---

## Source 6: Predicate-Argument Structure in Linguistics

**Full Citation:**
Multiple academic sources on predicate-argument structure.
Primary URL: https://www.departments.bucknell.edu/linguistics/lectures/10lect09.html
Additional: https://mynlp.is.s.u-tokyo.ac.jp/enju/enju-manual/pas.html

### Summary
Predicate-argument structure represents the fundamental organization of semantic value in sentences, where predicates (typically verbs) specify relationships or states, and arguments (typically nouns) represent the participants in those relationships. Most predicates take one, two, or three arguments, and the system distinguishes these arguments from optional adjuncts.

### Key Quotes

1. "A predicate and its arguments form a predicate–argument structure."

2. "In linguistics, an argument is an expression that helps complete the meaning of a predicate, the latter referring in this context to a main verb and its auxiliaries."

3. "A predicate specifies a relationship between objects (concrete or abstract) or a state that characterizes an object, for example [BIT(BOY, DOG)] 'the boy bit the dog'."

4. "Most predicates take one, two, or three arguments."

5. "Arguments must be distinguished from adjuncts. While a predicate needs its arguments to complete its meaning, the adjuncts that appear with a predicate are optional; they are not necessary to complete the meaning of the predicate."

### Analysis
The argument-adjunct distinction provides a clear criterion for MVSUs: arguments are necessary for semantic completeness while adjuncts are optional. This has direct implications for telegraphic compression—the system must preserve arguments while it can potentially omit adjuncts. However, the analysis is complicated by the fact that grammatical code of arguments often uses function words (case markers, prepositions) that are targets for compression. The solution is that argument role relationships can be preserved through word order and semantic plausibility even when the system removes explicit role markers, provided the predicate and arguments themselves remain present and identifiable.

---

## Source 7: Abstract Meaning Representation (AMR)

**Full Citation:**
Banarescu, Laura, et al. "Abstract Meaning Representation for Sembanking." Linguistic Annotation Workshop (2013).
URL: https://en.wikipedia.org/wiki/Abstract_Meaning_Representation
Additional: https://arxiv.org/html/2505.03229v1

### Summary
Abstract Meaning Representation (AMR) is a semantic formalism that represents the semantic value of sentences as rooted, directed, acyclic graphs (DAGs) where nodes represent concepts and labeled edges represent semantic relationships. AMR abstracts away from syntactic variation, and it aims to assign the same representation to sentences with similar semantic values regardless of their surface form.

### Key Quotes

1. "Abstract Meaning Representation (AMR) is a semantic representation language with graphs that are rooted, labeled, directed, acyclic graphs (DAGs), comprising whole sentences."

2. "They are intended to abstract away from syntactic representations, in the sense that sentences which are similar in meaning should be assigned the same AMR, even if they are not identically worded."

3. "In this formalism, sentences are represented by rooted, directed acyclic graphs where vertices represent concepts and labelled arcs are used to represent semantic relationships between them."

4. "AMRs are a graphical representation of some text, where the nodes are concepts and the edges denote the relationships between concepts. Note that the nodes in the graph are not necessarily words in the sentence, but rather high-level abstractions of the concepts within that sentence."

5. "The property that makes AMR a graph instead of a tree is that AMR allows reentrancy, meaning that same concept can participate in multiple relations."

### Analysis
AMR demonstrates that MVSUs are better understood as concepts and relations rather than words or syntactic constituents. The fact that AMR abstracts away from surface form to achieve equivalence across paraphrases shows that telegraphic compression can remove substantial syntactic material while it preserves the underlying conceptual graph. For LLM comprehension, this shows that semantic value is represented at a level above surface tokens—the LLM constructs a conceptual representation where the same semantic content can be recovered from multiple surface realizations. However, the reliance on PropBank concepts shows that predicate-argument structure remains fundamental even in abstract representations.

---

## Source 8: Construction Grammar and Form-Meaning Pairs

**Full Citation:**
Goldberg, Adele E. "Construction Grammar." Multiple sources.
URL: https://en.wikipedia.org/wiki/Construction_grammar
Additional: https://www.academia.edu/24871161/Construction_Grammars

### Summary
Construction Grammar posits that linguistic knowledge consists of learned pairings of form and semantic value (constructions) at all levels of abstraction, from morphemes to abstract grammatical patterns. Constructions are the fundamental blocks of language, and any linguistic pattern is considered a construction if some aspect of its form or semantic value cannot be predicted from its components or other known constructions.

### Key Quotes

1. "In construction grammar, a grammatical construction, regardless of its formal or semantic complexity and make up, is a pairing of form and meaning."

2. "Construction grammarians argue that all pairings of form and meaning are constructions, including phrase structures, idioms, words and even morphemes."

3. "Constructions include words (aardvark, avocado), morphemes (anti-, -ing), fixed expressions and idioms (by and large, jog X's memory), and abstract grammatical rules such as the passive voice."

4. "Any linguistic pattern is considered to be a construction as long as some aspect of its form or its meaning cannot be predicted from its component parts, or from other constructions that are recognized to exist."

5. "The semantic meaning of a grammatical construction is made up of conceptual structures postulated in cognitive semantics: image-schemas, frames, conceptual metaphors, conceptual metonymies, prototypes of various kinds, mental spaces, and bindings across these."

### Analysis
Construction Grammar challenges the notion of atomic MVSUs as it argues that semantic units exist at multiple levels simultaneously. Some constructions (like idioms) function as unanalyzable semantic wholes despite syntactic complexity. This has important implications for telegraphic compression: certain multi-word expressions must be preserved intact because their semantic value is construction-specific rather than compositionally derived. However, many grammatical constructions (like passive voice, ditransitive patterns) contribute primarily structural rather than lexical semantic value, which shows they are candidates for compression. The key insight is that MVSUs are not uniform—they vary in size based on whether semantic value is lexically specified or constructionally determined.

---

## Source 9: Telegraphic Semantic Compression (TSC)

**Full Citation:**
Bispo, Nuno. "Telegraphic Semantic Compression (TSC) - A Semantic Compression Method for LLM Contexts." Django Unleashed (November 2025).
URL: https://medium.com/django-unleashed/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts-45de3ebbae96
Additional: https://developer-service.blog/telegraphic-semantic-compression-tsc-a-semantic-compression-method-for-llm-contexts/

### Summary
Telegraphic Semantic Compression (TSC) represents a practical application of semantic compression specifically designed for LLM contexts. TSC removes predictable grammatical elements (articles, prepositions, auxiliaries, pronouns, conjunctions) while it preserves content words (nouns, verbs, adjectives, adverbs) that carry core semantic information. The technique is based on the principle that LLMs can reconstruct omitted grammatical scaffold from context, which allows more efficient use of limited context windows.

### Key Quotes

1. "Telegraphic Semantic Compression (TSC) transforms verbose natural language into dense semantic packets, stripping away grammatical scaffolding while preserving the core informational payload."

2. "It removes what an LLM can reliably predict; grammar, filler words, and structural glue; while preserving the information it cannot reconstruct from context."

3. "TSC removes predictable grammatical elements (determiners, prepositions, auxiliaries, pronouns, conjunctions, particles) and filters out low-information words while preserving content words (nouns, verbs, adjectives, adverbs) that carry semantic meaning."

4. "The result is that more of your context window is spent on meaningful, high-entropy information, instead of predictable linguistic scaffolding."

5. "TSC is not suitable for text where nuance, style, or tone conveys meaning—poetry (rhythm, rhyme, and literary devices are lost), jokes or humor (timing, wordplay, and puns are removed), emotional or persuasive writing (tone, emphasis, and subtlety disappear)."

### Analysis
TSC provides empirical evidence for what constitutes MVSUs in LLM comprehension: content words carry "high-entropy information" that the system cannot reliably predict from context. The explicit exclusion of function words and grammatical elements shows these contribute primarily to syntactic structure rather than semantic content. This aligns with linguistic theories that distinguish content from function words, but TSC adds the computational insight that LLMs possess sufficient world knowledge and contextual capacity to reconstruct grammatical relationships from content words alone. The limitations of TSC (unsuitable for poetry, humor, emotional text) reveal that pragmatic and stylistic semantic value requires surface form preservation—these aspects of semantic value are not recoverable from content words alone.

---

## Source 10: Function Words vs. Content Words in Semantic Processing

**Full Citation:**
Multiple neurolinguistic and computational sources.
Primary: "Lexical-Semantic Content, Not Syntactic Structure, Is the Main Contributor to ANN-Brain Similarity"
URL: https://direct.mit.edu/nol/article/5/1/7/116784/Lexical-Semantic-Content-Not-Syntactic-Structure

### Summary
Research demonstrates that lexical-semantic content (carried primarily by content words) rather than syntactic structure (conveyed through word order and function words) drives both neural language process in humans and representation learners in artificial neural networks. Function words contribute minimal lexical semantic value and primarily encode logical relationships between content words.

### Key Quotes

1. "Lexical-semantic content of sentences (largely carried by content words) rather than the sentence's syntactic form (conveyed via word order or function words) is primarily responsible for neural language processing."

2. "Research in formal semantics has traditionally posited a categorical difference between the semantics of content (open-class/lexical) words and function (closed-class/logical) words, whereby function words, unlike content words, are assumed to carry little lexical meaning and primarily encode logical relationships between content words."

3. "Content words significantly exhibit higher semanticity than function words."

4. "Local word swap manipulations typically preserve local semantic dependency structure, as measured by statistical co-occurrence patterns among nearby words."

5. "Understanding the roles of content words (e.g., nouns, verbs, adjectives) and function words (e.g., prepositions, conjunctions) allows learners to identify the essential components of language to construct and comprehend sentences efficiently."

### Analysis
This neurolinguistic evidence supports the hypothesis that content words constitute MVSUs for both human and artificial language process. The result that content words carry semantic content while function words primarily encode syntactic relationships provides empirical support for telegraphic compression strategies. The preservation of "local semantic dependency structure" even when the system manipulates syntactic elements shows that semantic comprehension relies more on co-occurrence patterns among content words than on explicit grammatical markers. For LLMs, this shows that semantic representation is fundamentally lexical-semantic rather than syntactic, which makes function word removal a viable compression strategy as long as content word relationships can be recovered.

---

## Source 11: Frame Semantics and FrameNet

**Full Citation:**
Fillmore, Charles J., and Collin F. Baker. "Frame Semantics and FrameNet." Multiple sources.
URL: https://framenet.icsi.berkeley.edu/
Additional: https://en.wikipedia.org/wiki/FrameNet

### Summary
Frame Semantics, developed by Charles Fillmore, posits that word values are best understood in terms of semantic frames—structured knowledge representations that describe situations, events, or relationships along with their participants. FrameNet is a computational implementation of frame semantics that documents frames and their frame elements (semantic roles specific to each frame). Words evoke frames, and comprehension requires access to the entire frame structure.

### Key Quotes

1. "Frame semantics is a theory of linguistic meaning developed by Charles J. Fillmore that extends his earlier case grammar."

2. "One cannot understand the meaning of a single word without access to all the essential knowledge that relates to that word. For example, one would not be able to understand the word 'sell' without knowing anything about the situation of commercial transfer."

3. "A word activates, or evokes, a frame of semantic knowledge relating to the specific concept it refers to (or highlights, in frame semantic terminology)."

4. "Each frame has a number of core and non-core FEs which can be thought of as semantic roles. Core FEs are essential to the meaning of the frame while non-core FEs are generally descriptive (such as time, place, manner, etc.)."

5. "The project's fundamental notion is simple: most words' meanings may be best understood in terms of a semantic frame, which is a description of a certain kind of event, connection, or item and its actors."

### Analysis
Frame Semantics provides a holistic view of MVSUs that contrasts with atomistic approaches. Rather than individual predicates and arguments as minimal units, frames represent the minimal unit for complete comprehension—all core frame elements must be present or inferable for comprehension. This has nuanced implications for telegraphic compression: to remove non-core frame elements (time, place, manner) may be acceptable, but core frame elements must be preserved or strongly implied by context. The insight that words evoke entire frames shows that LLMs achieve comprehension as they activate frame knowledge from preserved content words, then they use that activated frame structure to reconstruct omitted elements. This explains why telegraphic compression works—core frame-evocation words trigger sufficient background knowledge to enable frame completion.

---

## Source 12: Neo-Davidsonian Event Semantics

**Full Citation:**
Multiple sources on Davidsonian and Neo-Davidsonian semantics.
Primary URLs: https://www.coli.uni-saarland.de/courses/incsem-12/neodavidsonian.pdf
Additional: https://d-nb.info/1161944222/34

### Summary
Neo-Davidsonian semantics extends Donald Davidson's approach as it treats events as fundamental semantic primitives, with verbs that take event arguments as their sole argument and participant roles expressed through separate thematic relation predicates. This decomposition allows for flexible modification and compositional semantics where each component can be independently quantified and modified.

### Key Quotes

1. "The Neo-Davidsonian approach has lately developed into a kind of standard for event semantics."

2. "Davidson's central claim is that events are spatiotemporal things, i.e., concrete particulars with a location in space and time."

3. "While Davidson introduced event arguments as an additional argument of some verbs, Neo-Davidsonian accounts take the event argument of a verbal predicate to be its only argument. The relation between events and their participants is accounted for by the use of thematic roles."

4. "Neo-Davidsonian approaches typically assume that it is not only action verbs that introduce Davidsonian event arguments, but also adjectives, nouns, and prepositions."

5. "Neo-Davidsonian Logical Forms contain thematic relations and the main mode of composition is conjunction."

### Analysis
Neo-Davidsonian semantics shows that events themselves constitute fundamental semantic units, with participant roles as separate semantic contributions. This decomposition has important implications for MVSUs: the event concept and core participants represent the minimal semantic content, while the specific grammatical code of thematic roles (through case markers, word order, or prepositions) is secondary. For telegraphic compression, this shows the system can preserve event predicates and participant nouns while it allows role-marker elements to be omitted or simplified. The LLM can reconstruct thematic role assignments through a combination of world knowledge (typical agent/patient assignments for specific events) and contextual inference. The conjunctive composition also explains why telegraphic compression works—to omit conjunctions doesn't eliminate the underlying conjunction operation, which can be reconstructed.

---

## Source 13: Lambda Calculus in Compositional Semantics

**Full Citation:**
Church, Alonzo. Lambda calculus in linguistic semantics (multiple sources).
Primary URL: https://mbrenndoerfer.com/writing/montague-semantics-formal-compositional-natural-language-understanding
Additional: https://plato.stanford.edu/entries/lambda-calculus/

### Summary
Lambda calculus provides the formal foundation for compositional semantics in linguistics, particularly through Montague Grammar. The lambda calculus enables precise formalization of how semantic values compose, with lambda-abstraction that allows functional application of semantic values. This mathematical framework ensures that semantic composition is systematic and recursive.

### Key Quotes

1. "Lambda calculus was introduced by mathematician Alonzo Church in the 1930s as part of an investigation into the foundations of mathematics."

2. "Thanks to Richard Montague and other linguists' applications in the semantics of natural language, the lambda calculus has begun to enjoy a respectable place in both linguistics and computer science."

3. "Compositionality holds that the meaning of a phrase depends on the meanings of its parts."

4. "Lambda-models provide a recursive, compositional semantics, where the syntactical operation of lambda-abstraction is interpreted by a corresponding semantic operation on applicative structures."

5. "Between 1970 and his untimely death in 1971, Montague developed Montague Grammar, publishing three seminal papers that laid out a formal framework for compositional natural language semantics."

### Analysis
Lambda calculus demonstrates that compositionality can be formally precise—semantic values combine through function application in a systematic way. For MVSUs, this shows that semantic units must be capable of participation in functional composition: they must be either arguments or functions that can combine through lambda-application. Content words typically denote entities (arguments) or relations (functions), while function words often contribute to the compositional machinery itself (function composition, type-shift). The implication for telegraphic compression is that content words carry the denotational content that feeds into compositional operations, while function words implement the compositional operations themselves. Since LLMs can implement compositional operations implicitly (through learned associations), explicit function words become redundant.

---

## Source 14: Minimum Description Length and Information Theory

**Full Citation:**
Grünwald, Peter D. "The Minimum Description Length Principle." MIT Press (2007).
URL: https://homepages.cwi.nl/~pdg/ftp/mdlintro.pdf
Additional: https://en.wikipedia.org/wiki/Minimum_description_length

### Summary
The Minimum Description Length (MDL) principle states that the best model or explanation for data is the one that permits the greatest compression of that data. MDL has roots in information theory and algorithmic information theory, and it formalizes Occam's razor through data compression. In linguistics, MDL has been applied to model selection and linguistic probe.

### Key Quotes

1. "Minimum Description Length (MDL) is a model selection principle where the shortest description of the data is the best model."

2. "The principle holds that the best explanation, given a limited set of observed data, is the one that permits the greatest compression of the data."

3. "MDL has its origins mostly in information theory and has been further developed within the general fields of statistics, theoretical computer science and machine learning."

4. "With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data."

5. "Within Jorma Rissanen's theory of learning, models are statistical hypotheses and descriptions are defined as universal codes."

### Analysis
MDL provides an information-theoretic perspective on MVSUs: the minimal semantic unit is the unit that achieves optimal compression while it preserves information needed for reconstruction. This formalizes the intuition behind telegraphic compression—remove only information that is predictable (low information content) while the system preserves high-information elements. For LLM comprehension, this shows that content words represent high-information semantic units (they have high surprisal, low predictability) while function words represent low-information structural units (highly predictable from context). The MDL perspective explains why telegraphic compression doesn't harm comprehension: the removed elements had minimal information content (high compressibility), so their removal doesn't significantly increase description length of the semantic content.

---

## Source 15: Uniform Information Density Hypothesis

**Full Citation:**
Multiple sources on Uniform Information Density.
Primary: Crocker, Matthew W., and Vera Demberg. "Uniform Information Density."
URL: https://link.springer.com/article/10.1007/s13218-015-0391-y
Additional: https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00589/117221/

### Summary
The Uniform Information Density (UID) hypothesis proposes that speakers organize language to transmit information at a relatively constant rate, which avoids spikes in information density that might exceed processor capacity. Speakers make lexical and syntactic choices to distribute information more evenly across utterances. Recent research shows that language structure has evolved to support uniform information distribution.

### Key Quotes

1. "The Uniform Information Density (UID) hypothesis proposes that speakers communicate by transmitting information close to a constant rate."

2. "The UID hypothesis posits a communicative pressure to avoid spikes in information within an utterance, thereby keeping the information profile of an utterance relatively close to uniform over time."

3. "The UID hypothesis asserts that production processes will prefer encodings which distribute surprisal more uniformly across the signal—so as to avoid peaks that may exceed the comprehenders processing, or channel, capacity."

4. "The lexica of many human languages have adapted so as to encode words that are more predictable (on average) using shorter forms than less predictable words, with longer forms used for words that typically convey more information."

5. "Findings suggest that a tendency for uniform information density may exist in human language, with two potential sources: (i) word order rules, and (ii) choices made by speakers, who use the flexibility present in real languages to structure information more uniformly."

### Analysis
UID provides a process-based perspective on MVSUs: semantic units should be defined relative to their information content and process load. Function words, as highly predictable units, contribute minimal information and serve primarily to distribute information more evenly across the signal. Content words, as less predictable units, carry higher information density. Telegraphic compression, which removes low-information elements, actually violates UID as it creates spikes in information density (all content, no filler). However, LLMs may have different process constraints than humans—they can handle higher information density without process difficulties. This shows that MVSUs for LLMs differ from optimal units for human process: LLMs prefer dense semantic content (telegraphic form) while humans prefer distributed information (full grammatical form).

---

## Source 16: LLM Tokenization and Semantic Units

**Full Citation:**
Trott, Sean. "Tokenization in Large Language Models, Explained." Substack (2024).
URL: https://seantrott.substack.com/p/tokenization-in-large-language-models
Additional: https://www.dailydoseofds.com/llmops-crash-course-part-2/

### Summary
This source examines how LLMs tokenize text with subword tokenization algorithms like Byte-Pair Encode (BPE), and whether these tokens align with semantic units. Subword tokenization is based on frequency rather than linguistic principle, which leads to tokens that often don't correspond to meaningful morphological units. Research shows that tokenization choices can impact semantic compositionality and LLM performance on some tasks.

### Key Quotes

1. "Subword tokenization is based on the observation that words are, by definition, composed of smaller substrings ('subwords') that recur across lots of other words."

2. "Subwords have no intrinsic meaning—at least to us humans."

3. "The way the noun was tokenized didn't really have a meaningful impact on the LLM's ability to predict the correct article."

4. "Other research contradicts this finding, showing that alien tokenization leads to poorer generalizations compared to morphological tokenization for semantic compositionality of word meanings."

5. "Subword tokenization works practically but is not really true that it implicitly learns morphology—it learns the most frequent subwords, which may or may not correspond to morphemes."

### Analysis
This source reveals a fundamental tension: LLMs process language at the token level, but tokens don't necessarily correspond to semantic units. Despite this mismatch, LLMs achieve impressive semantic comprehension, which shows that semantic representation emerges from learned associations across tokens rather than from token-level semantics. For MVSUs in LLM comprehension, this shows that the relevant semantic units are not the input tokens themselves but rather the learned embeddings and contextual representations that the LLM constructs from token sequences. Telegraphic compression works because it preserves content-word tokens that have strong, stable semantic embeddings, while it removes function-word tokens whose primary contribution is to the compositional process rather than to semantic content.

---

## Synthesis and Conclusions

### The Nature of Minimum Viable Semantic Units for LLMs

Based on the comprehensive review of linguistic theories and empirical research, we can draw several key conclusions about minimum viable semantic units (MVSUs) for LLM comprehension:

#### 1. MVSUs Are Multi-Level, Not Atomic

No single linguistic unit (morpheme, word, phrase, etc.) universally serves as the MVSU. Instead, MVSUs vary based on whether semantic value is:
- **Lexically specified** (content words: nouns, verbs, adjectives)
- **Compositionally derived** (syntactic constructions)
- **Constructionally determined** (idioms, multi-word expressions)
- **Frame-based** (events and their core participants)

#### 2. Content Words as Core MVSUs

Multiple lines of evidence converge on content words as primary MVSUs for LLM comprehension:
- **Neurolinguistic evidence**: Lexical-semantic content rather than syntactic structure drives neural language process
- **Information-theoretic evidence**: Content words carry high-entropy, low-predictability information
- **Compositional evidence**: Content words provide denotational content that feeds compositional operations
- **Frame semantic evidence**: Content words evoke semantic frames that enable inference of omitted elements

#### 3. Predicate-Argument Structure as Organizational Principle

Formal semantic theories (SRL, neo-Davidsonian semantics, MRS) consistently identify predicate-argument structure as the foundational semantic representation. MVSUs must preserve:
- **Predicates** (typically verbs, but also adjectives, nouns, prepositions in neo-Davidsonian approaches)
- **Arguments** (participants in events/states, typically nouns)
- **Structural relationships** between predicates and arguments (even if grammatical code is simplified)

#### 4. Function Words Are Structurally Necessary but Semantically Redundant

Function words contribute primarily to:
- **Compositional operations** (determiners, auxiliaries, copulas)
- **Structural markers** (prepositions, case markers, conjunctions)
- **Information distribution** (which maintains uniform information density for human process)

For LLMs with robust world knowledge and contextual inference, these functions can be performed implicitly, which makes function words redundant in telegraphic compression.

#### 5. When the System Fails to Preserve Semantic Value

Formal linguistic theories identify specific conditions where semantic compression causes loss of semantic value:

**Compositional Failure**:
- Idioms and non-compositional expressions
- Construction-specific semantic values
- Lexicalized phrases

**Pragmatic Dependency**:
- Context-dependent semantic values
- Presuppositions and implicatures
- Stylistic and tonal semantic value

**Discourse Dependency**:
- Cross-sentential anaphora
- Discourse coherence relations
- Reference tracker across sentences

**Information Density Violations**:
- When compression creates ambiguity that the system cannot resolve from context
- When multiple possible reconstructions exist with different semantic values

### Formal Criterion for Semantic Value Preservation

To integrate insights from compositionality, MRS, DRT, and information theory, we can propose a formal criterion for when telegraphic compression preserves semantic value:

**The system preserves semantic value under compression if and only if:**

1. **Structural Recoverability**: The predicate-argument structure (which includes scope relations) can be uniquely reconstructed from the compressed form plus context
2. **Referential Continuity**: Discourse referents remain identifiable across compression (even if explicit pronouns are removed)
3. **Frame Completability**: Sufficient frame-evocation elements remain to activate correct semantic frames and enable inference of core frame elements
4. **Compositional Transparency**: The elements that remain can compose through standard operations without idiomatic or construction-specific interpretation
5. **Information Sufficiency**: The compressed form retains all high-information (low-predictability) semantic content

### Implications for Brief Minification in LLM Contexts

The research strongly supports telegraphic semantic compression as a viable strategy for LLM brief minification:

**Theoretical Support**:
- Formal semantic theories demonstrate that surface grammatical form is not necessary for semantic representation
- Event semantics and predicate-argument structure provide abstract representations independent of surface realization
- Compositionality principles show that function words contribute primarily to compositional operations rather than semantic content

**Empirical Support**:
- Neurolinguistic evidence shows content words drive semantic process
- Information theory shows function words have minimal information content
- Construction grammar shows that some grammatical patterns contribute structural rather than lexical semantic value

**Practical Constraints**:
- Compression must preserve predicate-argument structure
- Core frame elements must be retained or strongly inferable
- The system must maintain discourse coherence across compressed sequences
- Idiomatic and construction-specific semantic values need intact surface forms

### Open Questions and Future Research

Several important questions remain:

1. **Optimal Compression Ratio**: What is the maximum compression ratio that maintains adequate comprehension for different LLM architectures?

2. **Task Dependency**: Do different downstream tasks need different MVSUs? (e.g., question answer vs. logical inference vs. code generation)

3. **Cross-linguistic Variation**: Do MVSUs differ across languages with different morphological and syntactic properties?

4. **Train Effect**: Do LLMs that are trained on telegraphic text develop different semantic representations than those trained on standard text?

5. **Reconstruction Fidelity**: How accurately can LLMs reconstruct full grammatical forms from telegraphic input, and does reconstruction accuracy correlate with comprehension?

### Final Assessment

The linguistic and computational evidence strongly indicates that **predicate-argument structure with content-word realization** constitutes the MVSU for LLM comprehension. Function words, while linguistically significant, are semantically redundant in the presence of:
- Strong world knowledge
- Robust contextual inference
- Learned compositional operations
- Frame-based semantic inference

Telegraphic semantic compression successfully exploits this redundancy, which removes predictable structural elements while it preserves the high-information semantic core. The formal linguistic theories reviewed—particularly compositionality, DRT, neo-Davidsonian semantics, and frame semantics—provide principled foundations to understand when such compression preserves semantic value versus when it causes semantic degradation.
