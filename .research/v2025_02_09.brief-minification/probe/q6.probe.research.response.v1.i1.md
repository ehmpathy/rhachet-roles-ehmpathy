# Research Question: What does Zipf's Law tell us about which tokens carry most information? Should high-frequency tokens be preferentially dropped?

**Research Date:** February 9, 2026
**Researcher:** Claude Sonnet 4.5
**Sources Consulted:** 14 authoritative sources

---

## Executive Summary

Zipf's Law reveals a fundamental inverse relationship between token frequency and information content: **high-frequency tokens carry significantly less information than low-frequency tokens**. This has profound implications for text process, information retrieval, and language model optimization. However, the answer to whether high-frequency tokens should be "preferentially dropped" is nuanced: while they carry less semantic information, they serve critical structural roles in language and communication efficiency.

**Key Finds:**
1. High-frequency words are predominantly function words (articles, prepositions, pronouns) with minimal semantic content
2. Low-frequency words are primarily content words (nouns, verbs, adjectives) that carry substantial semantic information
3. Information content follows Shannon's surprisal principle: information ∝ -log(probability)
4. Modern approaches use weight (e.g., TF-IDF) rather than categorical drop
5. Recent LLM research shows vocabulary prune should target rare tokens, not common ones
6. The Uniform Information Density hypothesis suggests optimal communication distributes information evenly, which requires both high and low frequency tokens

---

## Source 1: Zipf's Word Frequency Law in Natural Language (PMC Critical Review)

**Citation:** Piantadosi, S. T. (2014). Zipf's word frequency law in natural language: A critical review and future directions. *Psychonomic Bulletin & Review, 21*(5), 1112-1130. https://pmc.ncbi.nlm.nih.gov/articles/PMC4176592/

### Summary
This comprehensive critical review examines Zipf's law from cognitive, linguistic, and information-theoretic perspectives. The paper evaluates rival theoretical explanations for why word frequencies follow a power law distribution and discusses implications for our understand of language structure and information content.

### Key Quotes

1. **Mathematical Formulation:**
   > "f(r)∝1(r+β)α for α≈1 and β≈2.7"

   > "The most frequent word (r = 1) has a frequency proportional to 1, the second most frequent word (r = 2) has a frequency proportional to 1/2^α."

2. **Information Content Relationship:**
   > "the predictability (negative log probability) of a word is the measure of the information it conveys."

3. **Word Distribution:**
   > "there are few very high-frequency words that account for most of the tokens in text (e.g., 'a,' 'the,' 'I,' etc.) and many low-frequency words."

4. **Communicative Optimization Theory:**
   > "Zipf (1949) himself derived the law by consideration of a trade-off between speakers' and listeners' efforts."

   > "minimize information-theoretic notions of cost"

5. **Critical Evaluation:**
   > "the ability of a theory to derive the law provides very weak evidence for that account's cognitive validity"

6. **Memory-Based Account:**
   > "If human memory is the cause of near-Zipfian laws in language, it could provide a parsimonious and general explanation, able to unify word frequencies with memory."

### Analysis
This source establishes that information content is inversely related to word frequency through Shannon's information theory. High-frequency words are highly predictable and thus convey minimal information per occurrence. The review emphasizes that multiple theoretical mechanisms can produce Zipfian distributions, which suggests the phenomenon reflects fundamental constraints on communication and cognition rather than arbitrary linguistic design.

---

## Source 2: Drop Common Terms: Stop Words (Stanford NLP)

**Citation:** C. D. Manning, P. Raghavan, & H. Schütze (2008). Drop common terms: stop words. *Introduction to Information Retrieval*. https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html

### Summary
This authoritative NLP textbook chapter explains why information retrieval systems have historically filtered high-frequency "stop words" and discusses modern approaches that avoid categorical exclusion in favor of weighted schemes.

### Key Quotes

1. **Why Stop Words Are Filtered:**
   > "extremely common words which would appear to be of little value in help select documents match a user need"

2. **Practical Examples:**
   > "keyword searches with terms like the and by don't seem very useful"

3. **Problems with Categorical Drop:**
   > "President of the United States" [becomes less precise when stop words are removed]

   > "flights to London" [loses semantic mean if "to" is eliminated]

   > "song titles and well known pieces of verse consist entirely of words that are commonly on stop lists"

4. **Historical Evolution:**
   > [Stop list sizes have decreased] "from 200-300 terms historically to 7-12 terms currently"

   > Modern web search engines "generally do not use stop lists"

### Analysis
This source demonstrates that while high-frequency words carry little discriminative information for document retrieval, categorical drop creates problems. Modern systems recognize that context matters—even high-frequency words contribute to mean in certain configurations (phrases, titles, specialized queries). The trend away from stop lists toward weighted schemes (like TF-IDF) suggests that down-weight rather than drop is the optimal strategy.

---

## Source 3: Information is Surprise (Plus Mathematics)

**Citation:** Plus Magazine (2025). Information is surprise. https://plus.maths.org/content/information-surprise

### Summary
This accessible explanation of Shannon's information theory demonstrates the mathematical foundation for why rare events (which include rare words) carry more information than common events.

### Key Quotes

1. **Core Concept:**
   > "Claude Shannon's 1948 *Mathematical Theory of Communication* established information theory as a science. The fundamental insight: **information content correlates with surprise, which inversely relates to probability**."

2. **Mathematical Definition:**
   > "s(x) = log(1/p(x))"

3. **Impossible Events:**
   > "if the machine always produces the same letter x, so p(x) = 1, then we shouldn't be surprised at all"

4. **Rare vs. Common Words:**
   > Words like "invasion" (19 occurrences per million in English) generate greater surprise than "the" (61,847 per million).

5. **Entropy Formula:**
   > "H = -( p₁ log p₁ + p₂ log p₂ + p₃ log p₃ + ... + pₙ log pₙ)"

6. **Practical Example:**
   > "A fair coin (p(H) = 0.5) yields H = 1 bit of entropy, while a biased coin (p(H) = 0.9) produces only 0.469 bits, reflects reduced surprise and lower information content."

### Analysis
This source provides the theoretical foundation for why high-frequency tokens carry less information. The logarithmic relationship means that double a word's frequency roughly halves its information content. This mathematical principle underlies all modern approaches to weight terms by frequency—from TF-IDF to neural language models.

---

## Source 4: Predictability Effects on Word Duration (Stanford/Jurafsky Lab)

**Citation:** A. Bell, J. M. Brenier, M. Gregory, C. Girand, & D. Jurafsky (2009). Predictability effects on durations of content and function words in conversational English. *Journal of Memory and Language, 60*(1), 92-111. https://web.stanford.edu/~jurafsky/bell09.pdf

### Summary
This empirical study examines how speakers modulate word duration based on predictability, reveals fundamental differences between content words (high information) and function words (low information).

### Key Quotes

1. **Predictability and Information:**
   > "Words that are easily anticipated in context carry less information, while unpredictable words communicate more."

2. **Content vs. Function Words:**
   > "**Content words** demonstrate strong predictability effects. As these semantically rich words become more predictable, speakers shorten their pronunciation substantially."

   > "**Function words** show weaker or absent predictability effects. These grammatical elements maintain relatively stable durations regardless of how predictable they are contextually."

3. **Frequency vs. Predictability:**
   > "High-frequency words tend to be more predictable, yet frequency alone doesn't fully explain duration reduction. Instead, the actual probability of a word's occurrence in specific contexts—its predictability—drives duration shorten more directly than raw frequency statistics."

### Analysis
This source reveals that speakers unconsciously implement information-theoretic principles: they reduce articulatory effort when words are predictable (low information) and maintain full articulation when words are unpredictable (high information). Critically, content words show stronger effects than function words, which suggests that frequency and information content interact with word class in complex ways.

---

## Source 5: Zipf's Law (GeeksforGeeks NLP Guide)

**Citation:** GeeksforGeeks (2025). Zipf's Law. https://www.geeksforgeeks.org/nlp/zipfs-law/

### Summary
This technical resource provides a practical overview of Zipf's Law with concrete examples and NLP applications.

### Key Quotes

1. **Basic Pattern:**
   > "The second most used word appears half as often as the most used word."

2. **Mathematical Formula:**
   > "**f(r) = C/r^s**"

   Where:
   - f(r) = word frequency at rank r
   - C = constant value
   - s = Zipf exponent
   - r = word rank

3. **Communication Efficiency:**
   > "This principle reflects how humans communicate efficiently with minimal effort. We repeatedly use comfortable, familiar words while employ less common terms infrequently."

4. **Practical Example:**
   > In the sample text "Two friends were met by a bear..." the word "the" (rank 1) appears 3 times, while "a" (rank 2) appears 2 times—demonstrates the inverse relationship.

5. **NLP Applications:**
   - "**Information Retrieval**: Improves search algorithm efficiency by prioritize relevant terms"
   - "**Language Model**: Predicts word frequencies for speech recognition and machine translation"
   - "**Text Analysis**: Establishes baseline expectations for word distribution patterns"

6. **Important Limitation:**
   > "Only a small percentage of words actually fit the Zipfian distribution" in large corpora.

### Analysis
This source emphasizes the practical applications of Zipf's Law in NLP systems. The key insight is that information retrieval systems prioritize "relevant terms"—which are typically mid-to-low frequency content words, not high-frequency function words. This supports the principle that high-frequency tokens can be down-weighted rather than dropped.

---

## Source 6: Zipf's Law (Britannica Encyclopedia)

**Citation:** Britannica, T. Editors of Encyclopaedia (2024). Zipf's law. *Encyclopedia Britannica*. https://www.britannica.com/topic/Zipfs-law

### Summary
This authoritative encyclopedia entry provides historical context and concrete examples of Zipf's Law across domains.

### Key Quotes

1. **Definition:**
   > "**Zipf's law** describes a mathematical relationship where event frequencies are inversely proportional to their rank."

2. **Formula:**
   > "_f_(_r_) ≅ 0.1/_r_, where _f_ represents frequency and _r_ represents rank."

3. **Language Examples:**
   > "The most frequent English word, 'the,' appears roughly one-tenth of the time in typical text. The second-ranked word, 'of,' occurs about one-twentieth of the time."

4. **Limitations:**
   > "The law breaks down beyond approximately rank 1,000 in linguistic applications and lacks perfect fit across languages, populations, and other datasets."

5. **Applications:**
   - "**Data compression schemes**"
   - "**Resource allocation by urban planners**"

### Analysis
This source confirms the mathematical precision of Zipf's Law for high-frequency terms while notes limitations at lower frequencies. The mention of data compression is significant—compression algorithms like Huffman code assign shorter codes to frequent symbols, exploits the same inverse frequency-information relationship that Zipf's Law describes.

---

## Source 7: The Mystery of Zipf (Plus Mathematics)

**Citation:** Plus Magazine (2023). The mystery of Zipf. https://plus.maths.org/content/mystery-zipf

### Summary
This article explores rival theories about why Zipf's Law exists, which include random processes, word length patterns, and universal mathematical principles.

### Key Quotes

1. **Random Generation Theory:**
   > "words generated by random combine letters fit the Zipf distribution"

   > "the rank distribution arises naturally out of the fact that word length plays a part — long words tend not to be very common, whilst shorter words are"

2. **Statistical Artifact Hypothesis:**
   > "Li argues that as Zipf distributions arise in random-generated texts with no linguistic structure, the law may be a statistical artifact rather than a meaningful linguistic property."

3. **Empirical Evidence:**
   > "The three most frequent words were 'the' (114,001 occurrences, 6.86%), 'of' (62,964 times), and 'to' (45,045 times)."

   > "the word 'maths' features more highly than in normal usage, come in at 40th place have been mentioned 4,829 times"

4. **Universal Pattern:**
   > "Power laws relate rank to frequency have been demonstrated to occur naturally in many places — the size of cities, the number of hits on websites, the magnitude of earthquakes and the diameters of moon craters."

5. **Recent Understand (2023):**
   > researchers claim to have proven Zipf's law operates as "an emergent property of all discrete systems"

### Analysis
This source presents a crucial perspective: Zipf's Law may be a mathematical inevitability rather than a designed feature of language. If true, this suggests that the inverse frequency-information relationship is fundamental to any symbolic communication system, not just human language. This would support cautious approaches to drop high-frequency tokens, as their presence may be structurally necessary for efficient communication.

---

## Source 8: Content and Function Words (Baruch CUNY)

**Citation:** Baruch College CUNY (2024). Content and function words. https://tfcs.baruch.cuny.edu/content-and-function-words/

### Summary
This educational resource clearly distinguishes content words from function words and explains their different roles in mean and stress patterns.

### Key Quotes

1. **Definitions:**
   > "**Content Words** carry primary mean and include: Nouns (paper, coffee, Mr. Smith), Main verbs (talk, watched, need), Adjectives (tall, blue, enchant), Adverbs (slowly, nervously), Wh-words (why, what, how)"

   > "**Function Words** have minimal semantic value and include: Prepositions (on, for, with), Determiners (a, the, some), Auxiliary verbs (is, have, been), Conjunctions (and, but, yet), Most pronouns (she, they, him)"

2. **Frequency:**
   > "function words make up about 50% of any English text because of the conventional patterns of usage"

3. **Stress Patterns:**
   > "stressed syllables are typically longer than unstressed syllables"

   > "Kids beat drums" = "The kids will have been beat the drums" [both take roughly equivalent time]

4. **Information Content:**
   > In "What do you think about the black shirt?" the four content words (_what, think, black, shirt_) convey the core message, while four function words support grammatical relationships.

5. **Role in Communication:**
   > "Content words dominate in intelligibility. Prioritize their stress enhances overall communication clarity."

### Analysis
This source provides critical evidence that high-frequency words (function words) comprise half of all text yet carry minimal semantic information. They serve structural rather than content roles. This suggests that drop them entirely would destroy grammatical coherence, but down-weight them (as TF-IDF does) preserves their structural function while appropriately reduces their influence on semantic similarity and information retrieval.

---

## Source 9: Zipf's Law of Abbreviation and Principle of Least Effort

**Citation:** Multiple sources on Zipf's principle of least effort and abbreviation. Referenced from search results include PubMed (https://pubmed.ncbi.nlm.nih.gov/28494263/) and Springer (https://link.springer.com/article/10.3758/s13423-022-02142-9)

### Summary
These sources explain Zipf's original theory that word frequency patterns emerge from rival pressures to minimize speaker effort while maintain listener comprehension.

### Key Quotes

1. **Zipf's Original Observation:**
   > "Linguist George Kingsley Zipf observed that the more frequent a word is, the shorter it tends to be."

2. **Principle of Least Effort:**
   > "Zipf hypothesised that this universal design feature arises as a result of individuals optimise form-mean maps under rival pressures to communicate accurately but also efficiently—his famous Principle of Least Effort."

3. **Speaker vs. Hearer Trade-off:**
   > "Zipf argued that speakers try to minimize effort by use the smallest vocabulary of words as possible, while hearers prefer less ambiguity in a word use and hence a larger vocabulary to express all possible concepts."

4. **Information-Theoretic Solution:**
   > "The solution is to assign the shortest words to the most frequent means, leave longer words for less frequent means, similar to variable-length code used in information theory."

5. **Optimization Function:**
   > "Zipf's formula can be instantiated with a communication optimization function from information theory that consists of a pressure to maximize the expressivity of the system, while on the other hand a pressure of parsimony tries to limit the cost of communication."

6. **Empirical Support:**
   > "When pressures to communicate accurately and efficiently are both present and in conflict, language users exploit information in the input about the frequency of means to converge on an optimally-configured lexicon."

### Analysis
These sources reveal that high-frequency words exist *because* they carry less information—they can be shorter and simpler precisely because their mean is recoverable from context and familiarity. This suggests that drop high-frequency words entirely would disrupt the optimized balance between speaker effort and communicative accuracy that natural language achieves.

---

## Source 10: TF-IDF Analysis (Tidytext Mine)

**Citation:** J. Silge & D. Robinson (2017). Analyze word and document frequency: tf-idf. *Text Mine with R*. https://www.tidytextmining.com/tfidf

### Summary
This practical guide demonstrates how TF-IDF down-weights common terms while preserves important terms, with concrete examples from literary texts.

### Key Quotes

1. **IDF Purpose:**
   > "IDF (Inverse Document Frequency) reduces the weight assigned to words appear across many documents. This decreases the weight for commonly used words and increases the weight for words that are not used very much in a collection of documents."

2. **Mathematical Formula:**
   > `idf(term) = ln(n_documents / n_documents contain term)`

3. **Zero Weight for Universal Terms:**
   > "Words appear in all documents receive an IDF score of zero (ln(1) = 0), makes their tf-idf contribution negligible regardless of frequency within individual documents."

4. **Low IDF Examples:**
   > "Common words like 'the,' 'and,' 'to,' and 'of' appear across all six Jane Austen novels, results in zero IDF scores and near-zero tf-idf values despite high term frequencies."

5. **High IDF Examples:**
   > "Character names like 'elinor' (tf-idf: 0.00931), 'marianne' (0.00735), and 'darcy' (0.00547) appear in only some novels, preserves substantial tf-idf scores that identify distinctive content."

6. **Information-Rich Words:**
   > "what distinguishes one novel from the rest within the collection of her works are the proper nouns, the names of people and places."

7. **Domain-Specific Examples:**
   In physics texts:
   - Galileo: "surface," "body," "light"
   - Tesla: "current," "potential," "frequency"
   - Einstein: "relativity," "space," "time"
   - Huygens: "ray," "light," "wave"

8. **Superiority to Stop Word Lists:**
   > "some of these words might be more important in some documents than others"

### Analysis
This source provides concrete evidence that weight outperforms categorical drop. Common words receive near-zero weights automatically through IDF, while preserves them for contexts where they might matter. The proper noun examples are particularly instructive—names are relatively infrequent globally but carry high information content for distinguish specific documents.

---

## Source 11: Inverse Document Frequency (Stanford NLP)

**Citation:** C. D. Manning, P. Raghavan, & H. Schütze (2008). Inverse document frequency. *Introduction to Information Retrieval*. https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html

### Summary
This technical explanation from Stanford's authoritative IR textbook explains why document frequency (not collection frequency) is the appropriate metric for assess term importance.

### Key Quotes

1. **Problem with Raw Frequency:**
   > "all terms are considered equally important when it comes to assess relevancy on a query"

   > common terms like "auto" in an auto-industry document collection lack discriminate power

2. **Why Document Frequency:**
   > "in try to discriminate between documents for the purpose of score it is better to use a document-level statistic (such as the number of documents contain a term) than to use a collection-wide statistic for the term."

3. **Concrete Example:**
   > "try" and "insurance" had similar collection frequencies (~10,400), yet "insurance" appeared in only 3,997 documents versus 8,760 for "try." Consequently, "few documents that contain insurance" should receive "higher boost" in relevance score.

4. **Formula:**
   > **idf_t = log(N / df_t)**

   Where N represents total collection documents. This ensures "rare term is high, whereas the idf of a frequent term is likely to be low."

5. **Scale Example:**
   > The Reuters collection (806,791 documents) demonstrates practical application

### Analysis
This source clarifies that information content relates to *document* distribution, not just raw frequency. A word appears 1000 times in one document carries less information than a word appears once each in 1000 documents. This distinction is crucial for why simple frequency-based drop is suboptimal—contextual distribution matters as much as raw counts.

---

## Source 12: LLM Prune Techniques (GitHub Prune-LLMs)

**Citation:** Jordddan (2024). Prune-LLMs: Components, methods, and trade-offs. https://github.com/jordddan/Pruning-LLMs/blob/main/README_EN.md

### Summary
This technical resource documents practical approaches to prune large language models, which includes vocabulary/embed prune based on token frequency.

### Key Quotes

1. **Components Pruned:**
   - "**Depth Prune**: Removes entire transformer layers"
   - "**Width Prune**: Reduces hidden dimensions"
   - "**Vocabulary/Embed Prune**: Shrinks token embeds by remove rarely-used tokens"

2. **Importance Determination:**
   > "importance is based on **frequency statistics**: 'select tokens that the model is unlikely to encounter in subsequent usage through frequency sort.'"

3. **Vocabulary Prune Method:**
   > "identify high-frequency tokens within target datasets, prune the SentencePiece tokenizer accordingly, and remove correspond embed weights"

4. **Empirical Results:**
   > "Test on LLaMA2-7B reduced vocabulary from 32,000 to 15,000 tokens with minimal performance degradation (42.33→42.21 on MMLU)."

5. **Performance Trade-offs:**
   > "**Pruned-7B** (from LLaMA2-13B) achieved 1.56 loss after 20B train tokens versus 1.54 for original LLaMA2-7B trained on 2T tokens"

### Analysis
**This source provides critical evidence that contradicts the naive interpretation of Zipf's Law:** Modern LLM prune removes *low-frequency* (rare) tokens, not high-frequency ones. The rationale is that rare tokens consume embed space but appear infrequently at inference. This suggests that for computational efficiency, keep high-frequency tokens is essential, while rare tokens can be pruned with minimal performance loss. This is the opposite of drop high-frequency tokens.

---

## Source 13: Mandelbrot's Information-Theoretic Optimization

**Citation:** Multiple sources on Mandelbrot's refinement of Zipf's law through information theory. Referenced from ResearchGate and PMC sources (https://www.researchgate.net/publication/220469172, https://pmc.ncbi.nlm.nih.gov/articles/PMC4176592/)

### Summary
These sources explain Benoit Mandelbrot's 1953 information-theoretic interpretation of Zipf's Law, which frames the frequency distribution as an optimization problem.

### Key Quotes

1. **Zipf-Mandelbrot Law:**
   > "In 1966, mathematician Benoit Mandelbrot gave a generalized form of Zipf's law, which is now called the Zipf-Mandelbrot Law."

2. **Information-Theoretic Cost:**
   > "Mandelbrot (1953) showed how the Zipfian distribution could arise from minimize information-theoretic notions of cost."

3. **Optimization Principle:**
   > "Mandelbrot's model derives Zipf's law as a result of the optimization of information/cost ratio."

4. **Communication Evolution:**
   > "Word frequency distributions appear to have power law distributions and have been taken as evidence that they have evolved to optimize communication"

5. **Cognitive Limits:**
   > "Zipf's exponents' deviation to larger values marks a transition from an optimal to a suboptimal state, dictated by cognitive limits, impose a fundamental limit on communication efficiency where cognitive constraints lead to reliance on combinations of words rather than creation of new vocabulary."

### Analysis
Mandelbrot's framework suggests that Zipf's Law emerges from optimize the trade-off between information transmission and cognitive/articulatory cost. High-frequency words exist because they minimize production cost for frequently needed concepts, while their low information content is acceptable because of their high predictability. This optimization means the entire frequency distribution is functionally important—disrupt it by remove high-frequency tokens would degrade the optimized information/cost balance.

---

## Source 14: Uniform Information Density Hypothesis

**Citation:** Multiple sources on UID hypothesis. Referenced from ACL Anthology (https://aclanthology.org/2021.emnlp-main.74/), MIT Press (https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00589/117221/), and Cambridge (https://www.cambridge.org/core/journals/applied-psycholinguistics/)

### Summary
The Uniform Information Density (UID) hypothesis proposes that speakers distribute information relatively evenly across utterances to optimize communication efficiency and listener process.

### Key Quotes

1. **Core Definition:**
   > "The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal."

2. **Process Load:**
   > "speakers tend to spread information evenly throughout an utterance; large fluctuations in the per-unit information content of an utterance can impede communication by increase the process load on the listener."

3. **Constant Rate Transmission:**
   > "The UID hypothesis proposes that speakers communicate by transmit information close to a constant rate."

4. **Syntactic Variation:**
   > "When choose between two syntactic variants, it claims that speakers prefer the variant distribute information most evenly, avoid signal peaks and troughs."

5. **Linguistic Scope:**
   > "The studies adduced in support of this hypothesis in language production span levels of linguistic structure: from phonetics to lexical choice, to syntax, and to discourse."

6. **Concrete Example:**
   > "American English speakers' use of contractions ('you are' →'you're') follows the predictions of UID."

7. **Comprehension Effects:**
   > "If speakers prefer transmit information uniformly, then comprehenders should also prefer a uniform signal, experience difficulty whenever confronted with informational peaks."

### Analysis
The UID hypothesis provides crucial context for token drop decisions: if natural language optimizes for even information distribution, then remove entire classes of tokens (like all high-frequency words) would create pathological "lumpy" information distributions. Instead, optimal process might require the interleave of high-frequency (low-information) and low-frequency (high-information) tokens that natural language provides. This suggests that weighted approaches preserve distributional balance outperform categorical drop.

---

## Synthesis and Conclusions

### 1. The Information Content Gradient

**Find:** Zipf's Law describes a continuous gradient from high-frequency/low-information tokens to low-frequency/high-information tokens, not a binary division.

**Evidence:**
- Shannon's surprisal formula: I(w) = -log₂(P(w)) creates a continuous logarithmic relationship
- TF-IDF assigns weights on a spectrum, not categorically
- The top 10 words account for ~25% of tokens but minimal semantic content, while words ranked 1000-10000 occur rarely but carry specific mean

**Implication:** Simple binary decisions (drop/keep) are less effective than graduated weight schemes that preserve the full frequency spectrum.

### 2. Function vs. Content Words

**Find:** High-frequency tokens are predominantly function words (articles, prepositions, pronouns, auxiliary verbs) that serve grammatical rather than semantic functions.

**Evidence:**
- Function words comprise ~50% of text despite minimal semantic content
- Content words receive prosodic stress and longer durations; function words are unstressed and shortened
- Remove function words breaks grammatical structure: "President of the United States" vs. "President United States"

**Implication:** High-frequency tokens cannot be dropped entirely without destroy syntactic coherence. However, they can be down-weighted for semantic similarity and information retrieval tasks.

### 3. Optimal Communication Requires Both

**Find:** Natural language has evolved an optimized distribution balances speaker efficiency (reuse short, simple words) with communicative accuracy (employ specific, rare words for precise mean).

**Evidence:**
- Zipf's Principle of Least Effort explains the frequency distribution as an optimal trade-off
- Mandelbrot's information-theoretic derivation shows Zipfian distributions minimize communication cost
- Uniform Information Density hypothesis demonstrates that even information distribution improves comprehension

**Implication:** The Zipfian distribution itself is functional. Disrupt it by remove high-frequency tokens would create suboptimal "lumpy" information flow.

### 4. Context Determines Importance

**Find:** A token's information content depends not just on its global frequency but on its distribution across contexts and documents.

**Evidence:**
- IDF uses document frequency, not collection frequency
- "insurance" and "try" had similar collection frequencies but different document distributions, gives "insurance" higher information value
- Stop words become important in specific contexts (song titles, phrases, proper names like "The Who")

**Implication:** Frequency-based categorical decisions are too coarse. Context-aware weight (like TF-IDF or attention mechanisms) captures importance more accurately.

### 5. Modern LLM Prune: Opposite Direction

**Find:** Recent research on vocabulary prune for LLMs removes *low-frequency* (rare) tokens, not high-frequency ones.

**Evidence:**
- LLaMA2-7B pruned from 32,000 to 15,000 tokens by remove rare tokens with minimal performance loss
- Rationale: rare tokens consume embed space but appear infrequently at inference
- High-frequency tokens are essential for grammatical coherence and common expressions

**Implication:** For computational efficiency in neural models, keep high-frequency tokens is critical, while rare tokens can be sacrificed. This directly contradicts the idea of "preferentially drop" high-frequency tokens.

### 6. Different Tasks, Different Strategies

**Find:** The optimal approach to handle high-frequency tokens depends on the specific task.

**Evidence:**

| Task | Approach | Rationale |
|------|----------|-----------|
| Information Retrieval | Down-weight (TF-IDF) | Reduce influence without lose grammatical function |
| Semantic Similarity | Down-weight or filter | Focus on content words carry distinctive mean |
| Language Model | Keep all frequencies | Predict full natural language include function words |
| Text Compression | Shorter codes for high-freq | Exploit frequency distribution for efficiency |
| LLM Vocabulary Prune | Remove rare tokens | Optimize embed memory and inference speed |
| Syntactic Parse | Weight all tokens | Function words provide grammatical structure |

**Implication:** There is no universal answer to "should high-frequency tokens be dropped?" The answer depends entirely on the objective function optimized.

---

## Actionable Recommendations

### For Information Retrieval Systems:
1. **Use TF-IDF or BM25** rather than categorical stop word lists
2. **Down-weight high-frequency terms** but preserve them for phrase match
3. **Apply context-sensitive weight** where term importance varies by document collection
4. **Preserve function words for phrasal queries** (e.g., "return on investment," "out of office")

### For Semantic Analysis and Topic Model:
1. **Filter high-frequency function words** when extract key terms or topics
2. **Weight by information content** use IDF or similar metrics
3. **Preserve domain-specific high-frequency terms** that may carry mean in specialized contexts
4. **Consider mid-frequency content words** as the richest source of semantic information

### For Language Model Development:
1. **Do not prune high-frequency tokens** from vocabulary—they're essential for coherent generation
2. **Consider prune low-frequency tokens** to reduce embed size with minimal performance impact
3. **Use subword tokenization** (BPE, WordPiece) to handle rare words compositionally
4. **Maintain Zipfian train distributions** for optimal model convergence

### For Text Compression and Encode:
1. **Assign shorter codes to high-frequency tokens** (Huffman code principle)
2. **Exploit Zipfian distribution** for maximum compression efficiency
3. **Use variable-length encode** rather than uniform representation

### For Human-AI Interaction:
1. **Preserve natural frequency distributions** in generated text for readability
2. **Maintain function words** for grammatical fluency
3. **Balance information density** to avoid overwhelm users with rare/technical terms

---

## Final Answer to Research Question

**What does Zipf's Law tell us about which tokens carry most information?**

Zipf's Law reveals that **low-frequency tokens carry the most information per occurrence**, follows Shannon's principle that information content is inversely proportional to probability. High-frequency tokens (primarily function words) occur frequently precisely because they carry little semantic information—they're predictable and serve primarily grammatical functions. Mid-to-low frequency content words (nouns, verbs, adjectives) carry the semantic payload that distinguishes documents and conveys specific mean.

**Should high-frequency tokens be preferentially dropped?**

**No, but they should be preferentially down-weighted in most applications.** Complete removal creates multiple problems:

1. **Destroys grammatical structure** needed for syntactic analysis and natural language generation
2. **Breaks phrasal means** where function words matter ("out of order" ≠ "order")
3. **Disrupts optimized communication efficiency** that natural language evolution has achieved
4. **Contradicts modern LLM practices** which prune rare tokens, not common ones

The optimal strategy is **graduated weight** (TF-IDF, BM25, attention mechanisms) that reduces the influence of high-frequency tokens without eliminate them. This preserves structural integrity while appropriately emphasizes information-rich rare terms.

**Exception:** For specialized semantic analysis tasks (keyword extraction, topic model), filter high-frequency function words can improve results, but this should be task-specific rather than a universal policy.

**Key Insight:** Zipf's Law describes an *optimized* system. The frequency distribution exists because it balances rival pressures for communication efficiency. Rather than disrupt this optimization by drop categories of tokens, modern systems should work *with* the natural distribution through appropriate weight and context-sensitive importance model.

---

## Sources

1. [Zipf's word frequency law in natural language - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC4176592/)
2. [Drop common terms: stop words - Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/dropping-common-terms-stop-words-1.html)
3. [Information is surprise - Plus Mathematics](https://plus.maths.org/content/information-surprise)
4. [Predictability effects on word duration - Stanford/Jurafsky](https://web.stanford.edu/~jurafsky/bell09.pdf)
5. [Zipf's Law - GeeksforGeeks](https://www.geeksforgeeks.org/nlp/zipfs-law/)
6. [Zipf's law - Britannica](https://www.britannica.com/topic/Zipfs-law)
7. [The mystery of Zipf - Plus Mathematics](https://plus.maths.org/content/mystery-zipf)
8. [Content and Function Words - Baruch CUNY](https://tfcs.baruch.cuny.edu/content-and-function-words/)
9. [Zipf's Law of Abbreviation - PubMed](https://pubmed.ncbi.nlm.nih.gov/28494263/)
10. [Analyze word frequency: tf-idf - Tidytext Mine](https://www.tidytextmining.com/tfidf)
11. [Inverse document frequency - Stanford NLP](https://nlp.stanford.edu/IR-book/html/htmledition/inverse-document-frequency-1.html)
12. [Prune-LLMs - GitHub](https://github.com/jordddan/Pruning-LLMs/blob/main/README_EN.md)
13. [Understand TF-IDF - GeeksforGeeks](https://www.geeksforgeeks.org/machine-learning/understanding-tf-idf-term-frequency-inverse-document-frequency/)
14. [Revisit Uniform Information Density - ACL Anthology](https://aclanthology.org/2021.emnlp-main.74/)

**Additional References from Search Results:**
- [Zipf's law - Wikipedia](https://en.wikipedia.org/wiki/Zipf's_law)
- [Word Frequency and Predictability - MIT Press](https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00119/120013/)
- [Zipf's Law and AI - Alphanome](https://www.alphanome.ai/post/zipf-s-law-a-hidden-power-shaping-ai-and-natural-language-processing)
- [Inverse Document Frequency - ScienceDirect](https://www.sciencedirect.com/topics/computer-science/inverse-document-frequency)
- [Dynamic Vocabulary Prune - ArXiv](https://arxiv.org/html/2512.23087)

---

**Document Metadata:**
- Total Sources Reviewed: 14 primary sources with extensive quotes, plus 10+ support references
- Research Depth: Comprehensive coverage across information theory, linguistics, NLP, and modern ML
- Authoritative Sources: Stanford NLP, PMC peer-reviewed journals, MIT Press, Encyclopedia Britannica, lead technical guides
- Quote Density: 70+ direct quotes across all sources
- Analysis: Multi-perspective synthesis address theoretical foundations, empirical evidence, and practical applications
