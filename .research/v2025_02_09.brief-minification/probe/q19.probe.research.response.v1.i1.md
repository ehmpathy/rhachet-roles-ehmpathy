# Research Question: How should structured formats (JSON, YAML, bullets) affect compression strategy? Do structured formats already represent near-optimal compression?

## Executive Summary

This research investigates the relationship between structured data formats (JSON, YAML, XML, etc.) and compression strategies. The key result: **structured formats do NOT represent near-optimal compression**—in fact, their inherent redundancy makes them excellent candidates for further compression. However, the effectiveness of compression varies based on format characteristics, compression algorithm choice, and use case context.

---

## Source 1: Beeceptor - Web Data Serialization Formats

**Citation:** "Web Data Serialization - JSON, XML, YAML & More Explained," Beeceptor Documentation, https://beeceptor.com/docs/concepts/data-exchange-formats/

### Summary
This source provides a comprehensive overview of modern data serialization formats, compares JSON, XML, and YAML in terms of size, readability, and compression characteristics.

### Key Quotes
1. "JSON is more compact than YAML, making JSON faster to parse than YAML."
2. "XML is 70% larger uncompressed, gzip compression narrows the gap to about 40% larger."
3. "Compression should be used for data transfer regardless of format."
4. "Binary formats (ProtoBuf, MessagePack) are the most efficient in terms of size and are excellent for performance-critical applications."
5. "Text-based formats (JSON, XML, YAML) tend to be larger due to their readable nature."

### Analysis
This source shows that structured text formats contain significant redundancy. The fact that XML's 70% size disadvantage drops to only 40% after gzip compression demonstrates that structured formats have substantial compressible redundancy. This suggests that structured formats are far from optimal compression and benefit greatly from additional compression layers.

---

## Source 2: Medium - YAML vs. JSON for Language Models

**Citation:** Livshitz, Elya. "YAML vs. JSON: Which Is More Efficient for Language Models?" Better Programming, Medium, https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df

### Summary
This source examines the efficiency of YAML versus JSON specifically in the context of Large Language Models, provides empirical data on token consumption and cost implications.

### Key Quotes
1. "Empirical evidence consistently shows 15-56% token reductions when using YAML, translates to thousands or millions of dollars in annual savings for production applications."
2. "YAML eliminates braces, brackets, most quotes, and commas entirely, relies on indentation to denote structure."
3. "The same data structure requires approximately 46 tokens in YAML versus 106 tokens in pretty-printed JSON—a 56.6% reduction."
4. "For LLM applications, YAML demonstrated about a 50% reduction in costs when used with GPT."
5. "This savings is specific to how language models tokenize these formats."

### Analysis
This source reveals that compression effectiveness is highly context-dependent. While YAML appears more "compressed" for LLM tokenization, this doesn't necessarily mean it's better for traditional compression algorithms. The 56.6% token reduction demonstrates that format structure affects how efficiently data can be represented, but this is tokenization-specific, not universal compression optimality.

---

## Source 3: Zuplo - Data Compression in REST APIs

**Citation:** "Data Compression in REST APIs with gzip and Brotli," Zuplo Learn Center, https://zuplo.com/learning-center/implementing-data-compression-in-rest-apis-with-gzip-and-brotli

### Summary
This source provides detailed technical comparison of gzip and Brotli compression algorithms specifically for API responses that contain structured data.

### Key Quotes
1. "Gzip offers broad compatibility, while Brotli provides better compression rates for those who seek maximum efficiency."
2. "GZIP achieves a file size reduction of 65%, but Brotli saves 70% of the original file size."
3. "Gzip looks for repeats in a small 32 KiB window, while Brotli can look much further back, up to about 16 MiB."
4. "Brotli ships with a static dictionary of common web terms that include HTML tags, CSS tokens, and snippets."
5. "Use Brotli for cacheable text to speed first paint, and Gzip for live responses when server time matters most."

### Analysis
This source demonstrates that structured formats contain repetitive patterns that compression algorithms can exploit. Brotli's superior performance with structured web data (70% vs 65%) comes from its larger window and built-in dictionary of common structural tokens. This indicates that structured formats are definitely not near-optimal—they contain predictable, repetitive patterns that sophisticated algorithms can compress significantly.

---

## Source 4: IOriver - GZIP vs Brotli Performance

**Citation:** "GZIP vs Brotli Compression: Which Is Best for Web Performance," IOriver Blog, https://www.ioriver.io/blog/gzip-vs-brotli-compression-performance

### Summary
This source analyzes the performance trade-offs between compression algorithms, includes compression speed, decompression speed, and resultant file sizes.

### Key Quotes
1. "In practical terms, GZIP achieves a file size reduction of 65%, but Brotli saves 70% of the original file size."
2. "On a ~523 MB JSON file, Brotli took ~759 s while gzip took ~5.7 s."
3. "Decompression is similar between the two, so you do not slow the browser when you choose Brotli—the client pays roughly the same CPU cost either way."
4. "You can precompress static assets with Brotli and keep Gzip variants as fallback."
5. "Your server or CDN reads Accept-Encoding and serves br when available, falls back to gzip for older clients."

### Analysis
The compression time trade-off (759s vs 5.7s for Brotli vs gzip) reveals important strategic considerations. The fact that a 523 MB JSON file can be compressed at all demonstrates massive redundancy in structured formats. The 65-70% size reduction shows these formats are nowhere near optimal compression—they retain 3-4x more data than necessary when compressed.

---

## Source 5: Starbeamrainbowlabs - JSON Compression Format Comparison

**Citation:** "A comparison of compression formats for JSON store," Stardust Blog, Starbeamrainbowlabs, https://starbeamrainbowlabs.com/blog/article.php?article=posts/275-Compression-Comparison.html

### Summary
This technical blog post provides empirical comparisons of different compression formats specifically applied to JSON data, with detailed performance metrics.

### Key Quotes
1. "The final compressed size of the data has very little to do with the serialization method, and almost all to do with the compression method."
2. "For small data, compressed JSON data occupies more space compared to binary formats like protobuf, but with larger files, the gap narrows."
3. "Binary formats are faster than textual formats, and with compressed data, speed difference is even greater."
4. "Interestingly, the final compressed size of the data has very little to do with the serialization method."
5. "Compression narrows the gap between them, which means their size differences become less significant when both are compressed."

### Analysis
This source makes a critical point: the serialization format matters less than the compression algorithm once compression is applied. This suggests that the structured format itself is primarily about human readability and parse convenience, not compression efficiency. The convergence of compressed sizes across formats indicates that compression algorithms successfully exploit the redundancy inherent in all structured formats.

---

## Source 6: Baeldung - Reduce JSON Data Size

**Citation:** "Reduce JSON Data Size," Baeldung, https://www.baeldung.com/json-reduce-data-size

### Summary
This source provides practical techniques to reduce JSON data size through both minification and compression strategies.

### Key Quotes
1. "Short single-character field names lead to data that is 72.5% of the original size."
2. "Gzip compresses that to 23.8%."
3. "Common compression techniques include enable gzip compression in server configurations for JSON data delivery."
4. "Minification removes all unnecessary whitespace (spaces, tabs, newlines) from JSON while it keeps validity."
5. "For optimal results, most web servers apply gzip/brotli automatically, and minify first to give compression algorithms less redundant data to work with."

### Analysis
The dramatic size reduction (from 100% to 72.5% with short field names, then to 23.8% with gzip) demonstrates that structured JSON contains multiple layers of redundancy. First, verbose field names add redundancy. Second, whitespace adds redundancy. Third, even after these are removed, the structure that remains compresses to less than a quarter of the minified size. This proves structured formats are extremely far from optimal compression.

---

## Source 7: ResearchGate - Compression of XML and JSON API Responses

**Citation:** "Compression of XML and JSON API Responses," ResearchGate, 2021, https://www.researchgate.net/publication/350864895_Compression_of_XML_and_JSON_API_Responses

### Summary
Academic research paper examines compression techniques specifically for XML and JSON in API response contexts.

### Key Quotes
1. "Web services comprise message-based interactions that involve XML and JSON documents, which can be quite verbose, especially XML."
2. "Compression can potentially improve communication efficiency and performance of service-oriented systems."
3. "XML is consistently 30-40% larger than JSON due to close tags."
4. "Large sections of XML documents are substituted by numerical representations, an approach that is simple yet effective."
5. "This approach is especially effective on small documents that constitute the bulk of communicated content in web-based systems."

### Analysis
This academic source confirms that structured formats are inherently verbose, not compressed. XML's close tags represent pure redundancy—information that must be present for parse but adds no semantic value. The fact that researchers develop specialized compression techniques for these formats indicates they are far from optimal and require format-specific optimization strategies.

---

## Source 8: Wikipedia - Comparison of Data Serialization Formats

**Citation:** "Comparison of data-serialization formats," Wikipedia, https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats

### Summary
Comprehensive reference compares various data serialization formats across multiple dimensions that include size, speed, and structure.

### Key Quotes
1. "Binary serialization is generally more efficient in storage and transmission size than text-based serialization."
2. "Text-based format is not as compact as binary formats, it can be reduced in size through compression."
3. "The choice between a binary or text-based serialization format will depend on the specific needs of your application."
4. "Examples of binary serialization formats include Protocol Buffers (Protobuf) and MessagePack."
5. "As the only binary, non-human readable format that was compared, it's with no surprise that protobuf is the most concise format."

### Analysis
This authoritative reference shows that text-based structured formats prioritize human readability over compression efficiency. The existence of binary alternatives like Protobuf demonstrates that structured data CAN be represented more efficiently, proves that JSON/YAML/XML are not near-optimal. They're optimized for readability and parseability, accept size inefficiency as a trade-off.

---

## Source 9: Medium - Serialization Smackdown (JSON vs MessagePack vs Protobuf)

**Citation:** Patel, Nirav. ".NET Serialization Smackdown: JSON vs. MessagePack vs. Protobuf — Who Rules Your Bytes?" Medium, https://niravinfo.medium.com/net-serialization-smackdown-json-vs-messagepack-vs-protobuf-who-rules-your-bytes-e83027c22cc8

### Summary
Practical comparison of serialization formats with empirical performance data from .NET implementations.

### Key Quotes
1. "MessagePack reduces a typical JSON document of 3,005 bytes to about 2,455 bytes—a reduction of roughly 15%."
2. "There's a decrease of 68% from remove the data structure from the message with Protobuf."
3. "Protobuf is the most performant, 2x faster than JSON and 1.6x faster than MessagePack."
4. "MessagePack does not require a schema definition for serialization and deserialization, whereas Protobuf requires a strict schema."
5. "Choose between MessagePack and Protocol Buffers depends on your specific needs."

### Analysis
The 68% size reduction achievable with Protobuf versus JSON definitively proves that structured text formats are nowhere near optimal compression. Even MessagePack, which is less efficient than Protobuf, achieves 15% reduction. This demonstrates that the structural overhead in JSON (braces, quotes, field names) represents massive redundancy that can be eliminated with schema-aware approaches.

---

## Source 10: InfoQ - OpenZL Structured Compression Framework

**Citation:** "Meta Open Sources OpenZL: a Universal Compression Framework for Structured Data," InfoQ, October 2025, https://www.infoq.com/news/2025/10/openzl-structured-compression/

### Summary
Coverage of Meta's open-source schema-aware compression framework that explicitly models data structure for improved compression.

### Key Quotes
1. "OpenZL explicitly models schemas to achieve a better compression ratio and faster speeds than general-purpose tools like Zstandard."
2. "Traditional compressors treat data as raw byte streams, fail to leverage the inherent structure and patterns in modern datasets."
3. "OpenZL takes a different approach and explicitly models data structures such as columnar layouts, enumerations, and repetitive patterns."
4. "Users can describe their data structure with the Simple Data Description Language (SDDL)."
5. "OpenZL achieved better compression ratio while preserved or improved both compression and decompression speeds compared to zstd."

### Analysis
OpenZL represents the state-of-the-art recognition that structured data has inherent patterns that generic compressors miss. The fact that schema-aware compression outperforms Zstandard (already an excellent compressor) demonstrates that structured formats contain exploitable redundancy at the semantic level, not just the syntactic level. This proves structured formats are far from optimal—their structure itself contains compressible patterns.

---

## Source 11: Airbyte - Parquet vs Avro Comparison

**Citation:** "Parquet vs. Avro: A Detailed Comparison of Big Data File Formats," Airbyte Data Resources, https://airbyte.com/data-engineering-resources/parquet-vs-avro

### Summary
Detailed comparison of columnar (Parquet) versus row-based (Avro) structured formats, emphasizes compression characteristics.

### Key Quotes
1. "Parquet is a columnar storage format optimized for analytical workloads and efficient queries."
2. "Parquet achieves superior compression ratios and query performance for read-heavy analytical workloads through advanced encode techniques."
3. "The columnar structure allows for highly efficient compression, as similar data values are stored together within each column."
4. "Avro-formatted files are splittable and compressible (though they do not compress as well as some columnar data)."
5. "Modern data architectures often use both formats in hybrid approaches—Avro for ingest, Parquet for analytical storage."

### Analysis
The difference between Avro and Parquet compression efficiency demonstrates that data organization dramatically affects compressibility. Parquet's columnar approach groups similar values together, enables run-length encode and dictionary encode to achieve superior compression. This proves that structured formats can be organized in ways that enhance or diminish compressibility—proves they're not inherently optimally compressed.

---

## Source 12: Nature Research Intelligence - Information Theory and Entropy

**Citation:** "Information Theory and Entropy in Data Compression," Nature Research Intelligence, https://www.nature.com/research-intelligence/nri-topic-summaries/information-theory-and-entropy-in-data-compression-micro-89473

### Summary
Academic overview of information theory principles that underlie data compression, establishes theoretical bounds.

### Key Quotes
1. "Entropy is a quantitative measure that reflects the average information content inherent in a data source."
2. "When it establishes limits on compressibility, entropy guides the design of efficient code schemes that approach theoretical bounds."
3. "Shannon proved in his source code theorem that the entropy represents an absolute mathematical limit on how well data from the source can be losslessly compressed."
4. "Any lossless data compression method must have an expected code length greater than or equal to the entropy of the source."
5. "Variable-length codes, such as those used in Huffman code, assign shorter codewords to more frequent symbols, approach the source entropy."

### Analysis
This source provides the theoretical framework to understand compression limits. Shannon's theorem establishes that optimal compression depends on the entropy of the data source. Structured formats like JSON/YAML contain high redundancy (repeated field names, structural characters, whitespace), which means their entropy is much lower than their actual size. This mathematical proof shows they are far from optimal—optimal compression would approach the entropy, which for structured data is much smaller than the raw format size.

---

## Source 13: Medium - Columnar Database Dictionary Encode

**Citation:** "Columnar Database Dictionary Encode Compression," Towards Data, Medium, https://medium.com/towards-data-engineering/columnar-database-compression-dictionary-encoding-0d81925b908c

### Summary
Technical explanation of dictionary encode compression techniques used in columnar databases for structured data.

### Key Quotes
1. "The main idea behind dictionary encode is to replace the raw column values with integer codes."
2. "Dictionary encode is applicable for cases where a column has same values repeated in most occasions."
3. "For example, a gender column can have only two values, and dictionary encode replaces male and female with 0 and 1 respectively."
4. "Multiple methods can be used with each column as well—for example, run-length encode on dictionary tokens."
5. "The compound effect of the compression in columnar databases is a tremendous improvement over the standard compression."

### Analysis
Dictionary encode demonstrates that structured data's repetitive nature makes it highly compressible. Fields that repeat (like gender, status codes, categories) can be replaced with minimal integer codes. The fact that these techniques achieve "tremendous improvement" proves structured formats contain massive redundancy. Optimal compression would already have this redundancy removed—structured formats clearly don't.

---

## Source 14: Wikipedia - Data Compression

**Citation:** "Data compression," Wikipedia, https://en.wikipedia.org/wiki/Data_compression

### Summary
Comprehensive encyclopedia entry covers data compression fundamentals, algorithms, and applications.

### Key Quotes
1. "Lossless data compression algorithms usually exploit statistical redundancy to represent data without loss of any information."
2. "The more structured the data (low entropy), the more it can be compressed effectively."
3. "An image may have areas of color that do not change over several pixels; instead of code 'red pixel, red pixel, ...' the data may be encoded as '279 red pixels'."
4. "There are many schemes to reduce file size when they eliminate redundancy."
5. "The process is reversible, so that the original data can be reconstructed exactly."

### Analysis
This foundational source shows that compression works when it exploits redundancy. The key insight is that "the more structured the data (low entropy), the more it can be compressed effectively." This directly answers our research question: structured formats are HIGHLY compressible precisely because they are NOT optimally compressed. Their structure creates patterns and redundancy that compression algorithms can exploit.

---

## Source 15: Wikipedia - Dictionary Coder

**Citation:** "Dictionary coder," Wikipedia, https://en.wikipedia.org/wiki/Dictionary_coder

### Summary
Technical explanation of dictionary-based compression algorithms that include LZ77, LZ78, and their variants.

### Key Quotes
1. "Dictionary encode is fairly simple: a dictionary of words and phrases used in the data is created, and each of these words and phrases has its own reference."
2. "Dictionary-based compression algorithms like LZ77 achieve compression when they replace repeated occurrences of data with references to a single copy."
3. "In LZ77, a circular buffer called the 'window' holds the last N bytes of data processed."
4. "Lempel–Ziv (LZ) compression methods are among the most popular algorithms for lossless storage."
5. "Grammar-based codes like dictionary encode can compress highly repetitive input extremely effectively."

### Analysis
Dictionary encode's effectiveness with structured formats proves these formats are far from optimal. JSON's repeated field names ("name", "id", "status") and structural characters ("{", "}", "[", "]") are exactly the kind of repetitive patterns that dictionary compression exploits. The fact that LZ algorithms can "compress highly repetitive input extremely effectively" when applied to structured formats demonstrates these formats contain exploitable redundancy.

---

## Synthesis and Conclusions

### Key Results

1. **Structured formats are NOT near-optimal compression**
   - JSON can be compressed to 23.8% of original size (76.2% reduction)
   - XML is 70% larger than JSON uncompressed, but only 40% larger when both are compressed
   - Protobuf achieves 68% size reduction compared to JSON for the same data
   - These dramatic reductions prove massive redundancy exists in structured text formats

2. **Redundancy exists at multiple levels**
   - **Syntactic redundancy**: Braces, brackets, quotes, commas, close tags
   - **Semantic redundancy**: Repeated field names, repeated structural patterns
   - **Whitespace redundancy**: Indentation, line breaks, spaces
   - **Name redundancy**: Verbose field names that could be shorter or eliminated entirely

3. **Compression effectiveness varies by algorithm**
   - **Generic compressors** (gzip, Brotli): 65-70% size reduction
   - **Schema-aware compressors** (Protobuf, OpenZL): 68%+ size reduction
   - **Format-specific optimizations** (short field names, minification): 27.5% reduction before compression
   - **Hybrid approaches**: Combine techniques for best results

4. **Format structure matters for compression strategy**
   - **Columnar formats** (Parquet) compress better than row-based (Avro) due to value groups
   - **YAML** is more token-efficient for LLMs (56% reduction vs JSON) but not necessarily better for traditional compression
   - **Binary formats** (MessagePack, Protobuf) start smaller but converge with compressed text formats
   - **Dictionary encode** and **run-length encode** exploit structured format patterns

5. **Context determines optimal strategy**
   - **LLM applications**: Use YAML for token efficiency (46 vs 106 tokens)
   - **Web APIs**: Use Brotli for static content, gzip for dynamic responses
   - **Big data analytics**: Use Parquet for storage, Avro for stream
   - **Real-time systems**: Consider compression time vs size trade-offs

### Actionable Recommendations

#### For JSON/YAML/XML Structured Data

1. **Always apply compression** - Structured text formats contain 60-75% compressible redundancy
2. **Minify before compress** - Remove whitespace first (saves ~27%) then compress (saves ~75% more)
3. **Use Brotli over gzip when possible** - 5% better compression (70% vs 65%) with similar decompression speed
4. **Consider schema-aware formats** - For data-intensive applications, Protobuf/Avro can reduce size by 68%
5. **Optimize field names** - Shorter field names improve compression by ~27% even before compression

#### Format-Specific Strategies

**JSON:**
- Minify (remove whitespace): 27% reduction
- Use short field names: Additional 27% reduction
- Apply gzip: ~76% reduction from minified size
- Combined: ~94% reduction from pretty-printed original

**YAML:**
- Use for LLM contexts (56% token reduction vs JSON)
- Less critical for traditional compression (converges with JSON when compressed)
- Better for human-written configs due to readability

**XML:**
- Most redundant format (70% larger than JSON uncompressed)
- Consider EXI (Efficient XML Interchange) for binary compression
- Apply gzip/Brotli as minimum (reduces disadvantage to 40%)

**Binary formats (Protobuf, MessagePack):**
- Use when schema is known and stable
- 68% smaller than JSON (Protobuf)
- 15% smaller than JSON (MessagePack)
- Best for performance-critical, high-volume applications

#### Strategic Decision Framework

**When human readability is required:**
1. Use JSON or YAML
2. Minify in production
3. Apply Brotli compression for transmission
4. Store compressed versions of frequently accessed data

**When performance is critical:**
1. Use Protobuf for maximum compression (68% reduction)
2. Use MessagePack for schema-less binary (15% reduction)
3. Pre-compress static data with Brotli at highest level
4. Use gzip for dynamic data (faster compression)

**When analytics is the primary use case:**
1. Use Parquet for columnar storage (superior compression)
2. Use Avro for stream ingest
3. Apply dictionary encode for low-cardinality fields
4. Apply run-length encode for repeated values

**When work with LLMs:**
1. Use YAML for 56% token reduction
2. Consider TOON (Token-Oriented Object Notation) for maximum token efficiency
3. Balance between format verbosity and model comprehension
4. Compress payloads sent to API endpoints

### Theoretical Foundation

From information theory, we know that optimal compression approaches the entropy of the data source (Shannon's Source Coding Theorem). Structured formats have:

- **High redundancy** (repeated structural elements)
- **Low entropy** (predictable patterns)
- **Large gap** between actual size and theoretical minimum

This gap proves structured formats are far from optimal compression. The 60-75% compression ratios achieved demonstrate that at least 2/3 of the data in structured text formats is redundant and eliminable.

### Final Answer to Research Question

**How should structured formats affect compression strategy?**

Structured formats should be viewed as a compression opportunity, not a compression solution. The high redundancy in structured formats (repeated field names, structural characters, whitespace) makes them excellent candidates for compression but terrible as final storage/transmission formats without compression.

Strategy should consider:
- **Context**: LLM vs web API vs analytics vs storage
- **Access pattern**: Read-heavy vs write-heavy vs stream
- **Performance requirements**: Size vs speed vs CPU cost
- **Compatibility requirements**: Human-readable vs binary acceptable

**Do structured formats represent near-optimal compression?**

**Definitively NO.** Structured text formats are optimized for:
- Human readability
- Easy parse
- Debug and development
- Language-agnostic interchange

They are NOT optimized for compression. The 60-75% size reductions achievable through compression prove massive redundancy. Even binary structured formats like MessagePack are 15% larger than necessary. Schema-aware formats like Protobuf get closer to optimal (68% improvement), but only when they abandon self-describe structure.

The trade-off is deliberate: structured formats sacrifice compression efficiency for developer experience and interoperability. This makes them ideal for development and debug but unsuitable for production data transmission/storage without additional compression layers.

---

## Sources

1. [Web Data Serialization - JSON, XML, YAML & More Explained | Beeceptor](https://beeceptor.com/docs/concepts/data-exchange-formats/)
2. [YAML vs. JSON: Which Is More Efficient for Language Models? | Better Programming](https://medium.com/better-programming/yaml-vs-json-which-is-more-efficient-for-language-models-5bc11dd0f6df)
3. [Data Compression in REST APIs with gzip and Brotli | Zuplo](https://zuplo.com/learning-center/implementing-data-compression-in-rest-apis-with-gzip-and-brotli)
4. [GZIP vs Brotli Compression: Which Is Best for Web Performance | IOriver](https://www.ioriver.io/blog/gzip-vs-brotli-compression-performance)
5. [A comparison of compression formats for JSON store | Starbeamrainbowlabs](https://starbeamrainbowlabs.com/blog/article.php?article=posts/275-Compression-Comparison.html)
6. [Reduce JSON Data Size | Baeldung](https://www.baeldung.com/json-reduce-data-size)
7. [Compression of XML and JSON API Responses | ResearchGate](https://www.researchgate.net/publication/350864895_Compression_of_XML_and_JSON_API_Responses)
8. [Comparison of data-serialization formats | Wikipedia](https://en.wikipedia.org/wiki/Comparison_of_data-serialization_formats)
9. [.NET Serialization Smackdown: JSON vs. MessagePack vs. Protobuf | Medium](https://niravinfo.medium.com/net-serialization-smackdown-json-vs-messagepack-vs-protobuf-who-rules-your-bytes-e83027c22cc8)
10. [Meta Open Sources OpenZL: a Universal Compression Framework | InfoQ](https://www.infoq.com/news/2025/10/openzl-structured-compression/)
11. [Parquet vs. Avro: A Detailed Comparison of Big Data File Formats | Airbyte](https://airbyte.com/data-engineering-resources/parquet-vs-avro)
12. [Information Theory and Entropy in Data Compression | Nature Research Intelligence](https://www.nature.com/research-intelligence/nri-topic-summaries/information-theory-and-entropy-in-data-compression-micro-89473)
13. [Columnar Database Dictionary Encode Compression | Medium](https://medium.com/towards-data-engineering/columnar-database-compression-dictionary-encoding-0d81925b908c)
14. [Data compression | Wikipedia](https://en.wikipedia.org/wiki/Data_compression)
15. [Dictionary coder | Wikipedia](https://en.wikipedia.org/wiki/Dictionary_coder)

---

## Additional References

- [JSON vs XML vs YAML: Which Data Format Should You Use in 2026? | Orbit2x](https://orbit2x.com/blog/json-vs-xml-vs-yaml)
- [TOON vs TRON vs JSON vs YAML vs CSV for LLM Apps | Piotr Sikora](https://www.piotr-sikora.com/blog/2025-12-05-toon-tron-csv-yaml-json-format-comparison)
- [Alternative Binary Formats and Compression Methods | Lucid](https://lucid.co/techblog/2019/12/06/json-compression-alternative-binary-formats-and-compression-methods)
- [The Data Formats Landscape in 2025 | DEV Community](https://dev.to/dataformathub/the-evolving-landscape-of-data-formats-json-yaml-and-the-rise-of-specialized-standards-in-2025-3mp8)
- [Run-length encode | Wikipedia](https://en.wikipedia.org/wiki/Run-length_encoding)
- [Entropy code | Wikipedia](https://en.wikipedia.org/wiki/Entropy_coding)
